
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Shifted window transformers &#8212; Random Walks</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css?v=03c54da9" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-HP14V4DGEF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-HP14V4DGEF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-HP14V4DGEF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/papers/swin/swin';</script>
    <link rel="icon" href="../../../_static/dicefav.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Random Walks</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Home
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes on books</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../toc/000-intro.html">Theory of Computation</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../toc/001-fsa.html">FSAs and Regular Expressions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../toc/002-cfl.html">PDAs and Context Free Grammars</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../toc/000-exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../prob-intro/intro.html">Probability: An introduction</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch01/content.html">Events and Probabilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch02/content.html">Discrete random variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch03/content.html">Multivariate discrete distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch04/content.html">Probability generating functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch05/content.html">Distribution and density functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch06/content.html">Multivariate distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch07/content.html">Moment generating functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch08/content.html">Main limit theorems</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mira/000-intro.html">Measure Theory</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mira/001-riemann.html">Riemann integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mira/002-measures.html">Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mira/000-exercises.html">Exercises</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Papers &amp; Miscellanous</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../intro.html">Stream of papers</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../transformers/transformers.html">Introduction to transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../why-covariances/why-covariances.html">Why covariance functions?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ais/ais.html">Annealed importance sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rff/rff.html">Random Fourier features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../score-matching/score-matching.html">Estimation by score matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../svgd/svgd.html">Stein variational gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../num-sde/num-sde.html">Numerical simulation of SDEs</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/stratisMarkou/random-walks" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/stratisMarkou/random-walks/issues/new?title=Issue%20on%20page%20%2Fbook/papers/swin/swin.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/book/papers/swin/swin.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Shifted window transformers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#windowed-self-attention">Windowed self-attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shifted-windows">Shifted windows</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#patch-merging">Patch merging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-vs-convolutions">Attention vs convolutions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="shifted-window-transformers">
<h1>Shifted window transformers<a class="headerlink" href="#shifted-window-transformers" title="Link to this heading">#</a></h1>
<script async defer src="https://buttons.github.io/buttons.js"></script>
<p><a class="github-button" href="https://github.com/stratisMarkou/random-walks" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="octicon-star" data-size="large" aria-label="Star stratisMarkou/random-walks on GitHub" style="float: right;">Star</a>
<a class="github-button" href="https://github.com/stratisMarkou/random-walks/issues" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="octicon-issue-opened" data-size="large" aria-label="Issue stratisMarkou/random-walks on GitHub">Issue</a>
<a class="github-button" href="https://github.com/stratisMarkou/random-walks/subscription" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="octicon-eye" data-size="large" aria-label="Watch stratisMarkou/random-walks on GitHub">Watch</a>
<a class="github-button" href="https://github.com/stratisMarkou" data-color-scheme="no-preference: light; light: light; dark: dark;" data-size="large" aria-label="Follow @stratisMarkou on GitHub">Follow</a></p>
<p><a class="reference internal" href="../transformers/transformers.html"><span class="std std-doc">Transformers</span></a> are an extremely flexible deep architecture which has greatly impacted a range of machine learning applications.
Arguably, the impact of the transformer is due to its high modelling capacity and the fact that it can easily be applied to different data modalities with minimal implementation changes.
The distinguishing feature which gives the transformer these advantages is its attention layer.
Attention allows the transformer block to update the features of its input tokens in an adaptive way that depends on the features themselves, making the overall architecture extremely flexible.
Further, after the data have been converted into tokens, attention can be straightforwardly applied and many of the details of the data can be abstracted away, which means that the transformer can be easily applied to a range of modalities such as text, images, graphs and many more.</p>
<p>However, one important limitation of atention is that its computation and memory costs scale quadratically with the number of tokens.
This makes standard transformers difficult to scale to inputs with many tokens such as, for example, long sentences or large images.
The shifted window transformer (Swin) is an architecture which helps mitigate the issues of computational and memory complexity.
Swin was originally formulated to tackle image data, on which we focus here, but note that the main idea behind Swin is also applicable to other data modalities such as text or, more generally, any kind of gridded data.</p>
<p>The innovation of the Swin transformer is to apply attention in a local way, such that only tokens which are near each other (in some appropriate sense of closeness) are allowed to attend to one another.
In this sense Swin transformers are somewhat similar to convolutional neural networks (CNNs), where each input pixel affects only a local neighbourhood of output pixels at each layer of the network.
This reduces the computational cost of the Swin transformer from quadratic to linear in the number of input tokens allowing scaling to large numbers of tokens.</p>
<section id="windowed-self-attention">
<h2>Windowed self-attention<a class="headerlink" href="#windowed-self-attention" title="Link to this heading">#</a></h2>
<p>The main bottleneck in scaling transformers to large numbers of tokens is the self-attention operation.
Given <span class="math notranslate nohighlight">\(N\)</span> tokens, simply building an <span class="math notranslate nohighlight">\(N \times N\)</span> attention matrix requires <span class="math notranslate nohighlight">\(\mathcal{O}(N^2)\)</span> compute and memory, which quickly gets very expensive for large images.
The idea behind the Swin transformer is to modify the self-attention operation, by breaking up an input image into smaller chunks, or windows, and having each token attend to all other tokens within its window, and to no other tokens outside it, as illustrated in <code class="xref std std-numref docutils literal notranslate"><span class="pre">swin:vit_vs_swin</span></code>.</p>
<figure class="align-default" id="swin-vit-vs-swin">
<a class="reference internal image-reference" href="../../../_images/swin_vs_vit.png"><img alt="../../../_images/swin_vs_vit.png" src="../../../_images/swin_vs_vit.png" style="height: 150px;" /></a>
<figcaption>
<p><span class="caption-text">Comparison of the self-attention operations in a standard vision transformer (left) and a Swin transformer (right).
Each gray square corresponds to a token, and red boxes mark groups of tokens over which attention is applied.</span><a class="headerlink" href="#swin-vit-vs-swin" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Whereas in a standard vision transformer (left) each token (gray squares) attends to every other token in the image (large red square), in a Swin transformer each token attends only to those tokens within its window (smaller red squares), and none outside it.
This significantly reduces the size of the attention matrices that need to be calculated.
For example in the ViT in the illustration above, we have <span class="math notranslate nohighlight">\(64\)</span> tokens, resulting in a <span class="math notranslate nohighlight">\(64 \times 64\)</span> attention matrix which has <span class="math notranslate nohighlight">\(4096\)</span> entries.
Breaking these up into four independent windows means each attention window contains <span class="math notranslate nohighlight">\(16\)</span> tokens, resulting in four <span class="math notranslate nohighlight">\(16 \times 16\)</span> attention matrices with <span class="math notranslate nohighlight">\(256\)</span> entries, for a total of <span class="math notranslate nohighlight">\(1024\)</span> attention matrix entries across all four windows - a factor of <span class="math notranslate nohighlight">\(4\)</span> lower.
These savings become much more significant for larger images, and increase as the window size is reduced.
However, this approach has a problem: tokens across different windows do not affect each other, which is a problem.
Swin gets around this issue by shifting the windows.</p>
</section>
<section id="shifted-windows">
<h2>Shifted windows<a class="headerlink" href="#shifted-windows" title="Link to this heading">#</a></h2>
<p>In order to allow information to propagate across windows, Swin uses shifted windows.
Specifically, each windowed transformer block is followed by another transformer block with shifted windows, where the amount of shift is set to be half the window size, as illustrated by <code class="xref std std-numref docutils literal notranslate"><span class="pre">swin:shifting</span></code>.
Since the original and shifted windows overlap, information can propagate across them.</p>
<figure class="align-default" id="swin-shifting">
<a class="reference internal image-reference" href="../../../_images/swin_two_windows.png"><img alt="../../../_images/swin_two_windows.png" src="../../../_images/swin_two_windows.png" style="height: 160px;" /></a>
<figcaption>
<p><span class="caption-text">Illustration of shifted attention layers in Swin.
Transformer blocks with standard attention windows (first) are interleaved with blocks using shifted windows (second).
This enables information to propagate across windows.</span><a class="headerlink" href="#swin-shifting" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The question then becomes how to handle windows which are cut short across the border of the image.
Swin handles this by grouping these edge windows together by using cyclic shifts.
First, the input image is shifted vertically and horizontally by half the window size.
This aligns</p>
<figure class="align-default" id="swin-cyclic">
<a class="reference internal image-reference" href="../../../_images/cyclic_shifts.png"><img alt="../../../_images/cyclic_shifts.png" src="../../../_images/cyclic_shifts.png" style="height: 160px;" /></a>
<figcaption>
<p><span class="caption-text">Illustration of a shifted window block in Swin.
Colours illustrate the groups of tokens which attend to one another.
The first shift here is two pixels (half the window size of four pixels) downwards and rightwards.
The windowed transformer block is applied in the shifted space modifying the values of the tokens (indicated by heavier coloured entries).
The second, reverse, shift here is two pixels up</span><a class="headerlink" href="#swin-cyclic" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="patch-merging">
<h2>Patch merging<a class="headerlink" href="#patch-merging" title="Link to this heading">#</a></h2>
</section>
<section id="attention-vs-convolutions">
<h2>Attention vs convolutions<a class="headerlink" href="#attention-vs-convolutions" title="Link to this heading">#</a></h2>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
An aside: reverse of patch extraction as transpose operation<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Invertibility.</strong>
Under certain conditions patch extraction can be undone, leveraging the fact that the reverse operation is actually its transpose.
However, there are exaples of patch extraction which cannot be reversed.
For example, if we use large strides and small patch sizes, or if we use dilation (also known as à trous; this is the <code class="docutils literal notranslate"><span class="pre">rate</span></code> argument in <code class="docutils literal notranslate"><span class="pre">tf.image.extract_patches</span></code>), then some of the pixels of the input image may be skipped over, and not affect the output at all.
This means the patch extraction operation cannot be undone.
From now on, we will assume that the patch extraction is performed using a combination of patch size, striding and dilation such that every pixel in the input affects the output, and no pixels are skipped over.</p>
<p class="sd-card-text"><strong>Matrix view.</strong>
Suppose the input image <span class="math notranslate nohighlight">\(x\)</span> is an array of shape <span class="math notranslate nohighlight">\((H, W, C),\)</span> where <span class="math notranslate nohighlight">\(H\)</span> is its height, <span class="math notranslate nohighlight">\(W\)</span> is its width and <span class="math notranslate nohighlight">\(C\)</span> is the number of channels.
We can view patch extraction as a linear operation <span class="math notranslate nohighlight">\(P\)</span> which maps the image <span class="math notranslate nohighlight">\(x\)</span> to another image <span class="math notranslate nohighlight">\(y = P(x),\)</span> of shape <span class="math notranslate nohighlight">\((I, J, K),\)</span> where the dimensions denote the horizontal patch index, vertical patch index and patch dimension respectively.
Note that by the definition of patch extraction, each pixel in the output is affected by exactly one pixel in the input (though each pixel in the input may affect more than one pixel in the output).
For now, let us assume that each pixel in the input affects exactly one pixel in the output, i.e. each <span class="math notranslate nohighlight">\(x_{hwc}\)</span> affects exactly one <span class="math notranslate nohighlight">\(y_{ijk}.\)</span>
Let’s define <span class="math notranslate nohighlight">\(\texttt{flat}\)</span> to be the operation that takes a multi-dimensional array and flattens it into a single-dimensional array.
Since <span class="math notranslate nohighlight">\(\texttt{flat}\)</span> is an invertible linear operation, and patch extraction is also an invertible linear operation, then their composition is also an invertible linear opeartion <span class="math notranslate nohighlight">\(P'.\)</span>
Let <span class="math notranslate nohighlight">\(\mathcal{M}(P')\)</span> be the matrix corresponding to <span class="math notranslate nohighlight">\(P',\)</span> so that</p>
<div class="math notranslate nohighlight">
\[\texttt{flat}(y) = \mathcal{M}(P') \texttt{flat}(x).\]</div>
<p class="sd-card-text">Now note that since each element of <span class="math notranslate nohighlight">\(\texttt{flat}(x)\)</span> affects precisely one element of <span class="math notranslate nohighlight">\(\texttt{flat}(y),\)</span> the matrix <span class="math notranslate nohighlight">\(\mathcal{M}(P')\)</span> is actually a permutation matrix, i.e. it contains exactly one <span class="math notranslate nohighlight">\(1\)</span> in each row and in each column, and all other entries are <span class="math notranslate nohighlight">\(0.\)</span>
Now, the inverse of a permutation matrix is its transpose, that is</p>
<div class="math notranslate nohighlight">
\[\texttt{flat}(x) = \mathcal{M}(P')^\top \texttt{flat}(y).\]</div>
<p class="sd-card-text">So the inverse of the patch embedding operation is actually its transpose operation, and that’s because the patch embedding is actually a permutation!</p>
<p class="sd-card-text"><strong>General case.</strong>
But what happens in the more general case where some <span class="math notranslate nohighlight">\(x_{hwc}\)</span> affect more than one <span class="math notranslate nohighlight">\(y_{ijk}\)</span>?
The equation <span class="math notranslate nohighlight">\(\texttt{flat}(y) = \mathcal{M}(P') \texttt{flat}(x)\)</span> still holds, and the matrix <span class="math notranslate nohighlight">\(\mathcal{M}(P')\)</span> stil contains a single <span class="math notranslate nohighlight">\(1\)</span> per row.
However, <span class="math notranslate nohighlight">\(\mathcal{M}(P')\)</span> may now contain multiple <span class="math notranslate nohighlight">\(1\)</span> entries in each column.
Therefore, it is no longer a permutation matrix, and <span class="math notranslate nohighlight">\(\texttt{flat}(x) \neq \mathcal{M}(P')^\top \texttt{flat}(y).\)</span>
In particular, when we multiply <span class="math notranslate nohighlight">\(\texttt{flat}(y)\)</span> by <span class="math notranslate nohighlight">\(\mathcal{M}(P')^\top,\)</span> each entry of <span class="math notranslate nohighlight">\(\texttt{flat}(x)\)</span> which affects <span class="math notranslate nohighlight">\(n\)</span> entries in <span class="math notranslate nohighlight">\(\texttt{flat}(y)\)</span> will be counted <span class="math notranslate nohighlight">\(n\)</span> times.
To illustrate this problem, suppose consider the following vectors and matrices (which do not correspond to patch extraction, and are just an illustration of the problem) and the following matrix-vector multiplication</p>
<div class="math notranslate nohighlight">
\[\begin{split}
u = \begin{bmatrix}
u_1 \\
u_2
\end{bmatrix}, M = \begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
0 &amp; 1
\end{bmatrix} \implies v = Mu = \begin{bmatrix}
u_1 \\
u_2 \\
u_2
\end{bmatrix}.
\end{split}\]</div>
<p class="sd-card-text">If we multiply <span class="math notranslate nohighlight">\(v = Mu\)</span> by <span class="math notranslate nohighlight">\(M^\top\)</span> we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}
M^\top v = M^\top Mu = \begin{bmatrix}
1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 1
\end{bmatrix} \begin{bmatrix}
u_1 \\
u_2 \\
u_2
\end{bmatrix} = \begin{bmatrix}
u_1 \\
2u_2
\end{bmatrix},
\end{split}\]</div>
<p class="sd-card-text">i.e. we have double-counted <span class="math notranslate nohighlight">\(u_2.\)</span>
If instead we divide each row of <span class="math notranslate nohighlight">\(M^\top\)</span> by its sum before multiplying we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}
M^\top v = \begin{bmatrix}
1 &amp; 0 &amp; 0\\
0 &amp; \frac{1}{2} &amp; \frac{1}{2}
\end{bmatrix} \begin{bmatrix}
u_1 \\
u_2 \\
u_2
\end{bmatrix} = \begin{bmatrix}
u_1 \\
u_2
\end{bmatrix},
\end{split}\]</div>
<p class="sd-card-text">which is the desired result.
Therefore, all we have to do is divide each row of <span class="math notranslate nohighlight">\(M^\top\)</span> by its sum, or alternatively divide each entry of <span class="math notranslate nohighlight">\(M^\top v\)</span> by the sum of each row of <span class="math notranslate nohighlight">\(M^\top.\)</span></p>
<p class="sd-card-text"><strong>Implementation.</strong>
We can perform all of the above in a few lines in Tensorflow, or a similar autodiff framework, as shown above.
First, the command</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad_ys</span><span class="o">=</span><span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text">computes the gradients of the scalar <code class="docutils literal notranslate"><span class="pre">sum(y)</span></code> with respect to <code class="docutils literal notranslate"><span class="pre">x</span></code> and multiplies these gradients together with the corresponding entries in <code class="docutils literal notranslate"><span class="pre">grad_ys</span></code>, as performed in revrse mode differentiation.
This is equivalent to the multiplication <span class="math notranslate nohighlight">\(\mathcal{M}(P')^\top \texttt{flat}(y)\)</span> except the tensors are not actually flattened, but retain their original shapes.
Then, the command</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text">computes the derivative of <code class="docutils literal notranslate"><span class="pre">sum(y)</span></code> with respect to <code class="docutils literal notranslate"><span class="pre">x</span></code>.
If an entry in <code class="docutils literal notranslate"><span class="pre">x</span></code> affects <span class="math notranslate nohighlight">\(n\)</span> entries in <code class="docutils literal notranslate"><span class="pre">y</span></code>, then the gradient of <code class="docutils literal notranslate"><span class="pre">sum(y)</span></code> with respect to that entry of <code class="docutils literal notranslate"><span class="pre">x</span></code> will be <span class="math notranslate nohighlight">\(n,\)</span> so the division</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad_ys</span><span class="o">=</span><span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text">gives the result we were after.</p>
</div>
</details><p>Here is an illustration of the application of a shifted multi-head self-attention layer.</p>
<p>Most of the implementation for a Swin transformer is identical to that of a standard transformer, so we will reuse most of the code from the <a class="reference internal" href="../transformers/transformers.html"><span class="std std-doc">introduction to transformers example</span></a>.</p>
<div class="cell tag_remove-output tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Seed</span><span class="p">,</span>
        <span class="n">projection_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;self_attention&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Split the seed and set up the dense layers</span>
        <span class="n">seed1</span><span class="p">,</span> <span class="n">seed2</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Uk</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">projection_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">GlorotNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">seed1</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Uq</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">projection_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">GlorotNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">seed2</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
        <span class="p">)</span>


    <span class="k">def</span> <span class="nf">self_attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute self-attention weights for tokens in a sequence</span>

<span class="sd">        Args:</span>
<span class="sd">            x: input sequence of tokens, shape (B, N, D)</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            attention weights, shape (B, N, N)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Uk</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Uq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">dot_product</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">dot_product</span> <span class="o">/=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">k</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dot_product</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply self-attention to a sequence of tokens</span>

<span class="sd">        Args:</span>
<span class="sd">            x: input sequence of tokens, shape (B, N, D)</span>

<span class="sd">        Returns:</span>
<span class="sd">            output sequence of tokens, shape (B, N, D)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">self_attention_weights</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Seed</span><span class="p">,</span>
        <span class="n">token_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">projection_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;multi_head_self_attention&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">keys</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">SelfAttention</span><span class="p">(</span>
                <span class="n">seed</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
                <span class="n">projection_dim</span><span class="o">=</span><span class="n">projection_dim</span><span class="p">,</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tfk</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
                <span class="n">token_dim</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">GlorotNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">key</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply multi-head self-attention to a sequence of tokens</span>

<span class="sd">        Args:</span>
<span class="sd">            x: input sequence of tokens, shape (B, N, D)</span>

<span class="sd">        Returns:</span>
<span class="sd">            output sequence of tokens, shape (B, N, D)</span>
<span class="sd">        &quot;&quot;&quot;</span>
            
        <span class="c1"># Compute tokens for each head and apply linear </span>
        <span class="n">heads</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">linear</span><span class="p">(</span><span class="n">sa</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">sa</span><span class="p">,</span> <span class="n">linear</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Stack and sum across heads</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Seed</span><span class="p">,</span>
        <span class="n">num_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mlp&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Set up output dimensions of linear layers</span>
        <span class="n">out_feats</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_hidden</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span> <span class="o">+</span> <span class="p">[</span><span class="n">num_output</span><span class="p">]</span>

        <span class="c1"># Split the random key into sub-keys for each layer</span>
        <span class="n">seeds</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tfk</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
                <span class="n">out_feat</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">GlorotNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">seed</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">seed</span><span class="p">,</span> <span class="n">out_feat</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">seeds</span><span class="p">,</span> <span class="n">out_feats</span><span class="p">)</span>
        <span class="p">]</span>


    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute forward pass through the MLP.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: input tensor of shape (..., feature_dim,)</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            output tensor of shape (..., feature_dim,)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Seed</span><span class="p">,</span>
        <span class="n">token_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_num_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;swin_transformer_block&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">key1</span><span class="p">,</span> <span class="n">key2</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mhsa</span> <span class="o">=</span> <span class="n">MultiHeadSelfAttention</span><span class="p">(</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">key1</span><span class="p">,</span>
            <span class="n">token_dim</span><span class="o">=</span><span class="n">token_dimension</span><span class="p">,</span>
            <span class="n">projection_dim</span><span class="o">=</span><span class="n">token_dimension</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">key2</span><span class="p">,</span>
            <span class="n">num_hidden</span><span class="o">=</span><span class="n">mlp_num_hidden</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">mlp_num_layers</span><span class="p">,</span>
            <span class="n">num_output</span><span class="o">=</span><span class="n">token_dimension</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply the transformer block to input tokens `x`.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            x: input tensor of shape (B, N, D)</span>

<span class="sd">        Returns:</span>
<span class="sd">            output tensor of shape (B, N, D)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mhsa</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">x</span>
    

<span class="k">class</span> <span class="nc">ImageTokeniser</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Seed</span><span class="p">,</span>
        <span class="n">token_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;image_tokeniser&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">patch_size</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">patch_size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Patch size must be one or even&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">token_dimension</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">),</span>
            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">),</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;channels_last&quot;</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">GlorotNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">seed</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenise the image `x`, applying a strided convolution.</span>
<span class="sd">        This is equivalent to splitting the image into patches,</span>
<span class="sd">        and then linearly projecting each one of these using a</span>
<span class="sd">        shared linear projection.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            x: image input tensor of shape (B, W, H, C)</span>

<span class="sd">        Returns:</span>
<span class="sd">            output tensor of shape (B, N, D)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Input dimensions must be divisible by patch size, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;found </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">=}</span><span class="s2"> and </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">kernel_size</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    

<span class="k">class</span> <span class="nc">PositionEmbedding</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Seed</span><span class="p">,</span>
        <span class="n">token_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;position_embedding&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
                <span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">token_dimension</span><span class="p">),</span>
                <span class="n">seed</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">seed</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add position embeddings to input tensor.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            x: input tensor of shape (B, H, W, D)</span>

<span class="sd">        Returns:</span>
<span class="sd">            output tensor of shape (B, H, W, D)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shift_horizontally_and_vertically</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">shift</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shift windows in the input tensor `x` by shift along its width and height.</span>
<span class="sd">    For example, using shift == 1 (and neglecting the B and D dimensions),</span>
<span class="sd">    the H and W dimensions would change as follows:</span>
<span class="sd">    </span>
<span class="sd">                      Original                Shifted</span>
<span class="sd">                 -----------------       -----------------</span>
<span class="sd">                |  x   x   x   o  |     |  *   +   +   +  |</span>
<span class="sd">                |  x   x   x   o  |     |  o   x   x   x  |</span>
<span class="sd">                |  x   x   x   o  |     |  o   x   x   x  |</span>
<span class="sd">                |  +   +   +   *  |     |  o   x   x   x  |</span>
<span class="sd">                 -----------------       -----------------</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        x: input tensor of shape (B, H, W, D)</span>
<span class="sd">        shift: amount of shift to apply</span>

<span class="sd">    Returns:</span>
<span class="sd">        output tensor of shape (B, H, W, D)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">extract_windows</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract non-overlapping windows from input tensor `x`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        x: input tensor of shape</span>

<span class="sd">    Returns:</span>
<span class="sd">        output tensor of shape (B*(H//2)*(W//2), 4, D)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">3</span><span class="p">]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">W</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>  <span class="c1"># (B, H//2, W//2, 2, 2, D)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span><span class="o">*</span><span class="n">H</span><span class="o">//</span><span class="mi">2</span><span class="o">*</span><span class="n">W</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">combine_windows</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">original_shape</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Combine windows extracted from input tensor `x`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        x: input tensor of shape (B*(H//2)*(W//2), 4, D)</span>
<span class="sd">        x_shape: shape of original tensor</span>

<span class="sd">    Returns:</span>
<span class="sd">        output tensor of shape (B, H, W, D)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">original_shape</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">W</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>  <span class="c1"># (B, H//2, 2, W//2, 2, D)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PatchMergingLayer</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Seed</span><span class="p">,</span>
        <span class="n">num_out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;patch_merging&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">num_out_features</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">GlorotNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">seed</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_out_features</span> <span class="o">=</span> <span class="n">num_out_features</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply patch merging to input tensor `x`.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            x: input tensor of shape (B, H, W, D)</span>

<span class="sd">        Returns:</span>
<span class="sd">            output tensor of shape (B, H//2, W//2, 2*D)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">extract_patches</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">rates</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">,</span>
        <span class="p">)</span>  <span class="c1"># (B, H//2, W//2, 4*D)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, H//2, W//2, 2*D)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SwinTransformerBlockStage</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Seed</span><span class="p">,</span>
        <span class="n">token_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_num_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_block_pairs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;swin_transformer_block&quot;</span><span class="p">,</span> 
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">seeds</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">num_block_pairs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">first_blocks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">TransformerBlock</span><span class="p">(</span>
                <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="p">],</span>
                <span class="n">token_dimension</span><span class="o">=</span><span class="n">token_dimension</span><span class="p">,</span>
                <span class="n">mlp_num_hidden</span><span class="o">=</span><span class="n">mlp_num_hidden</span><span class="p">,</span>
                <span class="n">mlp_num_layers</span><span class="o">=</span><span class="n">mlp_num_layers</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_block_pairs</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">second_blocks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">TransformerBlock</span><span class="p">(</span>
                <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">token_dimension</span><span class="o">=</span><span class="n">token_dimension</span><span class="p">,</span>
                <span class="n">mlp_num_hidden</span><span class="o">=</span><span class="n">mlp_num_hidden</span><span class="p">,</span>
                <span class="n">mlp_num_layers</span><span class="o">=</span><span class="n">mlp_num_layers</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_block_pairs</span><span class="p">)</span>
        <span class="p">]</span>


    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply the Swin Transformer block to input tokens `x`.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            x: input tensor of shape (B, H, W, D)</span>

<span class="sd">        Returns:</span>
<span class="sd">            output tensor of shape (B, H, W, D)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">for</span> <span class="n">first_block</span><span class="p">,</span> <span class="n">second_block</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">first_blocks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">second_blocks</span><span class="p">):</span>

            <span class="n">original_shape</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">extract_windows</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B*H//2*W//2, 4, D)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">first_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B*H//2*W//2, 4, D)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">combine_windows</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">original_shape</span><span class="p">)</span>  <span class="c1"># (B, H, W, D)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">shift_horizontally_and_vertically</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, H, W, D)</span>

            <span class="n">original_shape</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">extract_windows</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B*H//2*W//2, 4, D)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">second_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B*H//2*W//2, 4, D)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">combine_windows</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">original_shape</span><span class="p">)</span>  <span class="c1"># (B, H, W, D)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">shift_horizontally_and_vertically</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, H, W, D)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">combine_windows</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">original_shape</span><span class="p">)</span>  <span class="c1"># (B, H, W, D)</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TinySwinTransformer</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Seed</span><span class="p">,</span>
        <span class="n">tokeniser</span><span class="p">:</span> <span class="n">ImageTokeniser</span><span class="p">,</span>
        <span class="n">embedding</span><span class="p">:</span> <span class="n">PositionEmbedding</span><span class="p">,</span>
        <span class="n">token_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_num_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_blocks_per_stage</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;tiny_swin_transformer&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">num_stages</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_blocks_per_stage</span><span class="p">)</span>
        <span class="n">seeds</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">num_stages</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">stages</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_merging_layers</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">num_blocks_per_stage</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">patch_merging_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">PatchMergingLayer</span><span class="p">(</span>
                    <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">num_out_features</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span><span class="p">)</span><span class="o">*</span><span class="n">token_dimension</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">SwinTransformerBlockStage</span><span class="p">(</span>
                    <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">token_dimension</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span><span class="p">)</span><span class="o">*</span><span class="n">token_dimension</span><span class="p">,</span>
                    <span class="n">mlp_num_hidden</span><span class="o">=</span><span class="n">mlp_num_hidden</span><span class="p">,</span>
                    <span class="n">mlp_num_layers</span><span class="o">=</span><span class="n">mlp_num_layers</span><span class="p">,</span>
                    <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                    <span class="n">num_block_pairs</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">final_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">num_hidden</span><span class="o">=</span><span class="n">mlp_num_hidden</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">mlp_num_layers</span><span class="p">,</span>
            <span class="n">num_output</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tokeniser</span> <span class="o">=</span> <span class="n">tokeniser</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply vision transformer to batch of images.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            x: input image tensor of shape (B, H, W, C)</span>

<span class="sd">        Returns:</span>
<span class="sd">            output logits tensor of shape (B, num_classes)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokeniser</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">patch_merging</span><span class="p">,</span> <span class="n">stage</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_merging_layers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">patch_merging</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">stage</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_mlp</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_logsumexp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="dataset">
<h3>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">#</a></h3>
<p>Because this is meant to be a demo that should run on a laptop, we’ll use the MNIST dataset.
We’ll use <a class="reference external" href="https://www.tensorflow.org/datasets/api_docs/python/tfds">tensorflow datasets</a> to load the data and preprocess it.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>

<span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>

<span class="k">def</span> <span class="nf">get_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">split</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">data_dir</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">&quot;/tmp/tfds&quot;</span><span class="p">):</span>

    <span class="c1"># Conversion from labels to one-hot</span>
    <span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">],</span> <span class="s2">&quot;Split must be &#39;train&#39; or &#39;test&#39;&quot;</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mnist&quot;</span><span class="p">,</span>
        <span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">,</span>
        <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">,</span>
        <span class="n">shuffle_files</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">preprocess_image</span><span class="p">)</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">one_hot</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ds</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Link to this heading">#</a></h3>
<p>Now let’s train the network.
In general, when training a ViT, a few tricks are typically used, including for example, learning rate scheduling and data augmentation.
Dropout is also sometimes used in the architecture itself.
We won’t use any of these techniques here to keep it simple.</p>
<div class="cell tag_hide-input tag_hide-output docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span>
    <span class="n">images</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Loss</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span>

<span class="c1"># Model parameters</span>
<span class="n">token_dimension</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_mlp_hidden</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">num_mlp_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_blocks_per_stage</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Training parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1e-6</span>

<span class="c1"># Create the tokeniser and embeddings</span>
<span class="n">seeds</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split_seed</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">tokeniser</span> <span class="o">=</span> <span class="n">ImageTokeniser</span><span class="p">(</span>
    <span class="n">seeds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">token_dimension</span><span class="o">=</span><span class="n">token_dimension</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">embedding</span> <span class="o">=</span> <span class="n">PositionEmbedding</span><span class="p">(</span>
    <span class="n">seeds</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">token_dimension</span><span class="o">=</span><span class="n">token_dimension</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create a transformer</span>
<span class="n">transformer</span> <span class="o">=</span> <span class="n">TinySwinTransformer</span><span class="p">(</span>
    <span class="n">seeds</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="n">tokeniser</span><span class="o">=</span><span class="n">tokeniser</span><span class="p">,</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">embedding</span><span class="p">,</span>
    <span class="n">token_dimension</span><span class="o">=</span><span class="n">token_dimension</span><span class="p">,</span>
    <span class="n">mlp_num_hidden</span><span class="o">=</span><span class="n">num_mlp_hidden</span><span class="p">,</span>
    <span class="n">mlp_num_layers</span><span class="o">=</span><span class="n">num_mlp_layers</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
    <span class="n">num_blocks_per_stage</span><span class="o">=</span><span class="n">num_blocks_per_stage</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create loss function and accuracy helpers</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">CategoricalAccuracy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "db81c9ae1ece4e4e8aae5015311cd800", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "5d994592c82746b3b7cbd91004ae8db8", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "414378247fee43b99303f0e3e9e63aa0", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "b3224cf642c84356961c7c5e8c28b182", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "31a1d95291db4bf685277244b6104a75", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "fc638e5b9731407ca4281e40699e6641", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "705b39fc26064483bf3a93e6a7046175", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "9cbec332b7bb4d799dc306fb05e17bbf", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "0095bd4551014018808667bc26b395e0", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "77c391fd18ae463aba43a499f35e1e2d", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 10: loss 0.089 (train 0.024), acc. 0.973 (train 0.972)
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input tag_center-output docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/5b555565b39c4a5f1f1d65d51458d0ce8dfec8a60ba62cc51cb301d287ecd9e5.svg" src="../../../_images/5b555565b39c4a5f1f1d65d51458d0ce8dfec8a60ba62cc51cb301d287ecd9e5.svg" /></div>
</div>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>We have looked at the details of the transformer architecture.
It consists of identical blocks, each of which contains a self-attention and a multi-layer perceptron operation, together with normalisation layers and residual connections.
Coupling these together with position embeddings and an appropriate tokenisation layer makes up the entire transformer architecture.
We looked at a specific example for computer vision, the ViT, and trained it on MNIST.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<span class="target" id="id1"></span></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book/papers/swin"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#windowed-self-attention">Windowed self-attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shifted-windows">Shifted windows</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#patch-merging">Patch merging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-vs-convolutions">Attention vs convolutions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Stratis Markou
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>