
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Annealed importance sampling &#8212; Random Walks</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css?v=03c54da9" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-HP14V4DGEF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-HP14V4DGEF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-HP14V4DGEF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/papers/ais/ais';</script>
    <link rel="icon" href="../../../_static/dicefav.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Random Fourier features" href="../rff/rff.html" />
    <link rel="prev" title="Why covariance functions?" href="../why-covariances/why-covariances.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Random Walks</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Home
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes on books</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../toc/000-intro.html">Theory of Computation</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../toc/001-fsa.html">FSAs and Regular Expressions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../toc/002-cfl.html">PDAs and Context Free Grammars</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../toc/000-exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../prob-intro/intro.html">Probability: An introduction</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch01/content.html">Events and Probabilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch02/content.html">Discrete random variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch03/content.html">Multivariate discrete distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch04/content.html">Probability generating functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch05/content.html">Distribution and density functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch06/content.html">Multivariate distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch07/content.html">Moment generating functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch08/content.html">Main limit theorems</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mira/000-intro.html">Measure Theory</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mira/001-riemann.html">Riemann integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mira/002-measures.html">Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mira/000-exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../topology/000-intro.html">Topology</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../topology/001-metric-spaces.html">Metric spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../topology/002-topological-spaces.html">Topological Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../topology/003-connectivity.html">Connectivity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../topology/004-compactness.html">Compactness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../topology/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../lst/000-intro.html">Logic and set theory</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../lst/001-propositional-calculus.html">Propositional calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../lst/002-well-orderings-and-ordinals.html">Well-orderings and ordinals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../lst/exercises.html">Exercises</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Papers &amp; Miscellanous</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../intro.html">Stream of papers</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../swin/swin.html">Shifted window transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transformers/transformers.html">Introduction to transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../why-covariances/why-covariances.html">Why covariance functions?</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Annealed importance sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rff/rff.html">Random Fourier features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../score-matching/score-matching.html">Estimation by score matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../svgd/svgd.html">Stein variational gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../num-sde/num-sde.html">Numerical simulation of SDEs</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/stratisMarkou/random-walks" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/stratisMarkou/random-walks/issues/new?title=Issue%20on%20page%20%2Fbook/papers/ais/ais.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/book/papers/ais/ais.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Annealed importance sampling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-sampling">Importance sampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-weighted-mcmc">Importance-weighted MCMC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Annealed Importance Sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#toy-experiment">Toy experiment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="annealed-importance-sampling">
<h1>Annealed importance sampling<a class="headerlink" href="#annealed-importance-sampling" title="Link to this heading">#</a></h1>
<script async defer src="https://buttons.github.io/buttons.js"></script>
<p><a class="github-button" href="https://github.com/stratisMarkou/random-walks" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="octicon-star" data-size="large" aria-label="Star stratisMarkou/random-walks on GitHub" style="float: right;">Star</a>
<a class="github-button" href="https://github.com/stratisMarkou/random-walks/issues" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="octicon-issue-opened" data-size="large" aria-label="Issue stratisMarkou/random-walks on GitHub">Issue</a>
<a class="github-button" href="https://github.com/stratisMarkou/random-walks/subscription" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="octicon-eye" data-size="large" aria-label="Watch stratisMarkou/random-walks on GitHub">Watch</a>
<a class="github-button" href="https://github.com/stratisMarkou" data-color-scheme="no-preference: light; light: light; dark: dark;" data-size="large" aria-label="Follow @stratisMarkou on GitHub">Follow</a></p>
<p>Simulating samples from distributions is a central problem in Statistics and Machine Learning, because it enables estimating important quantities such as integrals.
For example, we are often interested in evaluating integrals of the form</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I = \int p(x) f(x) dx,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is a probability density and <span class="math notranslate nohighlight">\(f\)</span> is a function of interest.
If we have acccess to the cumulative density function <span class="math notranslate nohighlight">\(p\)</span>, we can easily draw samples from it using <a class="reference external" href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inverse transform sampling</a>.
However, in most cases, the cumulative density function does not have an analytically tractable closed form, so sampling via the inverse transform does not apply.
For example, non-conjugate Bayesian models typically involve intractable distributions of this kind.
In such cases, we must resort to approximation methods, such as Monte Carlo.</p>
<p>A standard Monte Carlo method for handling intractable integrals is importance sampling.
Importance sampling gets around the intractability of <span class="math notranslate nohighlight">\(p\)</span> by introducing another tractable distribution <span class="math notranslate nohighlight">\(q\)</span> and drawing samples from <span class="math notranslate nohighlight">\(q\)</span> insted of <span class="math notranslate nohighlight">\(p\)</span>.
It then corrects for the bias in the samples, to account for the fact that these now come from <span class="math notranslate nohighlight">\(q\)</span> rather than <span class="math notranslate nohighlight">\(p\)</span>, by weighing them appropriately, using so called importance weights.
In this way, importance sampling yields an unbiased estimate of downstream integrals, while circumventing the intractability of <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>Unfortunately, if <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(p\)</span> are very dissimilar, the importance sampling estimator has a large random error because, as we will see, the variance in the random weights is large.
Annealed importance sampling<span id="id1">[<a class="reference internal" href="#id12" title="Radford M Neal. Annealed importance sampling. Statistics and computing, 11:125–139, 2001.">Neal, 2001</a>]</span> is a method which circumvents this issue by using an annealing procedure which produces samples with more equally distributed importance weights while remaining unbiased, and can greatly reduce the variance of the resulting estimates.</p>
<section id="importance-sampling">
<h2>Importance sampling<a class="headerlink" href="#importance-sampling" title="Link to this heading">#</a></h2>
<p>Suppose we wish to evaluate an integral of the form</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I = \int p(x) f(x) dx.
\end{equation}\]</div>
<p>If we could draw samples from <span class="math notranslate nohighlight">\(p\)</span>, we could estimate this integral by Monte Carlo, by computing</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I \approx \frac{1}{N} \sum_{n=1}^N f(x_n) ~~\text{ where }~~ x_n \sim p.
\end{equation}\]</div>
<p>However, in many applications of interest, we cannot draw samples from <span class="math notranslate nohighlight">\(p\)</span> directly.
Importance sampling gets around this by introducing another tractable distribution <span class="math notranslate nohighlight">\(q\)</span>, and rewriting this integral as</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I = \int q(x) \frac{p(x)}{q(x)} f(x) dx.
\end{equation}\]</div>
<p>A technical, but important, requirement here is that <span class="math notranslate nohighlight">\(q\)</span> should be non-zero whenever <span class="math notranslate nohighlight">\(p\)</span> is non-zero, for the integral above to be well defined.
Hereafter we assume this to be the case.
Since the distribution <span class="math notranslate nohighlight">\(q\)</span> is tractable, we can estimate the value of the integral by drawing samples from it and computing</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I \approx \frac{1}{N} \sum_{n=1}^N \underbrace{\frac{p(x_n)}{q(x_n)}}_{w_n} f(x_n) ~~\text{ where }~~ x_n \sim q.
\end{equation}\]</div>
<p>The ratios <span class="math notranslate nohighlight">\(w_n\)</span> are called importance weights, since they weigh the contribution of each <span class="math notranslate nohighlight">\(f(x_n)\)</span> term in the sum.
Since <span class="math notranslate nohighlight">\(q\)</span> proposes the samples to be used in the Monte Carlo sum, it is commonly referred to it as the proposal distribution.
We also refer to <span class="math notranslate nohighlight">\(p\)</span> as the target distribution, since this is the one we are ultimately interested in.
This is called the importance sampling estimator, and is unbiased, meaning that in expectation it is equal to the original integral</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\mathbb{E}\left[ \frac{1}{N} \sum_{n=1}^N \frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{1}{N} \sum_{n=1}^N \mathbb{E}\left[\frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{1}{N} \sum_{n=1}^N I = I.
\end{equation}\]</div>
<p>However, in practice we always end up using a finite number of samples, so the importance sampling estimate will not be exactly equal to <span class="math notranslate nohighlight">\(I\)</span>, but will be off by some random error, whose magnitude is captured by the variance of the estimator.
In particular, if the variance of the estimator using a single sample is <span class="math notranslate nohighlight">\(V\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
V = \mathbb{V}\left[\sum_{n=1}^N \frac{p(x_n)}{q(x_n)} f(x_n) \right] \geq 0,
\end{equation}\]</div>
<p>then using <span class="math notranslate nohighlight">\(N\)</span> samples reduces the variance by a factor of <span class="math notranslate nohighlight">\(N\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\mathbb{V}\left[\frac{1}{N}\sum_{n=1}^N \frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{1}{N^2}\mathbb{V}\left[\sum_{n=1}^N \frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{1}{N^2}\sum_{n=1}^N \mathbb{V}\left[\frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{V}{N}.
\end{equation}\]</div>
<p>Using more samples reduces the overall variance, but increases the computational cost.
A common issue that arises with the importance sampling estimator is that its variance, <span class="math notranslate nohighlight">\(V\)</span>, can be very large.
This means that lots of samples, and thus lots of computate, must be used to obtain a reasonable estimate.</p>
<p>Let’s look at a simple example of Importance Sampling.
Suppose <span class="math notranslate nohighlight">\(f(x) = x^3\)</span> and let <span class="math notranslate nohighlight">\(p\)</span> be a mixture of Gaussians</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
p(x) = \pi \mathcal{N}(x; \mu_1, \sigma_1^2) + (1 - \pi) \mathcal{N}(x; \mu_2, \sigma_2^2).
\end{equation}\]</div>
<p>Of course, we can in fact draw samples from a mixture of Gaussians directly, but let’s pretend we can’t.
Now, define <span class="math notranslate nohighlight">\(q\)</span> to be a Gaussian</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
q(x) = \mathcal{N}(x; \mu_q, \sigma_q^2).
\end{equation}\]</div>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/61b4bcfc457d58f160300ee7b72d808723f0b23b133d0b4be1b69c2ddd02b0e2.svg" src="../../../_images/61b4bcfc457d58f160300ee7b72d808723f0b23b133d0b4be1b69c2ddd02b0e2.svg" /></div>
</div>
<p>Using importance sampling, we can obtain an estimate of this intergral.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Importance Sampling estimate I = -5.40
</pre></div>
</div>
</div>
</div>
<p>Note that the exact value of the integral is <span class="math notranslate nohighlight">\(I = 0\)</span>, which we can see because <span class="math notranslate nohighlight">\(p(x)\)</span> is symmetric around <span class="math notranslate nohighlight">\(x = 0\)</span> while <span class="math notranslate nohighlight">\(f(x) = x^3\)</span> is antisymmetric.
However the estimate was <span class="math notranslate nohighlight">\(I \approx -5.46\)</span> which seems a bit off.
Repeating this experiment <span class="math notranslate nohighlight">\(100\)</span> times we obtain:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Importance Sampling over 100 trials I = 0.10 +/- 13.309.
</pre></div>
</div>
</div>
</div>
<p>So even though the estimator is unbiased, it has a large variance.
Before we go further, let’s compare the importance sampling estimate to a Monte Carlo estimate using samples directly from <span class="math notranslate nohighlight">\(p\)</span>.
Note that in practice this is not possible, because <span class="math notranslate nohighlight">\(p\)</span> is not in general available in closed form, but that’s why we picked <span class="math notranslate nohighlight">\(p\)</span> to be a mixture of Gaussians.
The point of this additional Monte Carlo estimate is to give us an idea of how much random error we should expect if we had access to exact samples from <span class="math notranslate nohighlight">\(p\)</span>, which we cannot expect to beat by simply reducing the variance in the importance weights.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Monte Carlo with samples from p over 100 trials I = -0.07 +/- 1.848.
</pre></div>
</div>
</div>
</div>
<p>The Importance sampling estimator has seven times larger random error than the Monte Carlo estimator.
To see why, this occurs, let’s plot the raw samples drawn from <span class="math notranslate nohighlight">\(q\)</span> (in green) as well as the samples weighted according to their importance weights (in blue).</p>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/22e61fb5414df871a0c287748ff9f0c3983a3ab0f86fee612f9f0c4f55584967.svg" src="../../../_images/22e61fb5414df871a0c287748ff9f0c3983a3ab0f86fee612f9f0c4f55584967.svg" /></div>
</div>
<p>First off, this plot illustrates how importance sampling works.
Although <span class="math notranslate nohighlight">\(q\)</span> proposes many samples in the middle, these are down-weighted by the importance weights, since <span class="math notranslate nohighlight">\(p\)</span> has small density there.
Note that the samples in the middle are still there in the blue histogram, but they have just been down-weighted by the importance weights.
Conversely, whenever <span class="math notranslate nohighlight">\(q\)</span> proposes a sample at a point where <span class="math notranslate nohighlight">\(p\)</span> has high density, the importance weight becomes much larger.
In this way, importance sampling moulds the empirical distribution of samples from <span class="math notranslate nohighlight">\(q\)</span> (in green) to resemble the target distribution <span class="math notranslate nohighlight">\(p\)</span> (in blue).</p>
<p>However, the samples which receive a high importance are relatively infrequent, which means that sometimes we may get sample with a large importance weight in one of the two Gaussian modes, but not in the other mode.
Since this sample has a large importance weight, it greatly affects the overall estimate, introducing lots of random error.
Looking at a histogram of the importance weights we see that most weights are very small, and it’s only a few large weights which dominate the value of the integral (note the <span class="math notranslate nohighlight">\(x\)</span>-axis has a log-scale).</p>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/b9923449337d83ac47f4e490788b83adf5d529a9f096ec628e3ca0d39a0d884c.svg" src="../../../_images/b9923449337d83ac47f4e490788b83adf5d529a9f096ec628e3ca0d39a0d884c.svg" /></div>
</div>
<p>Going a little further, note that the expectation of the value of an importance weight is always equal to one since</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\int q(x) \frac{p(x)}{q(x)} dx = 1 \implies \mathbb{E}_{x \sim q(x)}\left[\frac{p(x)}{q(x)} \right] = 1,
\end{equation}\]</div>
<p>This means that if <span class="math notranslate nohighlight">\(q\)</span> proposes samples with small importance weights most of the time, it must also propose some samples with very large importance weights to make up for the small importance weights: the integrand cannot always be smaller than <span class="math notranslate nohighlight">\(1\)</span> or always larger than <span class="math notranslate nohighlight">\(1\)</span>, otherwise it would not integrate to <span class="math notranslate nohighlight">\(1\)</span>.
The quantity which affects the amount of random error in an importance sampling estimator is the variability of the importance weights around this mean, that is the variance of the importance weights.
It is reasonable to expect that the more dissimilar <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(p\)</span> are, the larger the variance will be.
In partricular, we can show that the variance of the importance weights can be lower bounded by a quantity that scales exponentially with the KL divergence.</p>
<div class="proof lemma admonition" id="lemma-0">
<p class="admonition-title"><span class="caption-number">Lemma 51 </span> (Lower bound to importance weight variance)</p>
<section class="lemma-content" id="proof-content">
<p>Given distributions <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>, it holds that</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\mathbb{V}_{x \sim q}\left[\frac{p(x)}{q(x)}\right] \geq e^K - 1, \text{ where } K = \max\left(D_{KL}(p || q),~ 2 D_{KL}(q || p)\right) 
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(D_{KL}\)</span> denotes the Kullback-Leibler diverence, measured in nats.</p>
</section>
</div><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Derivation (Lower bound to importance weight variance)<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">The variance in the importance weights can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{V}_{x \sim q}\left[\frac{p(x)}{q(x)}\right] &amp;= \mathbb{E}_{x \sim q}\left[\left(\frac{p(x)}{q(x)}\right)^2\right] - \mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)}\right]^2, \\
&amp;= \mathbb{E}_{x \sim q}\left[\left(\frac{p(x)}{q(x)}\right)^2\right] - 1, \\
&amp;= \mathbb{E}_{x \sim p}\left[\frac{p(x)}{q(x)}\right] - 1, \\
\end{align}\end{split}\]</div>
<p class="sd-card-text">By applying Jensen’s inequality once, we can get a lower bound to the expectation above, to obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{V}_{x \sim q}\left[\frac{p(x)}{q(x)}\right] &amp;= \exp\left(\log\left(\mathbb{E}_{x \sim q}\left[\frac{p(x)^2}{q(x)^2}\right] \right)\right) - 1 \\
&amp;\geq \exp\left(\mathbb{E}_{x \sim p}\left[2 \log \frac{p(x)}{q(x)} \right]\right) - 1 \\
&amp;= e^{2 D_{KL}(q || p)} - 1,
\end{align}\end{split}\]</div>
<p class="sd-card-text">and similarly</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{V}_{x \sim q}\left[\frac{p(x)}{q(x)}\right] &amp;= \exp\left(\log\left( \mathbb{E}_{x \sim p}\left[\frac{p(x)}{q(x)}\right] \right)\right) - 1 \\
&amp;\geq \exp\left(\mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q(x)} \right]\right) - 1 \\
&amp;= e^{D_{KL}(p || q)} - 1,
\end{align}\end{split}\]</div>
<p class="sd-card-text">which is the lower bound in the lemma.</p>
</div>
</details><p>Note that when <span class="math notranslate nohighlight">\(q = p\)</span>, this lower bound becomes <span class="math notranslate nohighlight">\(0\)</span>.
This is in agreement with the fact that when <span class="math notranslate nohighlight">\(q = p\)</span>, all importance weights are equal to <span class="math notranslate nohighlight">\(1\)</span> and their variance is <span class="math notranslate nohighlight">\(0\)</span>.
As <span class="math notranslate nohighlight">\(q\)</span> becomes more and more dissimilar to <span class="math notranslate nohighlight">\(p\)</span>, <span class="math notranslate nohighlight">\(D_{KL}(q || p)\)</span> increases and so does the lower bound.
Therefore the variance also increases and in fact it increases at least exponentially with <span class="math notranslate nohighlight">\(D_{KL}(q || p)\)</span>.
Of course, this is only a lower bound, and the actual variance can be even larger than this.
On the flipside, this lower bound tells us that if we want to reduce the variance in the importance weights we must draw our samples from a proposal distribution which is as similar as possible with the target distribution, in the sense of having a small KL divergence.</p>
<p>This is where Annealed Importance Sampling (AIS) becomes useful.
AIS is an importance sampling method which uses an annealing procedure based on Markov Chain Monte Carlo (MCMC), to produce samples whose distribution is closer to <span class="math notranslate nohighlight">\(p\)</span>.
To achieve this, instead of using samples from <span class="math notranslate nohighlight">\(q\)</span> directly in the importance sampling estimator, AIS gradually transforms the samples, such that the distribution of the transformed samples is closer to <span class="math notranslate nohighlight">\(p\)</span>.
This reduces the variability in the importance samples and thus the random error in the importance sampling estimator.</p>
</section>
<section id="importance-weighted-mcmc">
<h2>Importance-weighted MCMC<a class="headerlink" href="#importance-weighted-mcmc" title="Link to this heading">#</a></h2>
<p>Motivated by the above intuition, given some initial samples from <span class="math notranslate nohighlight">\(q\)</span>, we want to transform them in a way such that the distribution of the transformed samples is as close as possible to <span class="math notranslate nohighlight">\(p\)</span>.
This would reduce the variance of the importance weights and thus the error in our estimator.
Markov Chain Monte Carlo (MCMC) is a standard class of methods geared towards solving this type of problem: an MCMC algorithm begins from an arbitrarily initialised distribution, and proceeds to transform this distribution acording to a randomised rule, such that the resulting distribution is closer to a target distribution.
So we could, in principle, use MCMC within an importance-weighted estimator to reduce its variance.
The following algorithm is based on this intuition.</p>
<div class="proof definition admonition" id="definition-1">
<p class="admonition-title"><span class="caption-number">Definition 124 </span> (Importance weighted MCMC algorithm)</p>
<section class="definition-content" id="proof-content">
<p>Given a proposal density <span class="math notranslate nohighlight">\(q\)</span>, a target density <span class="math notranslate nohighlight">\(p\)</span> and a sequence of transition kernels <span class="math notranslate nohighlight">\(T_1(x, x'), \dots, T_K(x, x')\)</span> be a sequence of transition kernels such that <span class="math notranslate nohighlight">\(T_k\)</span> leaves <span class="math notranslate nohighlight">\(p\)</span> invariant.
Sampling <span class="math notranslate nohighlight">\(x_0 \sim q(x)\)</span> followed by</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
x_k \sim T_k(x_{k-1}, x_k) ~\text{ for }~ k = 1, \dots, K,
\end{equation}\]</div>
<p>and return the sample <span class="math notranslate nohighlight">\(x_K\)</span> with an appropriately chosen importance weight <span class="math notranslate nohighlight">\(w\)</span> such that the resulting estimator is unbiased.</p>
</section>
</div><p>Note that the only requirement on the transition kernels <span class="math notranslate nohighlight">\(T_k(x, x')\)</span> is that they leave <span class="math notranslate nohighlight">\(p\)</span> invariant, and do not need to result in an ergodic Markov Chain, which is a usual requirement in standard MCMC.
The distribution which results after drawing an intial sample drawn from the proposal <span class="math notranslate nohighlight">\(q\)</span> and applying the transition kernels <span class="math notranslate nohighlight">\(T_1(x, x'), \dots, T_K(x, x')\)</span> can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
q_K(x_K) = \int q(x_0) T_1(x_0, x_1) \dots T_K(x_{K-1}, x_K) dx_0 \dots dx_{K-1},
\end{align}\]</div>
<p>and gets closer to <span class="math notranslate nohighlight">\(p\)</span> as we increase <span class="math notranslate nohighlight">\(K\)</span>.
Note that this algorithm does not specify how to select <span class="math notranslate nohighlight">\(w\)</span>.
However, while we could in principle draw <span class="math notranslate nohighlight">\(x_K \sim q_K\)</span> and return this sample together with the importance weight</p>
<div class="math notranslate nohighlight">
\[\begin{align}
w = \frac{p(x_K)}{q_K(x_K)},
\end{align}\]</div>
<p>each of the nested integrals above is intractable, which means we cannot compute <span class="math notranslate nohighlight">\(q_K\)</span> in closed form, and by extension we cannot compute <span class="math notranslate nohighlight">\(w\)</span> in closed form either.
One way to get around this issue, is to consider the joint distribution of <span class="math notranslate nohighlight">\((x_0, x_1, \dots, x_K)\)</span> under the Markov Chain, which has density</p>
<div class="math notranslate nohighlight">
\[\begin{align}
h(x_0, x_1, \dots, x_K) = q(x_0) T_1(x_0, x_1) \dots T_K(x_{K-1}, x_K).
\end{align}\]</div>
<p>We can then define the reverse transition kernels <span class="math notranslate nohighlight">\(\tilde{T}_k\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\tilde{T}_k(x, x') = T_k(x', x)\frac{p(x')}{p(x)}.
\end{align}\]</div>
<p>Because <span class="math notranslate nohighlight">\(T_k\)</span> leaves <span class="math notranslate nohighlight">\(p\)</span> invariant, it holds that</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\int \tilde{T}_k(x, x') dx' = \int T_k(x', x)\frac{p(x')}{p(x)} dx' = \frac{1}{p(x)}\int T_k(x', x)p(x') dx' = 1,
\end{align}\]</div>
<p>so the reverse kernels integrate to <span class="math notranslate nohighlight">\(1\)</span> and are also valid transition kernels.
We can now consider a reversed Markov chain which starts with the target distribution <span class="math notranslate nohighlight">\(p\)</span> as its intial distribution and applies the reverse kernels <span class="math notranslate nohighlight">\(\tilde{T}_k\)</span>.
This Markov chain has corresponding joint distribution</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\tilde{h}(x_0, x_1, \dots, x_K) = p(x_K) \tilde{T}_K(x_K, x_{K-1}) \dots \tilde{T}_1(x_1, x_0).
\end{align}\]</div>
<p>Now, consider performing importance sampling in this augmented space, with <span class="math notranslate nohighlight">\(h\)</span> as the proposal and <span class="math notranslate nohighlight">\(\tilde{h}\)</span> as the target distributions.
Specifically, we draw draw <span class="math notranslate nohighlight">\((x_{n, 0}, \dots, x_{n, K}) \sim h\)</span> for <span class="math notranslate nohighlight">\(n = 1, \dots, N\)</span> and compute the importance weight</p>
<div class="math notranslate nohighlight">
\[\begin{align}
w_n = \frac{\tilde{h}(x_{n, 0}, \dots, x_{n, K})}{h(x_{n, 0}, \dots, x_{n, K})}.
\end{align}\]</div>
<p>The importance weights <span class="math notranslate nohighlight">\(w_n\)</span> ensure that for any function <span class="math notranslate nohighlight">\(g\)</span> of the augmented sample, the importance weighted estimator</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\int g(x_0, \dots, x_K) \tilde{h}(x_0, \dots, x_K) dx_0 \dots dx_K \approx \frac{1}{N}\sum_{n=1}^N w_n g(x_{n, 0}, \dots, x_{n, K}),
\end{align}\]</div>
<p>is unbiased.
Therefore, if we set <span class="math notranslate nohighlight">\(g(x_{n, 0}, \dots, x_{n, K}) = f(x_{n, K})\)</span>, we obtain the estimator</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\int f(x_K) p(x_K) dx_K \approx \frac{1}{N}\sum_{n=1}^N w_n g(x_{n, 0}, \dots, x_{n, K}),
\end{align}\]</div>
<p>which is also unbiased.
Crucially, the importance weights <span class="math notranslate nohighlight">\(w_n\)</span> can now be computed in closed form.
Using the definition of the reverse transition kernels we have</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(x_K) \tilde{T}_K(x_K, x_{K-1}) \dots \tilde{T}_1(x_1, x_0) = p(x_0) T_1(x_0, x_1) \dots T_K(x_{K-1}, x_K).
\end{align}\]</div>
<p>and all terms coming from the transition kernels cancel in the importance weight ratio, yielding</p>
<div class="math notranslate nohighlight">
\[\begin{align}
w_n = \frac{\tilde{h}(x_{n, 0}, \dots, x_{n, K})}{h(x_{n, 0}, \dots, x_{n, K})} = \frac{p(x_0)}{q(x_0)}.
\end{align}\]</div>
<p>By performing importance sampling in this augmented space, we have got around the issue of intractable importance weights, by cancelling out a load of terms.
However, unfortunately these importance weight are exactly the same as the importance weights of the standard importance sampling estimator, so this algorithm does not improve on the variance of the standard estimator at all! However, it is possible to modify this algorithm to obtain better importance weights, while still taking advantage of the cancellation of the transition kernels.</p>
</section>
<section id="id2">
<h2>Annealed Importance Sampling<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>Given a sequnece <span class="math notranslate nohighlight">\(0 = \beta_0 \leq \dots \leq \beta_K = 1\)</span>, AIS introduces a sequence of distributions with densities</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\pi_k(x) = p(x)^{\beta_k} q(x)^{(1 - \beta_k)}.
\end{equation}\]</div>
<p>These distributions interpolate between <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(p\)</span> as we vary <span class="math notranslate nohighlight">\(\beta\)</span>.
AIS then proceeds in a similar way to the importance weighted MCMC algorithm we highlighted above, except that it requires that each <span class="math notranslate nohighlight">\(T_k\)</span> leaves <span class="math notranslate nohighlight">\(\pi_k\)</span>, instead of <span class="math notranslate nohighlight">\(p\)</span>, invariant.</p>
<div class="proof definition admonition" id="definition-2">
<p class="admonition-title"><span class="caption-number">Definition 125 </span> (Annealed Importance Sampling)</p>
<section class="definition-content" id="proof-content">
<p>Given a target density <span class="math notranslate nohighlight">\(p\)</span>, a proposal density <span class="math notranslate nohighlight">\(q\)</span> and a sequence <span class="math notranslate nohighlight">\(0 = \beta_0 \leq \dots \leq \beta_K = 1\)</span>, define</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\pi_k(x) = p(x)^{\beta_n} q(x)^{(1 - \beta_k)},
\end{equation}\]</div>
<p>and let <span class="math notranslate nohighlight">\(T_1(x, x'), \dots, T_K(x, x')\)</span> be a sequence of transition kernels such that <span class="math notranslate nohighlight">\(T_k\)</span> leaves <span class="math notranslate nohighlight">\(\pi_k\)</span> invariant.
Annealed Impoprtance Sampling amounts to drawing <span class="math notranslate nohighlight">\(x_0 \sim \pi_0(x)\)</span> followed by</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
x_k \sim T_k(x_{k-1}, x_k) ~\text{ for }~ k = 1, \dots, K,
\end{equation}\]</div>
<p>and return the sample <span class="math notranslate nohighlight">\(x_K\)</span> together with the importance weight</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
w = \frac{\pi_1(x_1)}{\pi_0(x_1)} \dots \frac{\pi_K(x_K)}{\pi_{K-1}(x_K)}.
\end{equation}\]</div>
</section>
</div><p>Note that drawing samples according to this algorithm and setting <span class="math notranslate nohighlight">\(g(x_{n, 0}, \dots, x_{n, K}) = f(x_K)\)</span> still results in an unbiased estimator of <span class="math notranslate nohighlight">\(I\)</span>.
This is because the augmented sample <span class="math notranslate nohighlight">\((x_{n, 0}, \dots, x_{n, K})\)</span> is distributed acccording to <span class="math notranslate nohighlight">\(h\)</span> whose marginal, after integrating out all but the last variable, is <span class="math notranslate nohighlight">\(q\)</span>.
Let’s implement this and see how it performs.</p>
<section id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Link to this heading">#</a></h3>
<p>To implement this procedure, we need to specify the transition kernels, and the sequence of annealing parameters <span class="math notranslate nohighlight">\(\beta_k\)</span>.
For the transition kernel, we will use a standard <a class="reference external" href="https://en.wikipedia.org/wiki/Metropolis-Hastings_algorithm">Metropolis-Hastings</a> transition kernel, which itself uses a Gaussian distribution as its proposal distribution (not to be confused with the initial proposal distribution <span class="math notranslate nohighlight">\(p\)</span>).
We’ll leave the <span class="math notranslate nohighlight">\(\beta_k\)</span> to be specified by the user.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransitionKernel</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">pass</span>

    
<span class="k">class</span> <span class="nc">GaussianTransitionKernel</span><span class="p">(</span><span class="n">TransitionKernel</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">distribution</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        
        <span class="c1"># Create forward proposal distribution and propose next point</span>
        <span class="n">forward</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">next_x</span> <span class="o">=</span> <span class="n">forward</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        
        <span class="c1"># Create reverse proposal distribution</span>
        <span class="n">reverse</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">next_x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        
        <span class="c1"># Compute acceptance probability</span>
        <span class="n">log_prob_1</span> <span class="o">=</span> <span class="n">forward</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">next_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">distribution</span><span class="p">(</span><span class="n">next_x</span><span class="p">)</span>
        <span class="n">log_prob_2</span> <span class="o">=</span> <span class="n">reverse</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">distribution</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">log_prob_ratio</span> <span class="o">=</span> <span class="n">log_prob_1</span> <span class="o">-</span> <span class="n">log_prob_2</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="n">log_prob_ratio</span><span class="p">]))</span>
        
        <span class="c1"># Accept reject step</span>
        <span class="n">accept</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span>
            <span class="p">[[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">p</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)]],</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        
        <span class="n">x_accept</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">next_x</span><span class="p">])[</span><span class="n">accept</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">x_accept</span><span class="p">,</span> <span class="n">accept</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We can then put together an <code class="docutils literal notranslate"><span class="pre">AnnealedImportanceSampler</span></code>, which accepts an initial and a target distribution, a transition kernel and a list containing a schedule for <span class="math notranslate nohighlight">\(\beta_k\)</span>, to perform AIS.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AnnealedImportanceSampler</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">initial_distribution</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
            <span class="n">target_distribution</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
            <span class="n">transition_kernel</span><span class="p">:</span> <span class="n">TransitionKernel</span><span class="p">,</span>
            <span class="n">betas</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">initial_distribution</span> <span class="o">=</span> <span class="n">initial_distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_distribution</span> <span class="o">=</span> <span class="n">target_distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transition_kernel</span> <span class="o">=</span> <span class="n">transition_kernel</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">betas</span> <span class="o">=</span> <span class="n">betas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_steps</span> <span class="o">=</span> <span class="n">betas</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        
        <span class="c1"># Draw samples from intial distribution</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">num_samples</span><span class="p">])</span>
        
        <span class="c1"># Run AIS chain on the initial samples</span>
        <span class="n">samples_and_log_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">map_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_chain</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x0</span><span class="p">,</span> <span class="n">samples_and_log_weights</span>

    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">jit_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">run_chain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        
        <span class="c1"># Initialise chain history and current distribution</span>
        <span class="n">chain_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="n">annealed_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_distribution</span><span class="o">.</span><span class="n">log_prob</span>
        
        <span class="c1"># Initialise log importance weight</span>
        <span class="n">log_w</span> <span class="o">=</span> <span class="o">-</span> <span class="n">annealed_log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_steps</span><span class="p">):</span>
            
            <span class="c1"># Create next annealed distribution</span>
            <span class="n">next_annealed_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_geometric_mixture</span><span class="p">(</span>
                <span class="n">beta</span><span class="o">=</span><span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="n">log_w</span> <span class="o">=</span> <span class="n">log_w</span> <span class="o">+</span> <span class="n">next_annealed_log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="c1"># Propose next point</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">accept</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transition_kernel</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                <span class="n">distribution</span><span class="o">=</span><span class="n">next_annealed_log_prob</span><span class="p">,</span>
            <span class="p">)</span>
            
            <span class="n">log_w</span> <span class="o">=</span> <span class="n">log_w</span> <span class="o">-</span> <span class="n">next_annealed_log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="n">annealed_log_prob</span> <span class="o">=</span> <span class="n">next_annealed_log_prob</span>
            <span class="n">chain_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
        <span class="n">log_w</span> <span class="o">=</span> <span class="n">log_w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">log_w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
            
        
    <span class="k">def</span> <span class="nf">log_geometric_mixture</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
        
        <span class="k">def</span> <span class="nf">_log_geometric_mixture</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
            
            <span class="n">log_prob_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">log_prob_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="p">((</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">log_prob_1</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">log_prob_2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">_log_geometric_mixture</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="toy-experiment">
<h3>Toy experiment<a class="headerlink" href="#toy-experiment" title="Link to this heading">#</a></h3>
<p>Now we have an AIS sampler which can be used with arbitrary annealing parameters <span class="math notranslate nohighlight">\(\beta_k\)</span>.
For this example, let’s set</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\beta_k = \frac{1}{1 + e^{\gamma_k}},
\end{equation}\]</div>
<p>where we start from large and negative values to large and positive values.
This gives <span class="math notranslate nohighlight">\(\beta_k\)</span> which gradually interpolate from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>

<span class="n">transition_scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Intialise transition kernel</span>
<span class="n">transition_kernel</span> <span class="o">=</span> <span class="n">GaussianTransitionKernel</span><span class="p">(</span>
    <span class="n">scale</span><span class="o">=</span><span class="n">transition_scale</span>
<span class="p">)</span>

<span class="c1"># Initialise betas</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="mf">10.</span> <span class="o">*</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">))</span>

<span class="c1"># Initialise AIS sampler</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">AnnealedImportanceSampler</span><span class="p">(</span>
    <span class="n">initial_distribution</span><span class="o">=</span><span class="n">q</span><span class="p">,</span>
    <span class="n">target_distribution</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
    <span class="n">transition_kernel</span><span class="o">=</span><span class="n">transition_kernel</span><span class="p">,</span>
    <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>Let’s draw some samples on the same problem we considered earlier, and visualise them as before.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot proposal</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">p_plot</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">)</span>

<span class="c1"># Plot target</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">q_plot</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:green&quot;</span><span class="p">)</span>

<span class="c1"># Plot initial samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">x0</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:green&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Initial samples from $q$&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Plot samples after samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:purple&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;AIS samples&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Plot samples weighted by their importance weights</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="n">w</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Weighted AIS samples&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;AIS samples&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$p(x),~ q(x)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">twin_axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">f_plot</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$f(x)$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$f(x)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">,</span> <span class="mf">60.</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/c6ed21f5373ab588271bd37bfd3faa43332da6c2e07fb10d33588f913c9dd0fb.svg" src="../../../_images/c6ed21f5373ab588271bd37bfd3faa43332da6c2e07fb10d33588f913c9dd0fb.svg" /></div>
</div>
<p>We observe that the distribution of the AIS samples (purple bars) is similar to the target distribution <span class="math notranslate nohighlight">\(p\)</span> (blue line).
Further, weighing the AIS samples by their importance weights (blue bars) does not appear to significantly change the resulting distribution.
This would suggest that moost of the importance weights are close to <span class="math notranslate nohighlight">\(1\)</span>, which we can verify by plotting them on a histogram.</p>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/43285dbf501f8f3f5ab1c8c5c8008a8a59e222c066ba4c6d2e960fa6a6a52d9a.svg" src="../../../_images/43285dbf501f8f3f5ab1c8c5c8008a8a59e222c066ba4c6d2e960fa6a6a52d9a.svg" /></div>
</div>
<p>As expected, most of the AIS importance weights are close to <span class="math notranslate nohighlight">\(1\)</span>.
Further, they have a clearly smaller variance than standard importance sampling.
This has a significant impact on the variance of AIS estimates.
In the running example integral we’ve been using thus far, AIS reduces the variance of our Monte Carlo estimate by a factor of <span class="math notranslate nohighlight">\(\approx 6\)</span> over standard importance sampling, and gets close to the variance of an estimator that uses direct i.i.d. samples from <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Annealed Importance Sampling over 100 trials I = 0.02 +/- 2.198.
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In importance sampling, using a proposal distribution that is significantly different from the target distribution results in a large variance of the resulting importance weights.
Large variance in the importance weights typically induces large variance in downstream estimates obtained using these weights.
AIS is a method which helps address this issue.
AIS introduces a sequence of annealed distributions, which interpolate between the target and proposal distribution, iteratively transforming samples from the proposal distribution using a sequence of Markov transition kernels which preserve the interpolating (annealed) distributions.
In this way, AIS typically reduces the variance in the importance weights, which in turn reduces the variance in downstream Monte Carlo estimates.
For more details, Radford Neal’s original paper introducing AIS<span id="id3">[<a class="reference internal" href="#id12" title="Radford M Neal. Annealed importance sampling. Statistics and computing, 11:125–139, 2001.">Neal, 2001</a>]</span> is a classic worth reading.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id4">
<div role="list" class="citation-list">
<div class="citation" id="id12" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Nea01<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>Radford M Neal. Annealed importance sampling. <em>Statistics and computing</em>, 11:125–139, 2001.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book/papers/ais"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../why-covariances/why-covariances.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Why covariance functions?</p>
      </div>
    </a>
    <a class="right-next"
       href="../rff/rff.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Random Fourier features</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-sampling">Importance sampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-weighted-mcmc">Importance-weighted MCMC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Annealed Importance Sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#toy-experiment">Toy experiment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Stratis Markou
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>