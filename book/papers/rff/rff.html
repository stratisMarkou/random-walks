

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Random Fourier features &#8212; Random Walks</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=927b94d3fcb96560df09" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=927b94d3fcb96560df09" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=927b94d3fcb96560df09" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=927b94d3fcb96560df09" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=927b94d3fcb96560df09" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=927b94d3fcb96560df09" />
  <script src="../../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=927b94d3fcb96560df09"></script>

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-HP14V4DGEF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-HP14V4DGEF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/papers/rff/rff';</script>
    <link rel="shortcut icon" href="../../../_static/dicefav.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Estimation by score matching" href="../score-matching/score-matching.html" />
    <link rel="prev" title="Annealed importance sampling" href="../ais/ais.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../intro.html">
  
  
  
  
  
    <p class="title logo__title">Random Walks</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Home
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes on books</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../toc/000-intro.html">Theory of Computation</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../toc/001-fsa.html">FSAs and Regular Expressions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../toc/002-cfl.html">PDAs and Context Free Grammars</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../toc/000-exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../prob-intro/intro.html">Probability: An introduction</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch01/content.html">Events and Probabilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch02/content.html">Discrete random variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch03/content.html">Multivariate discrete distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch04/content.html">Probability generating functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch05/content.html">Distribution and density functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch06/content.html">Multivariate distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch07/content.html">Moment generating functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../prob-intro/ch08/content.html">Main limit theorems</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mira/000-intro.html">Measure Theory</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mira/001-riemann.html">Riemann integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mira/002-measures.html">Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mira/000-exercises.html">Exercises</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Papers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../intro.html">Stream of papers</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../ais/ais.html">Annealed importance sampling</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Random Fourier features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../score-matching/score-matching.html">Estimation by score matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../svgd/svgd.html">Stein variational gradient descent</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/stratisMarkou/random-walks" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/stratisMarkou/random-walks/issues/new?title=Issue%20on%20page%20%2Fbook/papers/rff/rff.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/book/papers/rff/rff.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Random Fourier features</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rff-approximation">The RFF approximation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rff-and-bayesian-regression">RFF and Bayesian regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rates-of-convergence">Rates of convergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-the-prior">Sampling from the prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-with-rff-features">Regression with RFF features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-starvation">Variance starvation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="random-fourier-features">
<h1>Random Fourier features<a class="headerlink" href="#random-fourier-features" title="Permalink to this heading">#</a></h1>
<p>One central difficulty with Gaussian Processes (GPs), and more generally all kernel methods such as Support Vector Machines (SVMs), is their computational cost. Exact GP regression scales cubically <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> in the number of datapoints <span class="math notranslate nohighlight">\(N,\)</span> which is prohibitive for even modestly large datasets.
Therefore we typically have to make approximations, of which there is a wealth of possible options. This page presents Random Fourier Features (RFF), <span id="id1">[<a class="reference internal" href="#id11" title="Ali Rahimi, Benjamin Recht, and others. Random features for large-scale kernel machines. In NIPS. 2007.">Rahimi <em>et al.</em>, 2007</a>]</span> an approximation which is applicable to stationary kernels.</p>
<p>RFF relies on the fact that kernels of stationary processes can be expressed as the Fourier transform of a probability density function.
This Fourier transform can then be efficiently approximated by Monte Carlo.
The samples drawn to approximate this integral are called Random Fourier Features and can be used as features for a linear-in-the-parameters Bayesian regression model, which approximates exact GP regression.
The computational cost of Bayesian linear regression is <span class="math notranslate nohighlight">\(\mathcal{O}(\min\{N^3, M^3\})\)</span> where <span class="math notranslate nohighlight">\(M\)</span> is the number of regression features, so by using <span class="math notranslate nohighlight">\(M &lt; N\)</span> we can significantly reduce the cost of the algorithm.
Further, one can prove that the covariance function induced by RFF closely approximates that of the exact GP with a relatively small number of samples. Lastly, RFF is rather easy to implement in practice.</p>
<section id="the-rff-approximation">
<h2>The RFF approximation<a class="headerlink" href="#the-rff-approximation" title="Permalink to this heading">#</a></h2>
<p>The starting point for deriving RFF is Bochner’s theorem, which relates stationary kernels with probability distributions over frequencies via the Fourier transform.</p>
<div class="proof theorem admonition" id="theorem-0">
<p class="admonition-title"><span class="caption-number">Theorem 77 </span> (Bochner’s theorem)</p>
<section class="theorem-content" id="proof-content">
<p>A continuous function of the form <span class="math notranslate nohighlight">\(k(x, y) = k(x - y)\)</span> is positive definite if and only if <span class="math notranslate nohighlight">\(k(\delta)\)</span> is the Fourier transform of a non-negative measure.</p>
</section>
</div><p>Note that this statement slightly abuses the <span class="math notranslate nohighlight">\(k\)</span> symbol, using it to denote both the kernel <span class="math notranslate nohighlight">\(k(x, y)\)</span> as well as its writing in an explicitly translation-invariant form <span class="math notranslate nohighlight">\(k(x - y)\)</span> - the implied use is clear from context.
From Bochner’s theorem, we see that we can express any positive definite measure as the Fourier transform of a probability measure, rather than simply a non-negative measure, by introducing an appropriate scaling constant <span class="math notranslate nohighlight">\(\sigma^2\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
k(x - y) = \sigma^2 \int p(\omega) e^{-i \omega^\top (x - y)} d\omega = \sigma^2 \mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right].
\end{align}\]</div>
<p>Thus, we can get an unbiased estimate of <span class="math notranslate nohighlight">\(k(x - y)\)</span> by sampling <span class="math notranslate nohighlight">\(\omega \sim p(\omega)\)</span> and computing <span class="math notranslate nohighlight">\(\zeta_{\omega}(x)\zeta_{\omega}^*(y).\)</span>
Note however, that even though <span class="math notranslate nohighlight">\(\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right]\)</span> is real, the sampled <span class="math notranslate nohighlight">\(\zeta_{\omega}(x)\zeta_{\omega}^*(y)\)</span> will in general be complex.
This is an issue if we want to use <span class="math notranslate nohighlight">\(\zeta_{\omega}\)</span> to represent real functions. To resolve this issue, we can write the integral as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right] &amp;= \text{Re}\left[\mathbb{E}_{\omega}\left[e^{-i \omega^\top (x - y)}\right]\right] \\
                                                                     &amp;= \mathbb{E}_{\omega}\left[\text{Re}\left[e^{-i \omega^\top (x - y)}\right]\right] \\
                                                                     &amp;= \mathbb{E}_{\omega}\left[\cos(\omega^\top (x - y))\right],
\end{align}\end{split}\]</div>
<p>which has a real-valued integrand, so a Monte Carlo estimate of it will always be real valued.
Next, we would like to manipulate the expression above into an expectation of the form <span class="math notranslate nohighlight">\(\mathbb{E}_{\omega}\left[r_{\omega}(x)r_{\omega}(y)\right]\)</span> rather than <span class="math notranslate nohighlight">\(\mathbb{E}\left[r_{\omega}(x - y)\right]\)</span>.
In this way, we will be able to interpret this expectation as the covariance of a Bayesian regression model - this will be clarified shortly. Using the fact that</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}_{\phi}\left[\cos(z + n\phi)\right] = 0,
\end{align}\]</div>
<p>for all <span class="math notranslate nohighlight">\(z \in \mathbb{R}, n \in \mathbb{N}^+\)</span>, where <span class="math notranslate nohighlight">\(\phi \sim \text{Uniform}[0, 2\pi]\)</span>, we can re-write the expectation as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right] &amp;= \mathbb{E}_{\omega, \phi}\left[\cos\left(\omega^\top (x - y)\right) + \cos\left(\omega^\top (x + y\right) + 2b)\right] \\
                                                                     &amp;= \mathbb{E}_{\omega, \phi}\left[2 \cos\left(\omega^\top x + b\right) \cos\left(\omega^\top y + b\right)\right].
\end{align}\end{split}\]</div>
<p>We can therefore get an unbiased, real valued estimate of <span class="math notranslate nohighlight">\(k(x - y)\)</span> by sampling <span class="math notranslate nohighlight">\(\omega, \phi\)</span> and computing</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right] \approx z_{\omega, b}(x) z_{\omega, b}(y), \text{ where } z_{\omega, b}(x) = \sqrt{2} \cos(\omega^\top x + b).
\end{align}\]</div>
<p>In fact, we can go a bit further by drawing <span class="math notranslate nohighlight">\(M\)</span> independent pairs of <span class="math notranslate nohighlight">\(\omega, b\)</span> and computing the estimate</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right] \approx \frac{1}{M} \sum_{m = 1}^M z_{\omega_m, \phi_m}(x) z_{\omega_m, \phi_m}(y).
\end{align}\]</div>
<p>This is also an unbiased estimate of the kernel, however its variance is lower than in the <span class="math notranslate nohighlight">\(M = 1\)</span> case, since the variance of the average of the sum of <span class="math notranslate nohighlight">\(K\)</span> i.i.d. random variables is lower than the variance of a single one of the variables.
We therefore arrive at the following algorithm for estimating <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="proof definition admonition" id="definition-1">
<p class="admonition-title"><span class="caption-number">Definition 67 </span> (Random Fourier Features)</p>
<section class="definition-content" id="proof-content">
<p>Given a translation invariant kernel <span class="math notranslate nohighlight">\(k\)</span> that is the Fourier transform of a probability measure <span class="math notranslate nohighlight">\(p\)</span>, we have the unbiased real-valued estimator</p>
<div class="math notranslate nohighlight">
\[\begin{align}
k(x - y) \approx \frac{1}{M} \sum_{m = 1}^M z_{\omega_m, \phi_m}(x) z_{\omega_m, \phi_m}(y) = z^\top(x)z(y),
\end{align}\]</div>
<p>where we have used the notation <span class="math notranslate nohighlight">\(z(x) = \left[ z_{\omega_1, \phi_1}(x), ..., z_{\omega_M, \phi_M}(x) \right]^\top\)</span> and <span class="math notranslate nohighlight">\(\omega_m \sim p(\omega), \phi_m \sim \text{Uniform}[0, 2\pi]\)</span> are independent and identically distributed samples.</p>
</section>
</div><section id="rff-and-bayesian-regression">
<h3>RFF and Bayesian regression<a class="headerlink" href="#rff-and-bayesian-regression" title="Permalink to this heading">#</a></h3>
<p>Now we are in a position to interpret this expression in terms of a Bayesian regression model. Consider the regressor</p>
<div class="math notranslate nohighlight">
\[\begin{align}
f(x) = z^\top(x)w, \text{ where } w \sim \mathcal{N}(0, I),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(z(x)\)</span> is as defined above. This is a Bayesian regression model which uses Random Fourier Features. Therefore we can approximate the exact GP regressor by a Bayesian linear regressor with Random Fourier Features.</p>
</section>
<section id="rates-of-convergence">
<h3>Rates of convergence<a class="headerlink" href="#rates-of-convergence" title="Permalink to this heading">#</a></h3>
<p>Now there remains the question of how large the error of the RFF estimator is. In other words, how closely does RFF estimate the exact kernel <span class="math notranslate nohighlight">\(k\)</span>? Since <span class="math notranslate nohighlight">\(-\sqrt{2} \leq z_{\omega, \phi} \leq \sqrt{2}\)</span>, we can use Hoeffding’s inequality<span id="id2">[<a class="reference internal" href="#id10" title="David Grimmett, Geoffrey Stirzaker. Probability and random processes. Oxford university press, 2020.">Grimmett, 2020</a>]</span> to obtain the following high-probability bound on the absolute error on our estimate of <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="proof lemma admonition" id="lemma-2">
<p class="admonition-title"><span class="caption-number">Lemma 7 </span> (Hoeffding for RFF)</p>
<section class="lemma-content" id="proof-content">
<p>The RFF estimator of <span class="math notranslate nohighlight">\(k\)</span>, using <span class="math notranslate nohighlight">\(M\)</span> pairs of <span class="math notranslate nohighlight">\(\omega, \phi\)</span>, obeys</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p\big(|z^\top(x)z(y) - k(x, y)| \geq \epsilon \big) \leq 2 \exp\left(-M \frac{\epsilon^2}{4}\right).
\end{align}\]</div>
</section>
</div><p>Therefore for any given input pair, the error of the RFF estimator decays exponentially quickly with <span class="math notranslate nohighlight">\(M\)</span>.
Note that this is a statement about the closeness of <span class="math notranslate nohighlight">\(z^\top(x)z(y)\)</span> and <span class="math notranslate nohighlight">\(k(x, y)\)</span> for any two input pairs, rather than the closeness of these functions over the whole input space.
In fact, it is possible<span id="id3">[<a class="reference internal" href="#id11" title="Ali Rahimi, Benjamin Recht, and others. Random features for large-scale kernel machines. In NIPS. 2007.">Rahimi <em>et al.</em>, 2007</a>]</span> to make a stronger statement about the uniform convergence of the estimator.</p>
<div class="proof lemma admonition" id="lemma-3">
<p class="admonition-title"><span class="caption-number">Lemma 8 </span> (Uniform convergence of RFF)</p>
<section class="lemma-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> be a compact subset of <span class="math notranslate nohighlight">\(\mathbb{R}^D\)</span>. Then the RFF estimator of <span class="math notranslate nohighlight">\(k\)</span>, using <span class="math notranslate nohighlight">\(M\)</span> pairs of <span class="math notranslate nohighlight">\(\omega, \phi\)</span> converges uniformly to <span class="math notranslate nohighlight">\(k\)</span> according to</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p~\Bigg(\sup_{x, y \in \mathcal{M}}|z^\top(x)z(y) - k(x, y)| \geq \epsilon \Bigg) \leq \frac{C}{\epsilon^2} \exp\left(-\frac{M\epsilon^2}{4(d + 2)}\right).
\end{align}\]</div>
</section>
</div></section>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">#</a></h2>
<p>We now turn to an implementation of RFF.
We consider approximating the same three kernels which were presented in the RFF paper, namely the EQ (or Gaussian) kernel, the Laplace kernel and the Cauchy kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample_rff</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">,</span>
    <span class="n">lengthscale</span><span class="p">,</span>
    <span class="n">coefficient</span><span class="p">,</span>
    <span class="n">num_functions</span><span class="p">,</span>
    <span class="n">num_features</span><span class="p">,</span>
<span class="p">):</span>
    
    <span class="c1"># Dimension of data space</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">omega_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_functions</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">)</span>
    
    <span class="c1"># Handle each of three possible kernels separately</span>
    <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;eq&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">omega_shape</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;laplace&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_cauchy</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">omega_shape</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;cauchy&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">laplace</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">omega_shape</span><span class="p">)</span>
        
    <span class="c1"># Scale omegas by lengthscale</span>
    <span class="n">omega</span> <span class="o">=</span> <span class="n">omega</span> <span class="o">/</span> <span class="n">lengthscale</span>
    
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_functions</span><span class="p">,</span> <span class="n">num_features</span><span class="p">),</span>
    <span class="p">)</span>
    
    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
        <span class="n">low</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
        <span class="n">high</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">),</span>
        <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_functions</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>
    
    <span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sfd, nd -&gt; sfn&#39;</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">phi</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">num_features</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">features</span> <span class="o">*</span> <span class="n">coefficient</span>
    
    <span class="n">functions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sf, sfn -&gt; sn&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">functions</span><span class="p">,</span> <span class="n">features</span>
</pre></div>
</div>
</div>
</div>
<section id="sampling-from-the-prior">
<h3>Sampling from the prior<a class="headerlink" href="#sampling-from-the-prior" title="Permalink to this heading">#</a></h3>
<p>We can now visualise functions sampled from the prior induced by RFF.
Each function corresponds to a different sample set <span class="math notranslate nohighlight">\(\{(\omega_m, \phi_m)\}_{m = 1}^M\)</span> as well as a different weight vector <span class="math notranslate nohighlight">\(w\)</span>.
The covariance function plots correspond to the RFF estimator of <span class="math notranslate nohighlight">\(k\)</span>, using a single sample of RFF parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Kernels to approximate</span>
<span class="n">kernels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;eq&#39;</span><span class="p">,</span> <span class="s1">&#39;laplace&#39;</span><span class="p">,</span> <span class="s1">&#39;cauchy&#39;</span><span class="p">]</span>

<span class="c1"># Kernel parameters, # function samples, # features for each function</span>
<span class="n">lengthscale</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">coefficient</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">num_functions</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Input locations</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># Draw random fourier features and functions</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">sample_rff</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> 
        <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> 
        <span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">,</span> 
        <span class="n">coefficient</span><span class="o">=</span><span class="n">coefficient</span><span class="p">,</span> 
        <span class="n">num_functions</span><span class="o">=</span><span class="n">num_functions</span><span class="p">,</span> 
        <span class="n">num_features</span><span class="o">=</span><span class="n">num_features</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/a66bf1c66c5de9536b10c2a60a069074322b607491e53585c193c530e982a9b7.svg" src="../../../_images/a66bf1c66c5de9536b10c2a60a069074322b607491e53585c193c530e982a9b7.svg" /></div>
</div>
</section>
<section id="regression-with-rff-features">
<h3>Regression with RFF features<a class="headerlink" href="#regression-with-rff-features" title="Permalink to this heading">#</a></h3>
<p>First we generate a toy dataset from a noisy sinusoid.
We place a gap in the data to observe the quality of the uncertainty estimates of the RFF regressor.
We purposefully use a large number of datapoints, <span class="math notranslate nohighlight">\(N = 5000,\)</span> which would be quite slow to process with an exact GP model.</p>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/3499941b0dddeba4af3def1e73d620006a4fb4a889d2c0a8a494492c42817d53.svg" src="../../../_images/3499941b0dddeba4af3def1e73d620006a4fb4a889d2c0a8a494492c42817d53.svg" /></div>
</div>
<p>The function <code class="docutils literal notranslate"><span class="pre">rff_posterior</span></code> below implements Bayesian linear regression with randomly sampled Fourier features.
For more details on Bayesian linear regression see Chapter 3 of Bishop’s PRML book.<span id="id4">[<a class="reference internal" href="#id12" title="Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.">Bishop, 2006</a>]</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rff_posterior</span><span class="p">(</span>
    <span class="n">x_data</span><span class="p">,</span>
    <span class="n">y_data</span><span class="p">,</span>
    <span class="n">x_pred</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">,</span>
    <span class="n">lengthscale</span><span class="p">,</span>
    <span class="n">coefficient</span><span class="p">,</span>
    <span class="n">num_features</span><span class="p">,</span>
    <span class="n">noise</span><span class="p">,</span>
<span class="p">):</span>
    
    <span class="n">num_data</span> <span class="o">=</span> <span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">x_data</span><span class="p">])</span>
    
    <span class="n">_</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">sample_rff</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x_full</span><span class="p">,</span> 
        <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> 
        <span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">,</span> 
        <span class="n">coefficient</span><span class="o">=</span><span class="n">coefficient</span><span class="p">,</span> 
        <span class="n">num_functions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
        <span class="n">num_features</span><span class="o">=</span><span class="n">num_features</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">features_pred</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="n">num_data</span><span class="p">]</span>
    <span class="n">features_data</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span> <span class="o">-</span><span class="n">num_data</span><span class="p">:]</span>
    
    <span class="n">iS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">features_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> \
        <span class="n">features_data</span> <span class="o">@</span> <span class="n">features_data</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">noise</span> <span class="o">**</span> <span class="o">-</span><span class="mi">2</span>

    <span class="n">mean_pred</span> <span class="o">=</span> <span class="n">noise</span> <span class="o">**</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> \
        <span class="n">features_pred</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">iS</span><span class="p">,</span> <span class="n">features_data</span> <span class="o">@</span> <span class="n">y_data</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
    
    <span class="n">var_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
        <span class="s1">&#39;fn, fn -&gt; n&#39;</span><span class="p">,</span>
        <span class="n">features_pred</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">iS</span><span class="p">,</span> <span class="n">features_pred</span><span class="p">),</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">mean_pred</span><span class="p">,</span> <span class="n">var_pred</span>
</pre></div>
</div>
</div>
</div>
<p>Now we perform Bayesian linear regression with RFF features for each of the three kernels.
The two helpers below, implement exact GP regression.
The <code class="docutils literal notranslate"><span class="pre">covariance</span></code> helper implements the EQ, Laplace and Cauchy covariance functions and the <code class="docutils literal notranslate"><span class="pre">exact_gp_posterior</span></code> helper performs exact GP regression. <span id="id5">[<a class="reference internal" href="#id14" title="Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on machine learning, 63–71. Springer, 2003.">Rasmussen, 2003</a>]</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">covariance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    
    <span class="n">diff</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">x2</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
    
    <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;eq&#39;</span><span class="p">:</span>
        <span class="n">l2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">diff</span> <span class="o">/</span> <span class="n">lengthscale</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">coefficient</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">l2</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;laplace&#39;</span><span class="p">:</span>
        <span class="n">l1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span> <span class="o">/</span> <span class="n">lengthscale</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">coefficient</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">l1</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;cauchy&#39;</span><span class="p">:</span>
        <span class="n">l2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">diff</span> <span class="o">/</span> <span class="n">lengthscale</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">coefficient</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">l2</span><span class="p">)</span>
        
    <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">cov</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">cov</span>


<span class="k">def</span> <span class="nf">exact_gp_posterior</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
    
    <span class="n">Kdd</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
    <span class="n">Kpd</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">Kpp</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    
    <span class="n">mean</span> <span class="o">=</span> <span class="n">Kpd</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Kdd</span><span class="p">,</span> <span class="n">y_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Kpp</span> <span class="o">-</span> <span class="n">Kpd</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Kdd</span><span class="p">,</span> <span class="n">Kpd</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span>
</pre></div>
</div>
</div>
</div>
<p>Timing the computation shows that the RFF implementation takes miliseconds to execute, as opposed to exact GP regression which takes much longer.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Timing exact GP regression:
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 13 s, sys: 1.82 s, total: 14.8 s
Wall time: 6.35 s
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Timing exact approximate RFF regression:

CPU times: user 231 ms, sys: 123 ms, total: 353 ms
Wall time: 90.6 ms
</pre></div>
</div>
</div>
</div>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/43ca4bb7d92611f8932c5369f664c54dcb3a2ff3037c6d5dc726f05741662ed9.svg" src="../../../_images/43ca4bb7d92611f8932c5369f664c54dcb3a2ff3037c6d5dc726f05741662ed9.svg" /></div>
</div>
<p>RFF has produced sensible regressors in each case, significantly faster than exact GP regression. The approximate posteriors roughly match the exact posteriors, while being significantly quicker to compute.</p>
</section>
<section id="variance-starvation">
<h3>Variance starvation<a class="headerlink" href="#variance-starvation" title="Permalink to this heading">#</a></h3>
<p>One issue with the RFF is that - like all other finte basis function models - is that it has a limited amount of degrees of freedom. Therefore, in some situations the datapoints may be such that they pin down the RFF model and significantly reducing the variance of the approximate regressor. To illustrate this, we sample a slightly different dataset, with a smaller gap in between the two data clumps.</p>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/7a48303d5a8a4e2fb0b08f923265b2381bbf96641d665fa1ba10286cc96260e1.svg" src="../../../_images/7a48303d5a8a4e2fb0b08f923265b2381bbf96641d665fa1ba10286cc96260e1.svg" /></div>
</div>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/0247bc912a8f5921659f8a1e36c382fd1663c0619b2e3e7d7f497f4c41b0b274.svg" src="../../../_images/0247bc912a8f5921659f8a1e36c382fd1663c0619b2e3e7d7f497f4c41b0b274.svg" /></div>
</div>
<p>The variance of the approximate estimator in between the data is (in some cases), signiticantly smaller than that of the exact posterior. So the speedup that the RFF gives does not come without a cost. In certain cases, we can end up with approximate posteriors which are significantly overfitted. This can be alleviated by increasing the number of RFF features. However this increases the computational cost of performing regression and may defeat the purpose of using RFF features in the first place.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>Random Fourier Features are a cheap and efficient way to sample from a Gaussian Process (GP) prior. By using a finite number of features <span class="math notranslate nohighlight">\(M\)</span>, smaller than the number of datapoints <span class="math notranslate nohighlight">\(N\)</span>, allows us to perform approximate GP regression, reducing the computational complexity from <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(NM^2)\)</span>, where <span class="math notranslate nohighlight">\(M\)</span> is the number of random features. Some interesting recent work<span id="id6">[<a class="reference internal" href="#id13" title="James T. Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Peter Deisenroth. Efficiently sampling functions from gaussian process posteriors. 2020.">Wilson <em>et al.</em>, 2020</a>]</span> combines RFF with sparse GP approximations to produce approximate GP regressors which are both cheap to train as well as to sample form their posteriors.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<div class="docutils container" id="id7">
<dl class="citation">
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id4">Bis06</a></span></dt>
<dd><p>Christopher M Bishop. <em>Pattern recognition and machine learning</em>. springer, 2006.</p>
</dd>
<dt class="label" id="id10"><span class="brackets"><a class="fn-backref" href="#id2">Gri20</a></span></dt>
<dd><p>David Grimmett, Geoffrey Stirzaker. <em>Probability and random processes</em>. Oxford university press, 2020.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">RR+07</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>Ali Rahimi, Benjamin Recht, and others. Random features for large-scale kernel machines. In <em>NIPS</em>. 2007.</p>
</dd>
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id5">Ras03</a></span></dt>
<dd><p>Carl Edward Rasmussen. Gaussian processes in machine learning. In <em>Summer school on machine learning</em>, 63–71. Springer, 2003.</p>
</dd>
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id6">WBT+20</a></span></dt>
<dd><p>James T. Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Peter Deisenroth. Efficiently sampling functions from gaussian process posteriors. 2020.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book/papers/rff"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../ais/ais.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Annealed importance sampling</p>
      </div>
    </a>
    <a class="right-next"
       href="../score-matching/score-matching.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Estimation by score matching</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rff-approximation">The RFF approximation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rff-and-bayesian-regression">RFF and Bayesian regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rates-of-convergence">Rates of convergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-the-prior">Sampling from the prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-with-rff-features">Regression with RFF features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-starvation">Variance starvation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Stratis Markou
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=927b94d3fcb96560df09"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=927b94d3fcb96560df09"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>