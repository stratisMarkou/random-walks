
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Variational Gaussian Processes &#8212; Random walks</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://random-walks.org/content/gp/sparse/vfe.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Efficiently sampling GP posteriors" href="gp-sampling.html" />
    <link rel="prev" title="Sparse Gaussian Processes" href="sparse-intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://random-walks.org/content/gp/sparse/vfe.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Variational Gaussian Processes" />
<meta property="og:description" content="Variational Gaussian Processes  The Variational Free Energy (VFE) method [Tit09] applied to GPs, is an approach for approximating the posterior p(f | \mathbf{y}" />
<meta property="og:image"       content="https://random-walks.org/_static/logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Random walks</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../home.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../prob-intro/intro.html">
   Probability: An introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch01/content.html">
     Events and Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch02/content.html">
     Discrete random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch03/content.html">
     Multivariate discrete distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch04/content.html">
     Probability generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch05/content.html">
     Distribution and density functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch06/content.html">
     Multivariate distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch07/content.html">
     Moment generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch08/content.html">
     Main limit theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch09/content.html">
     Branching processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch10/content.html">
     Random walks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch11/content.html">
     Processes in continuous time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch12/content.html">
     Markov chains
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../cvx/intro.html">
   Convex optimisation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch01.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch02.html">
     Convex sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch03.html">
     Convex functions
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../gp-intro.html">
   Gaussian Processes
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../why-covariances.html">
     Why covariance functions?
    </a>
   </li>
   <li class="toctree-l2 current active collapsible-parent">
    <a class="reference internal" href="sparse-intro.html">
     Sparse Gaussian Processes
    </a>
    <ul class="current collapse-ul">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Variational Free Energy GPs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="gp-sampling.html">
       Sampling GP posteriors
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../misc/misc.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/sde/num-sde.html">
     Numerical simulation of SDEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/sde-as-gp/sde-as-gp.html">
     VI for diffusion processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/node/node.html">
     Neural ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/optimisation/conjugate-gradients.html">
     Conjugate gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/kalman/kalman.html">
     The Kalman filter and smoother
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/ncs/ncs.html">
     Natural cubic splines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/ars/ars.html">
     Adaptive rejection sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/score-matching/score-matching.html">
     Estimation by score matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/interacting/interacting.html">
     Interacting particle FPK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/rff/rff.html">
     Random Fourier features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/svgd/svgd.html">
     Stein variational gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/addgp/addgp.html">
     Additive Gaussian Processes
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reading-and-links.html">
   Interesting reading and websites
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/gp/sparse/vfe.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/stratisMarkou/random-walks/master?urlpath=tree/./content/gp/sparse/vfe.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-variational-free-energy">
   The Variational Free Energy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sparse-vfe-approximation">
   Sparse VFE approximation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-and-approximate-q">
     Model and approximate
     <span class="math notranslate nohighlight">
      \(q\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximising-the-vfe">
     Maximising the VFE
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior-predictive">
     Posterior predictive
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cost-and-quality">
     Cost and quality
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampling-the-exact-gp">
     Sampling the exact GP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-the-vfe-gp">
     Implementing the VFE GP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-inducing-points">
     Learning inducing points
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#complete-learning">
     Complete learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="variational-gaussian-processes">
<h1>Variational Gaussian Processes<a class="headerlink" href="#variational-gaussian-processes" title="Permalink to this headline">¶</a></h1>
<p>The Variational Free Energy (VFE) method <a class="bibtex reference internal" href="#titsias2009variational" id="id1">[Tit09]</a> applied to GPs, is an approach for approximating the posterior <span class="math notranslate nohighlight">\(p(f | \mathbf{y}, \mathbf{X})\)</span> of a Gaussian Process. Instead of computing the exact GP posterior, VFE approximates it by another Gaussian distribution, which is cheaper to manipulate when making predictions or evaluating the marginal likelihood <span class="math notranslate nohighlight">\(p(\mathbf{y} | \mathbf{X})\)</span>. Similarly to other GP approximations, VFE achieves lower a computational cost by making sparsity assumptions. However, whereas other sparse methods like FITC and DTC make sparsity assumptions on the <em>likelihood</em>, VFE makes these assumptions on the <em>approximate posterior</em> and this has several important effects. <span class="math notranslate nohighlight">\( \def\Kxx{\mathbf{K}_{\mathbf{X}\mathbf{X}}}
 \def\Kxb{\mathbf{K}_{\mathbf{X}\mathbf{\bar{X}}}}
 \def\Kbx{\mathbf{K}_{\mathbf{\bar{X}}\mathbf{X}}}
 \def\Kbb{\mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}}
 \def\Ksb{\mathbf{K}_{\mathbf{X^*}\mathbf{\bar{X}}}}
 \def\Kbs{\mathbf{K}_{\mathbf{\bar{X}}\mathbf{X^*}}}
 \def\Ksx{\mathbf{K}_{\mathbf{X^*}\mathbf{\bar{X}}}}
 \def\Kxs{\mathbf{K}_{\mathbf{\bar{X}}\mathbf{X^*}}}
 \def\Kss{\mathbf{K}_{\mathbf{X^*}\mathbf{X^*}}}
 \def\fx{\mathbf{f}_{\mathbf{X}}}
 \def\fnb{f_{\neq \mathbf{\bar{X}}}}
 \def\fb{\mathbf{f}_{\mathbf{\bar{X}}}}
 \def\fstar{\mathbf{f}_{\mathbf{X^*}}}
 \def\X{\mathbf{x}}
 \def\xstar{\mathbf{x^*}}
 \def\X{\mathbf{X}}
 \def\Xb{\mathbf{\bar{X}}}
 \def\lrb[#1]{\left(#1\right)}
 \def\lrs[#1]{\left[#1\right]}
 \def\mb[#1]{\mathbf{#1}}
 \DeclareMathOperator*{\argmax}{arg\,max}
 \DeclareMathOperator*{\argmin}{arg\,min}
 \newcommand{\bs}[1]{\boldsymbol{#1}}
 \newcommand{\bm}[1]{\mathbf{#1}} \)</span></p>
<div class="section" id="the-variational-free-energy">
<h2>The Variational Free Energy<a class="headerlink" href="#the-variational-free-energy" title="Permalink to this headline">¶</a></h2>
<p>Often, the log marginal likelihood of the data can be costly or intractable to compute. To circumvent this difficulty, one solution is to optimise an alternative objective instead of the log-marginal. This objective should be chosen such that (1) it is cheap to evaluate; (2) optimising it cannot lead to worse overfitting than the original model. The Variational Free Energy (VFE), also known as the ELBO, can be used to meet both these criteria. The precise same bound is used for a variety of other models involving latent variables - including Gassian Mixture Models, Variational Autoencoders or Bayesian Networks to mention a few.</p>
<p>The VFE meets the two criteria above because (1) it can be cheap to evaluate if we make sensible approximation choices; (2) it lower bounds the marginal likelihood, so we can optimise it with respect to the hyperparameters without fear of overfitting any worse than the exact model would. The latter point is especially important because it decouples modelling assumptions (the awesome model we’d like to have) from our approximations (which we make out of computational necessity). A user can therefore state their assumptions clearly upfornt and buy themselves as good an approximation to the exact model as their comptational budget can afford them. We’ll expand on this further, later.</p>
<p>Here is a short derivation for the VFE, which can be used as a starting point for <em>any</em> latent variable model. Suppose we want to evalate the marginal likelihood of some data <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> given some other data <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> under a model with latent variables <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> and parameters <span class="math notranslate nohighlight">\(\bs{\theta}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(\mathbf{y} | \mathbf{x}, \bs{\theta}) = \int p(\mathbf{y}, \mathbf{s} | \mathbf{x}, \bs{\theta}) d \mathbf{s}
\end{align}\]</div>
<p>For Gaussian mixture models, <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> are cluster memberships of the datapoints, often written <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>. For VAEs, <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> is the latent representation, often written <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. For GPs it will denote a set of latent function values <span class="math notranslate nohighlight">\(\mathbf{f}\)</span>. Now, defining a new probability distribution over <span class="math notranslate nohighlight">\(\mathbf{s}\)</span>, written <span class="math notranslate nohighlight">\(q(\mathbf{s})\)</span>. This is widely referred to as the <em>variational posterior</em>. If we subtract the KL divergence between <span class="math notranslate nohighlight">\(q(\mathbf{s})\)</span> and the true posterior <span class="math notranslate nohighlight">\(p(\mathbf{s} | \mathbf{y}, \mathbf{x})\)</span> from the marginal likelihood, we get the inequality</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\log p(\mathbf{y} | \mathbf{x}) \geq \mathcal{F}(q, \bs{\theta}) &amp;= \int q(\mathbf{s}) \log p(\mathbf{y} | \mathbf{x}, \bs{\theta}) d \mathbf{s} + \int q(\mathbf{s}) \log \frac{p(\mathbf{s} | \mathbf{y}, \mathbf{x}, \bs{\theta})}{q(\mathbf{s})} d\bm{s} \\
&amp;= \int q(\mathbf{s}) \log \frac{p(\mathbf{y}, \mathbf{s} | \mathbf{x}, \bs{\theta})}{q(\mathbf{s})} d\bm{s},
\end{align}\end{split}\]</div>
<p>where we have used the fact that the KL is non-negative. This shows that the VFE, <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, is a lower bound to the exact marginal likelihood. We made no assumptions about the distribution <span class="math notranslate nohighlight">\(q\)</span> and the inequality always holds, irrespective of our choice of <span class="math notranslate nohighlight">\(q\)</span>. The inequality becomes an equality when <span class="math notranslate nohighlight">\(q\)</span> is equal to the true posterior, where the KL divergence becomes zero, attaining its minimum value. Although we could pick <span class="math notranslate nohighlight">\(q\)</span> to be any distribution we like, we should seek a <span class="math notranslate nohighlight">\(q\)</span> which makes <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> as large as possible while still remaining computationally tractable. It turns out that the VFE approximation for GPs achieves an excellent tradeoff between approximating the true posterior and keeping the computational cost low.</p>
</div>
<div class="section" id="sparse-vfe-approximation">
<h2>Sparse VFE approximation<a class="headerlink" href="#sparse-vfe-approximation" title="Permalink to this headline">¶</a></h2>
<p>Using the idea of introducing inducing points, much like those of the  DTC<a class="bibtex reference internal" href="#seeger2003fast" id="id2">[SWL03]</a> or FITC<a class="bibtex reference internal" href="#snelson2005sparse" id="id3">[SG05]</a> approximations, we will see how the VFE approximation will result in an approximate form which is significantly cheaper to work with than the exact GP. Unlike the DTC or FITC however, VFE approximates the posterior over the latent function <span class="math notranslate nohighlight">\(f\)</span> rather than the model. Whereas models like DTC or FITC can overfit when optimising their inducing points and model parameters <span class="math notranslate nohighlight">\(\theta\)</span>, the VFE approximation does not run this risk at all because it uses the Free Energy to optimise inducing points and parameters. Since the Free Energy is a lower bound to the exact log-marginal likelihood, VFE is guaranteed not to overfit any more than the exact GP would.</p>
<div class="section" id="model-and-approximate-q">
<h3>Model and approximate <span class="math notranslate nohighlight">\(q\)</span><a class="headerlink" href="#model-and-approximate-q" title="Permalink to this headline">¶</a></h3>
<p>Let’s start by stating our assumptions and inference approximations. First, define the input variables <span class="math notranslate nohighlight">\(\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_N)^\top\)</span> and the values of the function <span class="math notranslate nohighlight">\(f\)</span> at these input locations as <span class="math notranslate nohighlight">\(\mathbf{f}_\mathbf{X} = (f_{\mathbf{x}_1}, f_{\mathbf{x}_2}, ..., f_{\mathbf{x}_N})^\top\)</span>, placing a zero-mean GP prior over them, so that</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(\mathbf{f}_\mathbf{X} |\mathbf{X}, \bs{\theta}) \sim \mathcal{N}\lrb[\mathbf{0}, \bm{K}_{\bm{X}\bm{X}}].
\end{align}\]</div>
<p>Now define the observed variables <span class="math notranslate nohighlight">\(\mathbf{y} = (y_1, y_2, ..., y_N)^\top\)</span>, obtained by adding noise to <span class="math notranslate nohighlight">\(\mathbf{f}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(\mathbf{y}| \mathbf{f}_\mathbf{X}) \sim \mathcal{N}\lrb[\mathbf{f}_\mathbf{X}, \sigma^2 \bm{I}],
\end{align}\]</div>
<p>so that the marginal likelihood of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> given <span class="math notranslate nohighlight">\(\bm{X}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(\mathbf{y}| \bs{\theta}) \sim \mathcal{N}\lrb[\mathbf{0}, \bm{K}_{\bm{X}\bm{X}} + \sigma^2 \bm{I}].
\end{align}\]</div>
<p>This is the exact GP model, describing precisely how the inputs <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and outputs <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> are related. Exact inference and learning in this model is impossible because it involves the inversion of the full rank matrix <span class="math notranslate nohighlight">\(\bm{K}_{\bm{X}\bm{X}} + \sigma^2 \bm{I}\)</span>. To circumvent this, the VFE method places an approximate posterior based on inducing points. Let <span class="math notranslate nohighlight">\(\mathbf{\bar{X}} = (\mathbf{\bar{x}}_1, \mathbf{\bar{x}}_2, ..., \mathbf{\bar{x}}_M)^\top\)</span> be a set of inducing inputs, different from <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, and let <span class="math notranslate nohighlight">\(\fb = (f_{\mathbf{\bar{x}}_1}, f_{\mathbf{\bar{x}}_2}, ..., f_{\mathbf{\bar{x}}_N})^\top\)</span> be the corresponding function values at these points, called the inducing values. The VFE approximation chooses the approximate posterior to be</p>
<div class="math notranslate nohighlight">
\[\begin{align}
q(f) = p\left(\fnb | \fb\right)  q\left(\fb\right),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is the full latent function and <span class="math notranslate nohighlight">\(q\left(\fb\right)\)</span> is a free-form distribution to be optimised. Note that we have slightly abused notation, using <span class="math notranslate nohighlight">\(q\)</span> to denote both the variational distribution over the full latent function <span class="math notranslate nohighlight">\(f\)</span> as well as the variational distribution over the inducing point values <span class="math notranslate nohighlight">\(\fb\)</span>. This <span class="math notranslate nohighlight">\(q\)</span> separates <span class="math notranslate nohighlight">\(\fnb\)</span> from the data <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and instead uses the inducing values <span class="math notranslate nohighlight">\(\fb\)</span> to summarise the dataset and bring the approximate posterior <span class="math notranslate nohighlight">\(q(f)\)</span> close to the true posterior.</p>
</div>
<div class="section" id="maximising-the-vfe">
<h3>Maximising the VFE<a class="headerlink" href="#maximising-the-vfe" title="Permalink to this headline">¶</a></h3>
<p>We now look to maximise <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> with respect to <span class="math notranslate nohighlight">\(q\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{align}
q^* = \argmax_{q} \mathcal{F}(q, \bs{\theta}).
\end{align}\]</div>
<p>This maximisation can be performed in closed form. Perhaps unsuprisingly, the optimal <span class="math notranslate nohighlight">\(q^*\)</span> is also a Gaussian distribution.</p>
<div class="lemma">
<p><strong>Lemma (Optimal <span class="math notranslate nohighlight">\(q^*\)</span>)</strong> The variational posterior <span class="math notranslate nohighlight">\(q^*\)</span> which maximises the variational free energy is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
q^*\lrb[\fb] = \mathcal{N}\lrb[\fb;~\sigma^{-2} \Kbb \bs{\Sigma}^{-1} \Kbx \mathbf{y},~\Kbb\bs{\Sigma}^{-1}\Kbb]
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bs{\Sigma} = \lrb[\Kbb + \sigma^{-2} \Kbx \Kxb]\)</span>. The corresponding free energy is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathcal{F}(q^*, \bs{\theta}) = \mathcal{N}\lrb[\bm{y}; \bm{0}, \sigma^2 \bm{I} + \Kxb \Kbb^{-1} \Kbx] - \frac{1}{2\sigma^2} \text{Tr}\lrb[\Kxx - \Kxb\Kbb^{-1}\Kbx].
\end{align}\]</div>
</div>
<br>
<details class="proof">
<summary>Derivation: Optimal \(q^*\)</summary>
<p>Substituting the approximate posterior into <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align} \require{cancel}
\mathcal{F}(q, \bs{\theta}) &amp;= \int p\lrb[f_{\neq \Xb} | \fb] q\lrb[\fb | \mathbf{y}] \log \frac{p(\mathbf{y}, f | \bs{\theta})}{p\lrb[f_{\neq \Xb} | \fb] q\lrb[\mathbf{f}_{\mathbf{\bar{X}}} | \mathbf{y}]} df \\
&amp;= \int p\lrb[f_{\neq \Xb} | \fb] q\lrb[\fb | \mathbf{y}] \log \frac{p(\mathbf{y} | \fx, \bs{\theta})  \cancel{p\lrb[f_{\neq \Xb} | \fb]}  p\lrb[\fb]}{\cancel{p\lrb[f_{\neq \Xb} | \fb]} q\left(\fb | \mathbf{y} \right)} df \\
&amp;= \int p\lrb[\fx | \fb] q\lrb[\fb | \mathbf{y}] \log \frac{p(\mathbf{y} | \fx, \bs{\theta}) p(\fb)}{ q\lrb[\fb | \mathbf{y}]} d\fx d\fb.
\end{align}\end{split}\]</div>
<p>Now we seek to minimise <span class="math notranslate nohighlight">\(\mathcal{F}(q, \bs{\theta})\)</span> with respect to <span class="math notranslate nohighlight">\(q\)</span>. This requires a variational approach since <span class="math notranslate nohighlight">\(q\)</span> is a function, and also requires a Lagrange multiplier because <span class="math notranslate nohighlight">\(q\)</span> must be constrained to integrate to <span class="math notranslate nohighlight">\(1\)</span> since it is a distribution. Optimising <span class="math notranslate nohighlight">\(\mathcal{F}(q, \bs{\theta})\)</span> subject to this constraint is equivalent to optimising the Lagrangian <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> without constraints</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathcal{L} &amp;= \mathcal{F}(q, \bs{\theta}) - \lambda \left( \int q\lrb[\fb | \mathbf{y}] d\fb - 1 \right) \\
&amp;= \int p\lrb[\fx | \fb] q\lrb[\fb | \mathbf{y}] \log \left[p(\mathbf{y} | \fx, \bs{\theta}) p(\fb)\right] d\fx d\fb - \int p\lrb[\fx | \fb] q\lrb[\fb | \mathbf{y}] \log q\lrb[\fb | \mathbf{y}] d\fx d\fb - \lambda \left( \int q\lrb[\fb | \mathbf{y}] d\fb - 1 \right).
\end{align}\end{split}\]</div>
<p>Setting the (variational) derivative of <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> w.r.t. <span class="math notranslate nohighlight">\(q\)</span> to 0, we obtain:</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\frac{\delta \mathcal{L}}{\delta q} &amp;= \int p\lrb[\fx | \fb] \log p(\mathbf{y} | \fx, \bs{\theta}) d\fx + \log p\lrb[\fb] - \lrb[ \log q^*\lrb[\fb | \mathbf{y}] + 1] - \lambda = 0
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[\begin{align}
q^*\lrb[\fb | \mathbf{y}] = \frac{1}{Z} p\lrb[\fb]\exp \int p\lrb[\fx | \fb] \log p(\mathbf{y} | \fx, \bs{\theta}) d\fx
\end{align}\]</div>
<p>Substituting <span class="math notranslate nohighlight">\(p\lrb[\mathbf{y} | \fx] = \mathcal{N}\lrb[\mathbf{y}; \fx, \sigma^2 \mathbf{I}]\)</span> we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{align}
q^*\lrb[\fb | \mathbf{y}] = \frac{1}{Z} p\lrb[\fb] \exp \lrb[-\frac{N}{2}\log\lrb[2\pi\sigma^2] - \frac{1}{2\sigma^2} \underbrace{\int \lrb[\mathbf{y}^\top \mathbf{y} - 2 \mathbf{y}^\top \fx + \fx^\top \fx ] p\lrb[\fx | \fb] d\fx}_{= M}].
\end{align}\]</div>
<p>Using the fact that <span class="math notranslate nohighlight">\(\fx^\top \fx = \text{Tr}\lrb[\fx \fx^\top]\)</span> and substituting for <span class="math notranslate nohighlight">\(p\left(\fx | \fb \right)\)</span> and <span class="math notranslate nohighlight">\(p\lrb[\fb]\)</span> into <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, we evaluate the integral as:</p>
<div class="math notranslate nohighlight">
\[\begin{align}
M = \mathbf{y}^\top \mathbf{y} - 2 \mathbf{y}^\top \Kxb \Kbb^{-1} \fb  + \fb^\top \Kbb^{-1} \Kbx \Kxb \Kbb^{-1} \fb + \text{Tr}\lrb[\Kxx - \Kxb \Kbb^{-1} \Kbx].
\end{align}\]</div>
<p>We can now read off the <span class="math notranslate nohighlight">\(q^*\)</span> distribution easily as follows. Since <span class="math notranslate nohighlight">\(M\)</span> is a quadratic form in <span class="math notranslate nohighlight">\(\fb\)</span>, the whole exponential term above is an unnormalised Gaussian. The <span class="math notranslate nohighlight">\(p\lrb[\fb]\)</span> term is also a Gaussian, so its product with the exponential term will also be an unnormalised Gaussian in <span class="math notranslate nohighlight">\(\fb\)</span>, thus arriving at the result that the optimal <span class="math notranslate nohighlight">\(q^*\)</span> is also a Gaussian. We only have to determine the mean and covariance of the overall <span class="math notranslate nohighlight">\(q^*\)</span> and need not bother with constants at this stage. Using the standard results for the mean and covariance of a product of Gaussians we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{align}
q^*\lrb[\fb | \mathbf{y}] = \mathcal{N}\lrb[\fb;~\sigma^{-2} \Kbb \bs{\Sigma}^{-1} \Kbx \mathbf{y},~\Kbb\bs{\Sigma}^{-1}\Kbb]
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bs{\Sigma}^{-1}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\bs{\Sigma}^{-1} &amp;= \lrb[\Kbb + \sigma^{-2} \Kbx \Kxb]^{-1}.
\end{align}\]</div>
<p>Substituting this back into the free energy we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathcal{F}(q^*, \bs{\theta}) = \mathcal{N}\lrb[\bm{y}; \bm{0}, \sigma^2 \bm{I} + \Kxb \Kbb^{-1} \Kbx] - \frac{1}{2\sigma^2} \text{Tr}\lrb[\Kxx - \Kxb\Kbb^{-1}\Kbx].
\end{align}\]</div>
</details>
<br>
<p>Looking at this expression for <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, we observe that the first term is a Gaussian involving a covariance with special structure. The <span class="math notranslate nohighlight">\(\sigma^2 \bm{I}\)</span> term is diagonal, while the <span class="math notranslate nohighlight">\(\Kxb \Kbb^{-1} \Kbx\)</span> has rank <span class="math notranslate nohighlight">\(\min(M, N)\)</span>. Such matrices can be inverted in <span class="math notranslate nohighlight">\(\mathcal{O}\lrb[\min(M, N)^3]\)</span> time, using the <a class="reference external" href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury Identity</a>. The second term involves the trace of a matrix which, when <span class="math notranslate nohighlight">\(M &lt; N\)</span>, requires <span class="math notranslate nohighlight">\(\mathcal{O}\lrb[NM^2]\)</span> to compute. Therefore, the overal complexity of computing <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is <span class="math notranslate nohighlight">\(\mathcal{O}\lrb[NM^2]\)</span>, which is a significant gain over <span class="math notranslate nohighlight">\(\mathcal{O}\lrb[N^3]\)</span> if <span class="math notranslate nohighlight">\(M &lt;&lt; N\)</span>.</p>
</div>
<div class="section" id="posterior-predictive">
<h3>Posterior predictive<a class="headerlink" href="#posterior-predictive" title="Permalink to this headline">¶</a></h3>
<p>We are also interested in making predictions at new test points. In order to achieve this in a computationally efficient way, we can use <span class="math notranslate nohighlight">\(q^*\)</span> and evaluate</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p\lrb[\bm{y}^* | \bm{X}^*, \Xb] \approx \int p\lrb[\bm{y}^* | \fstar] p\lrb[\fstar | \fb] q\lrb[\fb] d\fb d\fstar.
\end{align}\]</div>
<p>Again, we can do this calculation in closed form to obtain an (approximate) predictive posterior.</p>
<div class="lemma">
<p><strong>Lemma (Approximate predictive posterior)</strong> Under the optimal approximate posterior <span class="math notranslate nohighlight">\(q^*\)</span>, the predictive posterior of the VFE approximation is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p\lrb[\bm{y}^* | \X^*, \Xb] = \mathcal{N}\lrb[\bm{y}^*; \sigma^{-2} \Ksb \bs{\Sigma}^{-1} \Kbx \mathbf{y}, \Kss - \Ksb \Kbb^{-1}\Kbs + \Ksb \bs{\Sigma}^{-1} \Kbs + \sigma^2 \mathbf{I}].
\end{align}\]</div>
</div>
<br>
<details class="proof">
<summary>Derivation: Approximate predictive posterior</summary>
<p>Starting from the expression for the approximate predictive posterior</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p\lrb[\bm{y}^* | \bm{X}^*, \Xb] \approx \int p\lrb[\bm{y}^* | \fstar] p\lrb[\fstar | \fb] q\lrb[\fb] d\fb d\fstar,
\end{align}\]</div>
<p>and remembering that the exact <span class="math notranslate nohighlight">\(p\lrb[\fstar | \fb]\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p\lrb[\fstar | \fb] = \mathcal{N}\lrb[\fstar;~\Ksb\Kbb^{-1}\fb,~\Kss - \Ksb\Kbb^{-1}\Kbs],
\end{align}\]</div>
<p>we obtain, using standard results for the product of Gaussians and marginals of Gaussians</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p\lrb[\bm{y}^* | \X^*, \Xb] = \mathcal{N}\lrb[\bm{y}^*; \sigma^{-2} \Ksb \bs{\Sigma}^{-1} \Kbx \mathbf{y}, \Kss - \Ksb \Kbb^{-1}\Kbs + \Ksb \bs{\Sigma}^{-1}\Kbs + \sigma^2 \mathbf{I} ].
\end{align}\]</div>
</details>
<br>
<p>To evaluate this predictive posterior, we need to invert the <span class="math notranslate nohighlight">\(M \times M\)</span> matrix <span class="math notranslate nohighlight">\(\Kbb\)</span>, costing us <span class="math notranslate nohighlight">\(\mathcal{O}\lrb[M^3]\)</span>, and also to compute certain matrix products, the worst of which will cost us <span class="math notranslate nohighlight">\(\mathcal{O}\lrb[NM^2]\)</span>, making the overall cost scale as <span class="math notranslate nohighlight">\(\mathcal{O}\lrb[NM^2]\)</span> when <span class="math notranslate nohighlight">\(M &lt; N\)</span>, just as with the free energy.</p>
</div>
<div class="section" id="cost-and-quality">
<h3>Cost and quality<a class="headerlink" href="#cost-and-quality" title="Permalink to this headline">¶</a></h3>
<p>Let’s take a moment to review the VFE approximation. How did we achieve the improvement from <span class="math notranslate nohighlight">\(\mathcal{O}\lrb[N^3]\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}\lrb[NM^2]\)</span>? In the original GP posterior predictive, we considered statistical relationships between all datapoints <span class="math notranslate nohighlight">\(\X, \bm{y}\)</span> and the prediction points <span class="math notranslate nohighlight">\(\X^*, \bm{y}^*\)</span></p>
<p>By approximating <span class="math notranslate nohighlight">\(p\lrb[f | \bm{y}, \X]\)</span> with the distribution <span class="math notranslate nohighlight">\(p\lrb[f_{\neq \Xb} | \fb] q\lrb[\fb]\)</span>, we instead account for statistical relationships only between <span class="math notranslate nohighlight">\(\fb\)</span> and <span class="math notranslate nohighlight">\(f\)</span>, thereby reducing the cost of evaluating the posterior predictive. The information of how <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> affects <span class="math notranslate nohighlight">\(\mathbf{y}^*\)</span> is entirely contained in the distribution <span class="math notranslate nohighlight">\(q\lrb[\fb]\)</span>, the form of which is picked to approximate the the true posterior as accurately as possible. Although this summarised representation reduces the cost of making predictions, it also limits the set of posterior distributions which can be accurately represented by our model. A way in which this approximation might fail is if there are not enough inducing points to sufficiently constrain the approximate posterior - consider the extreme case of using a single (or zero) inducing points to model many data points. This could be either due to picking too small an <span class="math notranslate nohighlight">\(M\)</span>, or placing the inducing points’ inputs <span class="math notranslate nohighlight">\(\Xb\)</span> at poor locations, leaving large areas of the input space uncovered.</p>
<p>As the dataset size <span class="math notranslate nohighlight">\(N\)</span> increases, a greater number of inducing points may be needed. In particular, if we wish to approximate the exact posterior <span class="math notranslate nohighlight">\(p\lrb[\fx | \fb]\)</span> sufficiently accurately throughout a larger input region, we may need more inducing points and thus a larger <span class="math notranslate nohighlight">\(M\)</span>. The scaling of <span class="math notranslate nohighlight">\(M\)</span> will therefore depend on (1) the distribution of input data <span class="math notranslate nohighlight">\(\bm{X}\)</span>, (2) the type of kernel used and (3) the specified quality of approxmation, for example in KL distance.</p>
<p>Clearly, the positions <span class="math notranslate nohighlight">\(\Xb\)</span> of the inducing points are important and we haven’t talked about how to optimise those. One of the main contributions of the VFE approximation is to provide a principled way of selecting the inducing point locations, which is to optimise <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> with respect to <span class="math notranslate nohighlight">\(\Xb\)</span>. This can be done without fear of overfitting, because we are optimising <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> which is a lower bound to the exact log-marginal likelihood. Adding more inducing points or optimising their locations for longer will only bring the approximate posterior closer to the true posterior. This is not so for model-approximating methods like DTC<a class="bibtex reference internal" href="#seeger2003fast" id="id4">[SWL03]</a> or FITC<a class="bibtex reference internal" href="#snelson2005sparse" id="id5">[SG05]</a> which approximate the model rather than the posterior. In this case, there is no guarantee that adding more inducing points or optimising their locations won’t overfit. This is one of the strengths of the VFE approximation.</p>
</div>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>We are now at a position to implement the sparse VFE approximation for GPs. We will fit the VFE approximation to data sampled from an exact GP for which we know the ground truth.</p>
<div class="section" id="sampling-the-exact-gp">
<h3>Sampling the exact GP<a class="headerlink" href="#sampling-the-exact-gp" title="Permalink to this headline">¶</a></h3>
<p>First, we implement a constant GP mean class and an EQ covariance class, both as <code class="docutils literal notranslate"><span class="pre">tf.keras.Models</span></code>, so that we can later re-use them to make a trainable <code class="docutils literal notranslate"><span class="pre">VFEGP</span></code>.</p>
<div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_probability</span> <span class="k">as</span> <span class="nn">tfp</span>


<span class="k">class</span> <span class="nc">ConstantMean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;constant_mean&#39;</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">constant</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
        
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">constant</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    
    
<span class="k">class</span> <span class="nc">EQcovariance</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">log_coeff</span><span class="p">,</span>
                 <span class="n">log_scales</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;eq_covariance&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
        <span class="c1"># Convert parameters to tensors</span>
        <span class="n">log_coeff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_coeff</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">log_scales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_scales</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Reshape parameter tensors</span>
        <span class="n">log_coeff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">log_coeff</span><span class="p">)</span>
        <span class="n">log_scales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">log_scales</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
        
        <span class="c1"># Set input dimensionality</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        
        <span class="c1"># Set EQ parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_scales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">log_scales</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_coeff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">log_coeff</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
        
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">x1</span><span class="p">,</span>
                 <span class="n">x2</span><span class="p">,</span>
                 <span class="n">diag</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="c1"># Convert to tensors</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Get vector of lengthscales</span>
        <span class="n">scales</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales</span>
        
        <span class="c1"># If calculating full covariance, add dimensions to broadcast</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">diag</span><span class="p">:</span>

            <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

            <span class="n">scales</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="c1"># Compute quadratic, exponentiate and multiply by coefficient</span>
        <span class="n">quad</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">scales</span>
        <span class="n">quad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">quad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">eq_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">quad</span><span class="p">)</span>
        
        <span class="c1"># Add jitter for invertibility</span>
        <span class="k">if</span> <span class="n">epsilon</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">eq_cov</span> <span class="o">=</span> <span class="n">eq_cov</span> <span class="o">+</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">eq_cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
                                               <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">eq_cov</span>
        
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">scales</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_scales</span><span class="p">)</span>
    
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">coeff</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed and tf.dtype</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>

<span class="c1"># Num. observations (N)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># EQ covariance hyperparameters</span>
<span class="n">log_coeff</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">log_scale</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Initialise covariance</span>
<span class="n">ground_truth_cov</span> <span class="o">=</span> <span class="n">EQcovariance</span><span class="p">(</span><span class="n">log_coeff</span><span class="o">=</span><span class="n">log_coeff</span><span class="p">,</span>
                                <span class="n">log_scales</span><span class="o">=</span><span class="n">log_scale</span><span class="p">,</span>
                                <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Pick inputs at random</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mf">4.</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Compute covariance matrix terms</span>
<span class="n">K_train_train</span> <span class="o">=</span> <span class="n">ground_truth_cov</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
<span class="n">I_noise</span> <span class="o">=</span> <span class="n">noise</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="c1"># Sample f_ind | x_ind</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">K_train_train</span> <span class="o">+</span> <span class="n">I_noise</span><span class="p">),</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Locations to plot mean and variance of generative model, y_plot | f_ind, x_plot</span>
<span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># Covariances between inducing points and input locations</span>
<span class="n">K_train_plot</span> <span class="o">=</span> <span class="n">ground_truth_cov</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_plot</span><span class="p">)</span>
<span class="n">K_plot_train</span> <span class="o">=</span> <span class="n">ground_truth_cov</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span>
<span class="n">K_plot_diag</span> <span class="o">=</span> <span class="n">ground_truth_cov</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">x_plot</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Mean and standard deviation of y_plot | f_ind, x_plot</span>
<span class="n">y_plot_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_plot_train</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K_train_train</span> <span class="o">+</span> <span class="n">I_noise</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">f_plot_var</span> <span class="o">=</span> <span class="n">K_plot_diag</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_plot_train</span><span class="p">,</span>
                                   <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K_train_train</span> <span class="o">+</span> <span class="n">I_noise</span><span class="p">,</span> <span class="n">K_train_plot</span><span class="p">)))</span>
<span class="n">y_plot_var</span> <span class="o">=</span> <span class="n">f_plot_var</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">y_plot_std</span> <span class="o">=</span> <span class="n">y_plot_var</span> <span class="o">**</span> <span class="mf">0.5</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide_input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot inducing points and observed data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    
<span class="c1"># Plot exact posterior predictive</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">y_plot_mean</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">y_plot_std</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span>  <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">y_plot_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span>  <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Exact post.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">y_plot_mean</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">y_plot_std</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span>  <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot sampled data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
            <span class="n">y_train</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span>
            <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Observed $\mathbf</span><span class="si">{y}</span><span class="s1">$&#39;</span><span class="p">)</span>

<span class="c1"># Plot formatting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Synthetic data and ground truth&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/vfe_10_0.svg" src="../../../_images/vfe_10_0.svg" /></div>
</div>
</div>
<div class="section" id="implementing-the-vfe-gp">
<h3>Implementing the VFE GP<a class="headerlink" href="#implementing-the-vfe-gp" title="Permalink to this headline">¶</a></h3>
<p>We can now put things together and implement the <code class="docutils literal notranslate"><span class="pre">VFEGP</span></code>. The <code class="docutils literal notranslate"><span class="pre">post_pred</span></code> and <code class="docutils literal notranslate"><span class="pre">free_energy</span></code> methods usee the Woodbury identity to speed up the inversion of the matrices required in this computation. It turns out however that when using the Woodbury identity, some of the matrices involved are poorly condtitioned. To alleviate this issue, I’ve found it necessary to make use of a <a class="reference external" href="https://gpflow.readthedocs.io/en/master/notebooks/theory/SGPR_notes.html?highlight=variational">conditioning trick</a> also used in the GPFlow library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VFEGP</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">x_train</span><span class="p">,</span>
                 <span class="n">y_train</span><span class="p">,</span>
                 <span class="n">x_ind_init</span><span class="p">,</span>
                 <span class="n">mean</span><span class="p">,</span>
                 <span class="n">cov</span><span class="p">,</span>
                 <span class="n">log_noise</span><span class="p">,</span>
                 <span class="n">trainable_noise</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vfe_gp&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># Set training data and inducing point initialisation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="c1"># Set inducing points</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">)</span>
        
        <span class="c1"># Set mean and covariance functions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cov</span> <span class="o">=</span> <span class="n">cov</span>
    
        <span class="c1"># Set log of noise parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_noise</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable_noise</span><span class="p">)</span>
    
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">noise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span><span class="p">)</span>
        
        
    <span class="k">def</span> <span class="nf">post_pred</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">):</span>
        
        <span class="c1"># Number of training points</span>
        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">M</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Compute covariance terms</span>
        <span class="n">K_ind_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
        <span class="n">K_train_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">)</span>
        <span class="n">K_ind_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">)</span>
        <span class="n">K_pred_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">)</span>
        <span class="n">K_ind_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">)</span>
        <span class="n">K_pred_pred_diag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Compute shared matrix and its cholesky:</span>
        <span class="c1"># L = chol(K_ind_ind)</span>
        <span class="c1"># U = iL K_ind_train</span>
        <span class="c1"># A = U / noise</span>
        <span class="c1"># B = I + A A.T</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">)</span>
        <span class="n">LT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">triangular_solve</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">K_ind_train</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">U</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span>
        <span class="n">B</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">B_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
        
        <span class="c1"># Compute mean</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="c1"># - self.mean(self.x_train)[:, None]</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="n">B_chol</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">diff</span><span class="p">))</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">triangular_solve</span><span class="p">(</span><span class="n">LT</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">K_pred_ind</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
        
        <span class="n">C</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">triangular_solve</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">K_ind_pred</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">triangular_solve</span><span class="p">(</span><span class="n">B_chol</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
        
        <span class="c1"># Compute variance</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">K_pred_pred_diag</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">diag_part</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">diag_part</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span>
        
        
    <span class="k">def</span> <span class="nf">free_energy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="c1"># Number of training points</span>
        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">M</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Compute covariance terms</span>
        <span class="n">K_ind_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
        <span class="n">K_train_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">)</span>
        <span class="n">K_ind_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">)</span>
        <span class="n">K_train_train_diag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Compute shared matrix and its cholesky:</span>
        <span class="c1"># L = chol(K_ind_ind)</span>
        <span class="c1"># U = iL K_ind_train</span>
        <span class="c1"># A = U / noise</span>
        <span class="c1"># B = I + A A.T</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">triangular_solve</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">K_ind_train</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">U</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span>
        <span class="n">B</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">B_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
        
        <span class="c1"># Compute log-normalising constant of the matrix</span>
        <span class="n">log_pi</span> <span class="o">=</span> <span class="o">-</span> <span class="n">N</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="n">log_det_B</span> <span class="o">=</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">diag_part</span><span class="p">(</span><span class="n">B_chol</span><span class="p">)))</span>
        <span class="n">log_det_noise</span> <span class="o">=</span> <span class="o">-</span> <span class="n">N</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Log of determinant of normalising term</span>
        <span class="n">log_det</span> <span class="o">=</span> <span class="n">log_pi</span> <span class="o">+</span> <span class="n">log_det_B</span> <span class="o">+</span> <span class="n">log_det_noise</span>       
        
        <span class="c1"># Compute quadratic form</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">triangular_solve</span><span class="p">(</span><span class="n">B_chol</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">diff</span><span class="p">),</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span>
        <span class="n">quad</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">diff</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">quad</span> <span class="o">=</span> <span class="n">quad</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">c</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Compute trace term</span>
        <span class="n">trace</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">K_train_train_diag</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">trace</span> <span class="o">=</span> <span class="n">trace</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        
        <span class="n">free_energy</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_det</span> <span class="o">+</span> <span class="n">quad</span> <span class="o">+</span> <span class="n">trace</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>
        
        <span class="k">return</span> <span class="n">free_energy</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
         <span class="n">ground_truth_cov</span><span class="p">,</span>
         <span class="n">x_pred</span><span class="p">,</span>
         <span class="n">x_train</span><span class="p">,</span>
         <span class="n">y_train</span><span class="p">,</span>
         <span class="n">x_ind_init</span><span class="p">,</span>
         <span class="n">step</span><span class="p">):</span>

    <span class="c1"># Get exact and approximate posterior predictive</span>
    <span class="n">vfe_mean</span><span class="p">,</span> <span class="n">vfe_var</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">post_pred</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>

    <span class="c1"># Covariances between inducing points and input locations</span>
    <span class="n">K_train_plot</span> <span class="o">=</span> <span class="n">ground_truth_cov</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_plot</span><span class="p">)</span>
    <span class="n">K_plot_train</span> <span class="o">=</span> <span class="n">ground_truth_cov</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span>
    <span class="n">K_plot_diag</span> <span class="o">=</span> <span class="n">ground_truth_cov</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">x_plot</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Mean and standard deviation of y_plot | f_ind, x_plot</span>
    <span class="n">exact_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_plot_train</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K_train_train</span> <span class="o">+</span> <span class="n">I_noise</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">exact_var</span> <span class="o">=</span> <span class="n">K_plot_diag</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_plot_train</span><span class="p">,</span>
                                       <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K_train_train</span> <span class="o">+</span> <span class="n">I_noise</span><span class="p">,</span> <span class="n">K_train_plot</span><span class="p">)))</span>
    <span class="n">exact_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">exact_var</span> <span class="o">+</span> <span class="n">noise</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    
    <span class="n">x_pred</span> <span class="o">=</span> <span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">x_ind</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">x_ind</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="n">vfe_mean</span> <span class="o">=</span> <span class="n">vfe_mean</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">vfe_var</span> <span class="o">=</span> <span class="n">vfe_var</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    
    <span class="c1"># Plot posterior predictive</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span>
             <span class="n">vfe_mean</span><span class="p">,</span>
             <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span>
             <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Approx. Post.&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span>
                     <span class="n">vfe_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">vfe_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">vfe_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">vfe_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    
    <span class="c1"># Plot exact posterior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span>
             <span class="n">exact_mean</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">exact_std</span><span class="p">,</span>
             <span class="s1">&#39;--&#39;</span><span class="p">,</span>
             <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span>
             <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span>
             <span class="n">exact_mean</span><span class="p">,</span>
             <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span>
             <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span>
             <span class="n">exact_mean</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">exact_std</span><span class="p">,</span>
             <span class="s1">&#39;--&#39;</span><span class="p">,</span>
             <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span>
             <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Exact Post.&#39;</span><span class="p">)</span>

    <span class="c1"># Plot training data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Plot initial inducing points</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">5.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">),</span>
                <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Init. $\bar{\mathbf</span><span class="si">{X}</span><span class="s1">}$&#39;</span><span class="p">,</span>
                <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Plot current inducing points</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ind</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">5.</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x_ind</span><span class="p">),</span>
                <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Curr. $\bar{\mathbf</span><span class="si">{X}</span><span class="s1">}$&#39;</span><span class="p">,</span>
                <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Format plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;VFE after </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s1"> optimisation steps&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    
<span class="k">def</span> <span class="nf">print_info</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
    
    <span class="n">free_energy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">free_energy</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Step: </span><span class="si">{</span><span class="n">step</span><span class="si">:</span><span class="s1">5&gt;</span><span class="si">}</span><span class="s1"> &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;Free energy: </span><span class="si">{</span><span class="n">free_energy</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s1">8.3f</span><span class="si">}</span><span class="s1"> &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;Coeff: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">cov</span><span class="o">.</span><span class="n">coeff</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s1">5.2f</span><span class="si">}</span><span class="s1"> &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;Scales: </span><span class="si">{</span><span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cov</span><span class="o">.</span><span class="n">scales</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span><span class="si">}</span><span class="s1"> &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;Noise: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">noise</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s1">5.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="learning-inducing-points">
<h3>Learning inducing points<a class="headerlink" href="#learning-inducing-points" title="Permalink to this headline">¶</a></h3>
<p>As a sanity check, we first try learning the inducing point locations only, setting the covariance parameters to their ground truth values.</p>
<div class="cell tag_hide_input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed and tensor dtype</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Number GP constants</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">inducing_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">)</span>
<span class="n">log_noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e-1</span><span class="p">)</span>
<span class="n">log_coeff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e0</span><span class="p">)</span>
<span class="n">log_scales</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e0</span><span class="p">)]</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Define mean and covariance</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">ConstantMean</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">cov</span> <span class="o">=</span> <span class="n">EQcovariance</span><span class="p">(</span><span class="n">log_coeff</span><span class="o">=</span><span class="n">log_coeff</span><span class="p">,</span>
                   <span class="n">log_scales</span><span class="o">=</span><span class="n">log_scales</span><span class="p">,</span>
                   <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                   <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                   <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>

<span class="c1"># Initial locations of inducing points</span>
<span class="n">x_ind_dist</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">*</span><span class="n">inducing_range</span><span class="p">)</span>
<span class="n">x_ind_init</span> <span class="o">=</span> <span class="n">x_ind_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x_ind_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Define sparse VFEGP</span>
<span class="n">vfe_gp</span> <span class="o">=</span> <span class="n">VFEGP</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span>
               <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span>
               <span class="n">log_noise</span><span class="o">=</span><span class="n">log_noise</span><span class="p">,</span>
               <span class="n">x_train</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
               <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
               <span class="n">x_ind_init</span><span class="o">=</span><span class="n">x_ind_init</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
               <span class="n">trainable_noise</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>

<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-1</span><span class="p">)</span>

<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

        <span class="n">free_energy</span> <span class="o">=</span> <span class="n">vfe_gp</span><span class="o">.</span><span class="n">free_energy</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">free_energy</span>
        
        <span class="c1"># Print information and plot at start and end</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">num_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            
            <span class="n">print_info</span><span class="p">(</span><span class="n">vfe_gp</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

            <span class="n">plot</span><span class="p">(</span><span class="n">vfe_gp</span><span class="p">,</span>
                 <span class="n">ground_truth_cov</span><span class="p">,</span>
                 <span class="n">x_pred</span><span class="p">,</span>
                 <span class="n">x_train</span><span class="p">,</span>
                 <span class="n">y_train</span><span class="p">,</span>
                 <span class="n">x_ind_init</span><span class="p">,</span>
                 <span class="n">step</span><span class="p">)</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">vfe_gp</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">vfe_gp</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 0 Free energy:  -35.235 Coeff:  1.00 Scales: [1.0] Noise:  0.10
</pre></div>
</div>
<img alt="../../../_images/vfe_15_1.svg" src="../../../_images/vfe_15_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 1000 Free energy:    0.532 Coeff:  1.00 Scales: [1.0] Noise:  0.10
</pre></div>
</div>
<img alt="../../../_images/vfe_15_3.svg" src="../../../_images/vfe_15_3.svg" /></div>
</div>
<p>Initially the positions of the inducing points (green) do not allow for an expressive enough posterior to match the exact GP posterior. By spreading out the inducing points, the model manages to recover an approximate posterior that is very close to the ground truth. We were expecting this to be possible since there are lots of observed data which are redundant, and should be summarisable by a smaller set of inducing points.</p>
</div>
<div class="section" id="complete-learning">
<h3>Complete learning<a class="headerlink" href="#complete-learning" title="Permalink to this headline">¶</a></h3>
<p>We now turn to joint learning of learning the inducing point locations as well as the covariance parameters. This time around we initialise the hyperparameters to values far from the ground truth, to see whether the model is able to recover something close to the true posterior.</p>
<div class="cell tag_hide_input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed and tensor dtype</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Number GP constants</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">inducing_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">)</span>
<span class="n">log_noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e0</span><span class="p">)</span>
<span class="n">log_coeff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e1</span><span class="p">)</span>
<span class="n">log_scales</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e1</span><span class="p">)]</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Define mean and covariance</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">ConstantMean</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">cov</span> <span class="o">=</span> <span class="n">EQcovariance</span><span class="p">(</span><span class="n">log_coeff</span><span class="o">=</span><span class="n">log_coeff</span><span class="p">,</span>
                   <span class="n">log_scales</span><span class="o">=</span><span class="n">log_scales</span><span class="p">,</span>
                   <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                   <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                   <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>

<span class="c1"># Initial locations of inducing points</span>
<span class="n">x_ind_dist</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">*</span><span class="n">inducing_range</span><span class="p">)</span>
<span class="n">x_ind_init</span> <span class="o">=</span> <span class="n">x_ind_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x_ind_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Re-define sparse VFEGP with trainable noise</span>
<span class="n">vfe_gp</span> <span class="o">=</span> <span class="n">VFEGP</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span>
               <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span>
               <span class="n">log_noise</span><span class="o">=</span><span class="n">log_noise</span><span class="p">,</span>
               <span class="n">x_train</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
               <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
               <span class="n">x_ind_init</span><span class="o">=</span><span class="n">x_ind_init</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
               <span class="n">trainable_noise</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>

<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-1</span><span class="p">)</span>

<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

        <span class="n">free_energy</span> <span class="o">=</span> <span class="n">vfe_gp</span><span class="o">.</span><span class="n">free_energy</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">free_energy</span>
        
        <span class="c1"># Print information and plot at start and end</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">num_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            
            <span class="n">print_info</span><span class="p">(</span><span class="n">vfe_gp</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

            <span class="n">plot</span><span class="p">(</span><span class="n">vfe_gp</span><span class="p">,</span>
                 <span class="n">ground_truth_cov</span><span class="p">,</span>
                 <span class="n">x_pred</span><span class="p">,</span>
                 <span class="n">x_train</span><span class="p">,</span>
                 <span class="n">y_train</span><span class="p">,</span>
                 <span class="n">x_ind_init</span><span class="p">,</span>
                 <span class="n">step</span><span class="p">)</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">vfe_gp</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">vfe_gp</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 0 Free energy:   -1.232 Coeff: 10.00 Scales: [10.0] Noise:  1.00
</pre></div>
</div>
<img alt="../../../_images/vfe_18_1.svg" src="../../../_images/vfe_18_1.svg" /><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step: 1000 Free energy:    0.547 Coeff:  1.16 Scales: [1.115] Noise:  0.10
</pre></div>
</div>
<img alt="../../../_images/vfe_18_3.svg" src="../../../_images/vfe_18_3.svg" /></div>
</div>
<p>We observe that the model has been able to recover both an approximate posterior which is very close to the true posterior, as well as parameter values close to the ground truth. It achieves this with a complexity of just <span class="math notranslate nohighlight">\(\mathcal{O}(NM^2)\)</span> as opposed to exact GP inference which requires <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> operations.</p>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>We have seen how the free energy may be used together with an inducing point approximate posterior to reduce the computational cost of doing infernce and learning with GPs. Unlike other methods such as DTC and FITC, the VFE method approximates the posterior rather than the model, bringing several benefits.<a class="bibtex reference internal" href="#bauer2016understanding" id="id6">[BvdWR16]</a> VFE decouples model assumptions and approximations. In this way, it keeps the exact model sacrosanct and attempts to approximate it as faithfully as possible. Second, because the Free Energy is a lower bound to the marginal log-likelihood, it is impossible for VFE to overfit any more than the exact. model. By contrast, other methods can overfit more than the original GP. VFE explicitly represents posterior uncertainty, keeping it separate from noise uncertainty whereas methods do not.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-content/gp/sparse/vfe-0"><dl class="citation">
<dt class="bibtex label" id="bauer2016understanding"><span class="brackets"><a class="fn-backref" href="#id6">BvdWR16</a></span></dt>
<dd><p>Matthias Bauer, Mark van der Wilk, and Carl Edward Rasmussen. Understanding probabilistic sparse gaussian process approximations. In <em>Advances in Neural Information Processing Systems</em>. 2016.</p>
</dd>
<dt class="bibtex label" id="bui2017unifying"><span class="brackets">BYT17</span></dt>
<dd><p>Thang Bui, Josiah Yan, and Richard Turner. A unifying framework for gaussian process pseudopoint approximations using power expectation propagation. <em>The Journal of Machine Learning Research</em>, 2017.</p>
</dd>
<dt class="bibtex label" id="seeger2003fast"><span class="brackets">SWL03</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Matthias W. Seeger, Christopher K. I. Williams, and Neil D. Lawrence. Fast forward selection to speed up sparse gaussian process regression. <em>Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics</em>, 2003.</p>
</dd>
<dt class="bibtex label" id="snelson2005sparse"><span class="brackets">SG05</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. <em>Advances in neural information processing systems</em>, 2005.</p>
</dd>
<dt class="bibtex label" id="titsias2009variational"><span class="brackets"><a class="fn-backref" href="#id1">Tit09</a></span></dt>
<dd><p>Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In <em>Artificial intelligence and statistics</em>, 567–574. PMLR, 2009.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "venv-random-walks"
        },
        kernelOptions: {
            kernelName: "venv-random-walks",
            path: "./content/gp/sparse"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'venv-random-walks'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="sparse-intro.html" title="previous page">Sparse Gaussian Processes</a>
    <a class='right-next' id="next-link" href="gp-sampling.html" title="next page">Efficiently sampling GP posteriors</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratos Markou<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>