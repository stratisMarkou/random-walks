{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VFEGP(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 x_train,\n",
    "                 y_train,\n",
    "                 x_ind_init,\n",
    "                 mean,\n",
    "                 cov,\n",
    "                 log_noise,\n",
    "                 dtype,\n",
    "                 name='vfe_gp',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "        \n",
    "        # Set training data and inducing point initialisation\n",
    "        self.x_train = tf.convert_to_tensor(x_train, dtype=dtype)\n",
    "        self.y_train = tf.convert_to_tensor(y_train, dtype=dtype)\n",
    "        \n",
    "        # Set inducing points\n",
    "        self.x_ind = tf.convert_to_tensor(x_ind_init, dtype=dtype)\n",
    "        self.x_ind = tf.Variable(self.x_ind)\n",
    "        \n",
    "        # Set mean and covariance functions\n",
    "        self.mean = mean\n",
    "        self.cov = cov\n",
    "    \n",
    "        # Set log of noise parameter\n",
    "        self.log_noise = tf.convert_to_tensor(log_noise,\n",
    "                                              dtype=dtype)\n",
    "        self.log_noise = tf.Variable(self.log_noise)\n",
    "        \n",
    "        \n",
    "    def post_pred(self, x_pred):\n",
    "        \n",
    "        # Compute covariance terms\n",
    "        K_ind_ind = self.cov(self.x_ind, self.x_ind, epsilon=1e-4)\n",
    "        K_train_ind = self.cov(self.x_train, self.x_ind)\n",
    "        K_ind_train = self.cov(self.x_ind, self.x_train)\n",
    "        K_pred_ind = self.cov(x_pred, self.x_ind)\n",
    "        K_ind_pred = self.cov(self.x_ind, x_pred)\n",
    "        K_pred_pred_diag = self.cov(x_pred, x_pred, diag=True)\n",
    "        \n",
    "        # Compute cholesky factor for numerical stability\n",
    "        U = K_ind_ind + tf.matmul(K_ind_train, K_train_ind) / self.noise\n",
    "        U_chol = tf.linalg.cholesky(U)\n",
    "        \n",
    "        # Difference between training data and mean\n",
    "        diff = self.y_train - self.mean(self.x_train)[:, None]\n",
    "        beta = tf.linalg.cholesky_solve(U_chol, tf.matmul(K_ind_train, diff))\n",
    "        beta = beta / self.noise\n",
    "        \n",
    "        # Mean at plotting locations\n",
    "        mean = tf.matmul(K_pred_ind, beta)[:, 0] + self.mean(x_pred)\n",
    "        \n",
    "        L = tf.linalg.cholesky(K_ind_ind)\n",
    "        M = tf.linalg.triangular_solve(L, K_ind_pred, lower=True)\n",
    "        \n",
    "        var = K_pred_pred_diag\n",
    "        var = var - tf.linalg.diag_part(tf.matmul(M, M, transpose_a=True))\n",
    "        var = var + self.noise ** 2\n",
    "        \n",
    "        return mean, var\n",
    "        \n",
    "        \n",
    "    def free_energy(self):\n",
    "        \n",
    "        # Number of training points\n",
    "        N = self.y_train.shape[0]\n",
    "        M = self.x_ind.shape[0]\n",
    "        \n",
    "        # Compute covariance terms\n",
    "        K_ind_ind = self.cov(self.x_ind, self.x_ind, epsilon=1e-6)\n",
    "        K_train_ind = self.cov(self.x_train, self.x_ind)\n",
    "        K_ind_train = self.cov(self.x_ind, self.x_train)\n",
    "        K_train_train = self.cov(self.x_train, self.x_train)\n",
    "        \n",
    "        # Compute shared matrix and its cholesky:\n",
    "        # LLT = K_ind_ind\n",
    "        # U = I + L-1 K_train_ind K_ind_train L / noise ** 2\n",
    "        L = tf.linalg.cholesky(K_ind_ind)\n",
    "        U = tf.linalg.triangular_solve(L, K_ind_train, lower=True)\n",
    "        A = U / self.noise\n",
    "        B = tf.eye(M, dtype=self.dtype) + tf.matmul(A, A, transpose_b=True)\n",
    "        B_chol = tf.linalg.cholesky(B)\n",
    "        \n",
    "        # Compute log-normalising constant of the matrix\n",
    "        log_pi = - N / 2 * tf.math.log(tf.constant(2 * np.pi, dtype=self.dtype))\n",
    "        log_det_B = - 0.5 * tf.linalg.slogdet(B)[1]\n",
    "        log_det_noise = - N / 2 * tf.math.log(self.noise ** 2)\n",
    "        \n",
    "        # Log of determinant of normalising term\n",
    "        log_det = log_pi + log_det_B + log_det_noise       \n",
    "        \n",
    "        # Compute quadratic form\n",
    "        diff = self.y_train - self.mean(self.x_train)[:, None]\n",
    "        c = tf.linalg.triangular_solve(B_chol, tf.matmul(A, diff), lower=True) / self.noise\n",
    "        quad = - 0.5 * tf.reduce_sum((diff / self.noise) ** 2)\n",
    "        quad = quad + 0.5 * tf.reduce_sum(c ** 2)\n",
    "        \n",
    "        # Compute trace term\n",
    "        trace = - 0.5 * tf.linalg.trace(K_train_train) / self.noise ** 2\n",
    "        trace = trace + 0.5 * tf.linalg.trace(tf.matmul(A, A, transpose_b=True))\n",
    "        \n",
    "        free_energy = (log_det + quad + trace) / N\n",
    "        \n",
    "        return free_energy\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def noise(self):\n",
    "        return tf.math.exp(self.log_noise)\n",
    "    \n",
    "    \n",
    "    def sample_posterior(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EQcovariance(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "                 log_coeff,\n",
    "                 log_scales,\n",
    "                 dim,\n",
    "                 dtype,\n",
    "                 name='eq_covariance',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "    \n",
    "        # Convert parameters to tensors\n",
    "        log_coeff = tf.convert_to_tensor(log_coeff, dtype=dtype)\n",
    "        log_scales = tf.convert_to_tensor(log_scales, dtype=dtype)\n",
    "\n",
    "        # Reshape parameter tensors\n",
    "        log_coeff = tf.squeeze(log_coeff)\n",
    "        log_scales = tf.reshape(log_scales, (-1,))\n",
    "        \n",
    "        # Set input dimensionality\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Set EQ parameters\n",
    "        self.log_scales = tf.Variable(log_scales)\n",
    "        self.log_coeff = tf.Variable(log_coeff)\n",
    "        \n",
    "        \n",
    "    def __call__(self,\n",
    "                 x1,\n",
    "                 x2,\n",
    "                 diag=False,\n",
    "                 epsilon=None):\n",
    "        \n",
    "        # Convert to tensors\n",
    "        x1 = tf.convert_to_tensor(x1, dtype=self.dtype)\n",
    "        x2 = tf.convert_to_tensor(x2, dtype=self.dtype)\n",
    "\n",
    "        # Get vector of lengthscales\n",
    "        scales = self.scales\n",
    "        \n",
    "        # If calculating full covariance, add dimensions to broadcast\n",
    "        if not diag:\n",
    "\n",
    "            x1 = x1[:, None, :]\n",
    "            x2 = x2[None, :, :]\n",
    "\n",
    "            scales = self.scales[None, None, :] ** 2\n",
    "\n",
    "        # Compute quadratic, exponentiate and multiply by coefficient\n",
    "        quad = - 0.5 * (x1 - x2) ** 2 / scales\n",
    "        quad = tf.reduce_sum(quad, axis=-1)\n",
    "        eq_cov = self.coeff ** 2 * tf.exp(quad)\n",
    "        \n",
    "        # Add jitter for invertibility\n",
    "        if epsilon is not None:\n",
    "            eq_cov = eq_cov + epsilon * tf.eye(eq_cov.shape[0], \n",
    "                                               dtype=self.dtype)\n",
    "\n",
    "        return eq_cov\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def scales(self):\n",
    "        return tf.math.exp(self.log_scales)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def coeff(self):\n",
    "        return tf.math.exp(self.log_coeff)\n",
    "    \n",
    "    \n",
    "    def sample_rff(self, x, num_functions, num_features):\n",
    "        \n",
    "        omega = np.random.normal(size=(num_functions, num_features, self.dim))\n",
    "\n",
    "        # Scale omegas by lengthscale -- same operation for all three kernels\n",
    "        omega = omega / self.scales\n",
    "\n",
    "        weights = np.random.normal(loc=0.,\n",
    "                                   scale=1.,\n",
    "                                   size=(num_functions, num_features))\n",
    "\n",
    "        phi = np.random.uniform(low=0.,\n",
    "                                high=(2 * np.pi),\n",
    "                                size=(num_functions, num_features, 1))\n",
    "\n",
    "        features = np.cos(np.einsum('sfd, nd -> sfn', omega, x) + phi)\n",
    "        features = (2 / num_features) ** 0.5 * features * coefficient\n",
    "\n",
    "        functions = np.einsum('sf, sfn -> sn', weights, features)\n",
    "\n",
    "        return functions, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-random-walks",
   "language": "python",
   "name": "venv-random-walks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
