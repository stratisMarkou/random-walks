
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Variational Inference Revisited &#8212; Random walks</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://random-walks.org/content/gp/vfer/vfer.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Miscellaneous" href="../../misc/misc.html" />
    <link rel="prev" title="Efficiently sampling GP posteriors" href="../sparse/gp-sampling.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://random-walks.org/content/gp/vfer/vfer.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Variational Inference Revisited" />
<meta property="og:description" content="Variational Inference Revisited  Variational inference (VI) is one of the standard methods for working with analytically intractable models, such as non-conjuga" />
<meta property="og:image"       content="https://random-walks.org/_static/logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Random walks</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../home.html">
   Random walks
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../prob-intro/intro.html">
   Probability: An introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch01/content.html">
     Events and Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch02/content.html">
     Discrete random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch03/content.html">
     Multivariate discrete distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch04/content.html">
     Probability generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch05/content.html">
     Distribution and density functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch06/content.html">
     Multivariate distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch07/content.html">
     Moment generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch08/content.html">
     Main limit theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch09/content.html">
     Branching processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch10/content.html">
     Random walks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch11/content.html">
     Processes in continuous time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch12/content.html">
     Markov chains
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../cvx/intro.html">
   Convex optimisation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch01.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch02.html">
     Convex sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch03.html">
     Convex functions
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../gp-intro.html">
   Gaussian Processes
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../why-covariances.html">
     Why covariance functions?
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../sparse/sparse-intro.html">
     Sparse Gaussian Processes
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../sparse/vfe.html">
       Variational Free Energy GPs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../sparse/gp-sampling.html">
       Sampling GP posteriors
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Variational Inference Revisited
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../misc/misc.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/sde/num-sde.html">
     Numerical simulation of SDEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/sde-as-gp/sde-as-gp.html">
     VI for diffusion processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/node/node.html">
     Neural ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/optimisation/conjugate-gradients.html">
     Conjugate gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/kalman/kalman.html">
     The Kalman filter and smoother
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/ncs/ncs.html">
     Natural cubic splines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/ars/ars.html">
     Adaptive rejection sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/score-matching/score-matching.html">
     Estimation by score matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/interacting/interacting.html">
     Interacting particle FPK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/rff/rff.html">
     Random Fourier features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/svgd/svgd.html">
     Stein variational gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/addgp/addgp.html">
     Additive Gaussian Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/gumbel/gumbel.html">
     Gumbel distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/gip/gip.html">
     Global inducing points for BNNs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reading-and-links.html">
   Interesting reading and websites
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/gp/vfer/vfer.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/stratisMarkou/random-walks/master?urlpath=tree/./content/gp/vfer/vfer.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-asssumptions">
   Prior asssumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimal-variational-posterior">
   Optimal variational posterior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiments">
   Experiments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cauchy-regression">
     Cauchy regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binary-classification">
     Binary classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="variational-inference-revisited">
<h1>Variational Inference Revisited<a class="headerlink" href="#variational-inference-revisited" title="Permalink to this headline">¶</a></h1>
<p>Variational inference (VI) is one of the standard methods for working with analytically intractable models, such as non-conjugate probabilistic models.
VI approximates an analytically intractable distribution with another distribution from a family of tractable distributions.
A typical choice which practitioners and researchers make is to choose a Gaussian family of distributions.
An <span class="math notranslate nohighlight">\(N\)</span>-dimensional multivariate Gaussian requires <span class="math notranslate nohighlight">\(N\)</span> parameters to specify its mean and <span class="math notranslate nohighlight">\(\frac{N(N+1)}{2}\)</span> parameters to specify its covariance.
Opper and Archembeau <a class="bibtex reference internal" href="#opper2009variational" id="id1">[OA09]</a> however have shown that for a certain class of probabilistic models, those which have a Gaussian prior and a factorising likelihood, the optimal posterior can be written in terms of the prior using only <span class="math notranslate nohighlight">\(N\)</span>, rather than <span class="math notranslate nohighlight">\(\frac{N(N+1)}{2}\)</span>, parameters.
This means that the optimal approximate posterior requires only need <span class="math notranslate nohighlight">\(2N\)</span> parameters in total.
Here we derive the family of Gaussian variational posteriors introduced by Opper and Archembeau, and apply it to two examples of non-conjugate Gaussian Process (GP) models <a class="bibtex reference internal" href="#williams2006gaussian" id="id2">[WR06]</a> for heavy-tailed regression and classification.</p>
<div class="section" id="prior-asssumptions">
<h2>Prior asssumptions<a class="headerlink" href="#prior-asssumptions" title="Permalink to this headline">¶</a></h2>
<p>Consider a model with latent variables <span class="math notranslate nohighlight">\(\mathbf{z} = (z_1, \dots, z_N)\)</span>, distributed according to a Gaussian prior and observed variables <span class="math notranslate nohighlight">\(\mathbf{y} = (y_1, \dots, y_N)\)</span> which, conditionally on <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, are distributed according to a factorising likelihood</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
p(\mathbf{y}, \mathbf{z} | \theta) = p(\mathbf{z} | \theta) \prod_{n=1}^N p(y_n | z_n, \theta) = \mathcal{N}(\mathbf{z}; \mathbf{0}, \mathbf{K}) \prod_{n=1}^N p(y_n | z_n, \theta).
\end{equation}\]</div>
<p>In general, unless the likelihood is Gaussian, such a model will be analytically intractable, and will require approximations both for inferring the distribution over the latent variables <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> as well as learning good hyperparameters <span class="math notranslate nohighlight">\(\theta\)</span>.
Variational inference (VI) is a standard approximation for a setting such as this.</p>
</div>
<div class="section" id="optimal-variational-posterior">
<h2>Optimal variational posterior<a class="headerlink" href="#optimal-variational-posterior" title="Permalink to this headline">¶</a></h2>
<p>We are interested in both inferring the distribution over the latent variables <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, as well as in learning good hyperparameters <span class="math notranslate nohighlight">\(\theta\)</span>.
We can achive this by approximating the posterior <span class="math notranslate nohighlight">\(p(\mathbf{z} | \mathbf{y}, \theta)\)</span> with a Gaussian distribution <span class="math notranslate nohighlight">\(q(\mathbf{z}) = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}, \mathbf{\Sigma})\)</span>, and maximising the Variational Free Energy (VFE) objective</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathcal{F}(q, \theta) = \int q(\mathbf{z}) \log \frac{p(\mathbf{y}, \mathbf{z} | \mathbf{x}, \theta)}{q(\mathbf{z})} d\mathbf{z} \leq \log p(\mathbf{y} | \theta),
\end{align}\]</div>
<p>also known as the Evidence Lower Bound (ELBO), with respect to both the variatinal parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, as well as the hyperparameters <span class="math notranslate nohighlight">\(\theta\)</span>.
It would appear that parameterising <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> would require <span class="math notranslate nohighlight">\(\frac{N(N+1)}{2}\)</span> parameters.
However, Opper and Archembeau <a class="bibtex reference internal" href="#opper2009variational" id="id3">[OA09]</a> show that the optimal posterior covariance can be written down with only <span class="math notranslate nohighlight">\(N\)</span> parameters.</p>
<div class="lemma">
<p><strong>Lemma (Free energy for factorising model with a Gaussian prior)</strong> Consider a model with a Gaussian prior and a factorising likelihood, written as</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
p(\mathbf{y}, \mathbf{z} | \theta) = \mathcal{N}(\mathbf{z}; \mathbf{0}, \mathbf{K}) \prod_{n=1}^N p(y_n | z_n, \theta).
\end{equation}\]</div>
<p>Assuming a Gaussian variational posterior <span class="math notranslate nohighlight">\(q(\mathbf{z}) = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}, \mathbf{K})\)</span>, the free energy <span class="math notranslate nohighlight">\(\mathcal{F}(q, \theta)\)</span> of the model is</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\mathcal{F}(q, \theta) = \sum_{n=1}^N \mathbb{E}_{q_n}\left[\log p(y_n | z_n, \theta)\right] - \frac{1}{2} \text{Tr}(\mathbf{K}^{-1}\boldsymbol{\Sigma}) - \boldsymbol{\mu}^\top\mathbf{K}^{-1}\boldsymbol{\mu} + \frac{1}{2} \log |\boldsymbol{\Sigma}| - \frac{1}{2} \log |\mathbf{K}| + C,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> is a constant which does not depend on <span class="math notranslate nohighlight">\(\boldsymbol{\mu}, \boldsymbol{\Sigma}\)</span> or <span class="math notranslate nohighlight">\(\theta\)</span>, and <span class="math notranslate nohighlight">\(q_n\)</span> is the marginal distribution of <span class="math notranslate nohighlight">\(z_n\)</span> under the variational posterior <span class="math notranslate nohighlight">\(q\)</span>.
Further, for fixed <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span>, the <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> which minimises <span class="math notranslate nohighlight">\(\mathcal{F}(q, \theta)\)</span> has the form</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\boldsymbol{\Sigma} = \left(\mathbf{K}^{-1} + \boldsymbol{\Lambda} \right)^{-1},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}\)</span> is a diagonal matrix.</p>
</div>
<br>
<details class="proof">
<summary>Derivation: Free energy for factorising model with a Gaussian prior</summary>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathcal{F}(q, \theta) &amp;= \int q(\mathbf{z}) \log \frac{p(\mathbf{y}, \mathbf{z} | \theta)}{q(\mathbf{z})} d\mathbf{z} \\
                       &amp;= \underbrace{\int q(\mathbf{z}) \log p(\mathbf{y} | \mathbf{z}, \theta) d\mathbf{z}}_{=~ \sum_{n=1}^N \mathbb{E}_{q_n}\left(\log p(y_n | z_n, \theta)\right)} + \int q(\mathbf{z}) \log p(\mathbf{z} | \theta) d\mathbf{z} - \int q(\mathbf{z}) \log q(\mathbf{z}) d\mathbf{z},
\end{align}\end{split}\]</div>
<p>where we have used the factorising structure of the model.
Substituting the expressions <span class="math notranslate nohighlight">\(q(\mathbf{z}) = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span> and <span class="math notranslate nohighlight">\(p(\mathbf{z} | \theta) = \mathcal{N}(\mathbf{z}; \mathbf{0}, \mathbf{K})\)</span> we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathcal{F}(q, \theta) &amp;= \sum_{n=1}^N \mathbb{E}_{q_n}\left[\log p(y_n | z_n, \theta)\right]  \\
                       &amp;~~~~~~~~~~~~~~~~~~+ \frac{1}{2} \int q(\mathbf{z}) \left(- \log |\mathbf{K}| - \mathbf{z}^\top \mathbf{K}^{-1}\mathbf{z}\right)d\mathbf{z} \\
                       &amp;~~~~~~~~~~~~~~~~~~- \frac{1}{2} \int q(\mathbf{z}) \left(-\log |\boldsymbol{\Sigma}| - (\mathbf{z} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{z} - \boldsymbol{\mu})\right) d\mathbf{z} \\
                       &amp;~~~~~~~~~~~~~~~~~~+ \text{ const.} \\
                       &amp;= \sum_{n=1}^N \mathbb{E}_{q_n}\left[\log p(y_n | z_n, \theta)\right] - \frac{1}{2} \log |\mathbf{K}| - \text{Tr}(\mathbf{K}^{-1} \boldsymbol{\Sigma}) - \frac{1}{2} \boldsymbol{\mu}^\top \mathbf{K}^{-1} \boldsymbol{\mu} + \frac{1}{2} \log |\boldsymbol{\Sigma}| + \text{ const.}
\end{align}\end{split}\]</div>
<p>where the constant terms do not depend on <span class="math notranslate nohighlight">\(\boldsymbol{\mu}, \boldsymbol{\Sigma}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.
Further, we can take a derivative with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, and set the resulting expression to zero to solve for the optimal <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>.
We note that the <span class="math notranslate nohighlight">\(n^{th}\)</span> log-likelihood term depends only on the <span class="math notranslate nohighlight">\(n^{th}\)</span> diagonal entry of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, and none of the non-diagonal entries.
Thus, taking a derivative with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{ij}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\frac{d}{d\boldsymbol{\Sigma}_{ij}} \mathcal{F}(q, \theta) &amp;= \underbrace{\delta_{ij} \frac{d}{d\boldsymbol{\Sigma}_{ii}} \mathbb{E}_{q_i}\left(\log p(y_i | z_i, \theta)\right)}_{- \boldsymbol{\Lambda}_{ij}} - \mathbf{K}_{ij}^{-1} + \boldsymbol{\Sigma}^{-1}_{ij}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta_{\cdot, \cdot}\)</span> is the Kronecker delta function.
Defining the matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}\)</span> as shown above, setting the derivative equal to zero and rearraning, we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\boldsymbol{\Sigma} = \left(\mathbf{K}^{-1} + \boldsymbol{\Lambda}\right)^{-1}.
\end{align}\]</div>
</details>
<br>
<p>For given <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, the optimal precision is the prior precision plus a diagonal term.
Therefore, a sensible parameterisation for <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\boldsymbol{\Sigma} \stackrel{\text{def}}{=} \left(\mathbf{K}^{-1} + \boldsymbol{\Lambda}\right)^{-1},
\end{equation}\]</div>
<p>where we let <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \dots, \lambda_N)\)</span> be diagonal matrix with <span class="math notranslate nohighlight">\(N\)</span> parameters which we constrain to be positive.
Using <span class="math notranslate nohighlight">\(N\)</span> further parameters for the mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>, we can parameterise a family of Gaussian variational posteriors which contains the optimal Gaussian variational posterior, using only <span class="math notranslate nohighlight">\(2N\)</span> parameters.
This is a significant improvement in the memory cost over naive covariance parameterisations, such as parameterising a Cholesky factor of the <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> matrix.</p>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>We now implement this approximate posterior, in the context of Gaussian process (GP) modelling. In these experiments, we will use a GP with a zero mean and an Exponentiated Quadratic (EQ) covariancce, implemented by the <code class="docutils literal notranslate"><span class="pre">EQCovariance</span></code> class below.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EQCovariance</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">log_coeff</span><span class="p">,</span>
                 <span class="n">log_scales</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">trainable</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;eq_covariance&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
        <span class="c1"># Convert parameters to tensors</span>
        <span class="n">log_coeff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_coeff</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">log_scales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_scales</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Reshape parameter tensors</span>
        <span class="n">log_coeff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">log_coeff</span><span class="p">)</span>
        <span class="n">log_scales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">log_scales</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
        
        <span class="c1"># Set input dimensionality</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        
        <span class="c1"># Set EQ parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_scales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">log_scales</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_coeff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">log_coeff</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">x1</span><span class="p">,</span>
                 <span class="n">x2</span><span class="p">,</span>
                 <span class="n">diag</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="c1"># Convert to tensors</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Get vector of lengthscales</span>
        <span class="n">scales</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales</span>
        
        <span class="c1"># If calculating full covariance, add dimensions to broadcast</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">diag</span><span class="p">:</span>

            <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

            <span class="n">scales</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="c1"># Compute quadratic, exponentiate and multiply by coefficient</span>
        <span class="n">quad</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">scales</span>
        <span class="n">quad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">quad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">eq_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">quad</span><span class="p">)</span>
        
        <span class="c1"># Add jitter for invertibility</span>
        <span class="k">if</span> <span class="n">epsilon</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">eq_cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">eq_cov</span> <span class="o">=</span> <span class="n">eq_cov</span> <span class="o">+</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">eps</span>

        <span class="k">return</span> <span class="n">eq_cov</span>
        
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">scales</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_scales</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">coeff</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can now implement a general non-conjugate GP, which should work with arbitrary factorising likelihood functions. The <code class="docutils literal notranslate"><span class="pre">NonConjugateGP</span></code> class below takes in the training data, a covariance function and the training data. It supports computing the VFE and sampling the approximate posterior.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NonConjugateGP</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">x_train</span><span class="p">,</span>
                 <span class="n">y_train</span><span class="p">,</span>
                 <span class="n">covariance</span><span class="p">,</span>
                 <span class="n">likelihood</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">&quot;non_conjugate_gp&quot;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="n">check_shape</span><span class="p">([</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">],</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,)])</span>
        
        <span class="c1"># Set training data and inducing point initialisation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span>
        
        <span class="c1"># Set mean, covariance and likelihood functions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span> <span class="o">=</span> <span class="n">covariance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood</span>
        
        <span class="c1"># Set variational parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,),</span>
                     <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">log_lamda</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="o">-</span><span class="mf">4.</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># Set epsilon for jitter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lamda</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_lamda</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
        
        <span class="n">Kpp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="n">Ktp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">)</span>
        <span class="n">Kpt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">)</span>
        <span class="n">Ktt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
        
        <span class="n">Sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_covariance</span>
        <span class="n">Sigma_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>
        
        <span class="n">qz</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalTriL</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">,</span>
            <span class="n">scale_tril</span><span class="o">=</span><span class="n">Sigma_chol</span>
        <span class="p">)</span>
        
        <span class="n">samples</span> <span class="o">=</span> <span class="n">qz</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,))</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        
        <span class="n">mcond</span> <span class="o">=</span> <span class="n">Kpt</span> <span class="o">@</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Ktt</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
        <span class="n">mcond</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">mcond</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        
        <span class="n">Kcond</span> <span class="o">=</span> <span class="n">Kpp</span> <span class="o">-</span> <span class="n">Kpt</span> <span class="o">@</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Ktt</span><span class="p">,</span> <span class="n">Ktp</span><span class="p">)</span>
        <span class="n">Kcond_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">Kcond</span><span class="p">)</span>
        
        <span class="n">qcond</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalTriL</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">mcond</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="n">scale_tril</span><span class="o">=</span><span class="n">Kcond_chol</span>
        <span class="p">)</span>
        
        <span class="n">samples</span> <span class="o">=</span> <span class="n">mcond</span> <span class="o">+</span> <span class="n">qcond</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,))</span>
        <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">samples</span><span class="p">,</span> <span class="n">transformed_samples</span>
        
        
    <span class="k">def</span> <span class="nf">free_energy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        
        <span class="n">cond_lik</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="n">q</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalFullCovariance</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">,</span>
            <span class="n">covariance_matrix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_covariance</span>
        <span class="p">)</span>
        
        <span class="n">p</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalFullCovariance</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">covariance_matrix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p_covariance</span>
        <span class="p">)</span>
        
        <span class="c1"># Compute log likelihood contribution from each sample</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
            
            <span class="n">z</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            
            <span class="n">cond_lik</span> <span class="o">=</span> <span class="n">cond_lik</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
            <span class="p">)</span>
            
        <span class="n">cond_lik</span> <span class="o">=</span> <span class="n">cond_lik</span> <span class="o">/</span> <span class="n">num_samples</span>
        
        <span class="c1"># Compute KL divergence contribution</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
        
        <span class="n">free_energy</span> <span class="o">=</span> <span class="n">cond_lik</span> <span class="o">-</span> <span class="n">kl</span>
        
        <span class="k">return</span> <span class="n">free_energy</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">q_covariance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="n">eye</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="n">Ktt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
            <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>
        <span class="p">)</span>
        
        <span class="n">Ktt_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">Ktt</span><span class="p">)</span>
        <span class="n">iKtt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="n">Ktt_chol</span><span class="p">,</span> <span class="n">eye</span><span class="p">)</span>
        
        <span class="n">Lamda</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lamda</span><span class="p">)</span>
        
        <span class="n">iSigma_tril</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">iKtt</span> <span class="o">+</span> <span class="n">Lamda</span><span class="p">)</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">triangular_solve</span><span class="p">(</span><span class="n">iSigma_tril</span><span class="p">,</span> <span class="n">eye</span><span class="p">)</span>
        <span class="n">Sigma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">Sigma</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">p_covariance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="n">p_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
            <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>
        <span class="p">)</span>
        
        <span class="k">return</span> <span class="n">p_cov</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">¶</a></h2>
<p>Now that we have the <code class="docutils literal notranslate"><span class="pre">NonConjugateGP</span></code> model in place, we can use it with arbitrary factorising likelihoods. First, we consider a non-Gaussian regression task with Cauchy noise, followed by a binary classification task.</p>
<div class="section" id="cauchy-regression">
<h3>Cauchy regression<a class="headerlink" href="#cauchy-regression" title="Permalink to this headline">¶</a></h3>
<p>The likelihood function for a Cauchy noise model has the form</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
p(y_n | z_n, \sigma^2) = \pi\sigma ~ \frac{1}{1 + \left(\frac{y_n - z_n}{\sigma}\right)^2},
\end{equation}\]</div>
<p>which together with a Gaussian prior leads to a non-conjugate model. The Cauchy likelihood has heavy tails, much heavier than those of a Gaussian likelihood. Because of this, Cauchy likelihoods are much more robust to outliers than Gaussian likelihoods. Below we generate data from a GP with an EQ covariance and a Cauchy noise model, which results in a few heavy outliers.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>

<span class="n">num_grid</span> <span class="o">=</span> <span class="mi">400</span>
<span class="n">num_train</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">x_min</span> <span class="o">=</span> <span class="o">-</span><span class="mf">5.</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="mf">5.</span>

<span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">log_coeff</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">log_scales</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">*</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]</span>
<span class="n">log_noise</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">covariance</span> <span class="o">=</span> <span class="n">EQCovariance</span><span class="p">(</span>
    <span class="n">log_coeff</span><span class="p">,</span>
    <span class="n">log_scales</span><span class="o">=</span><span class="n">log_scales</span><span class="p">,</span>
    <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">num_grid</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="n">Ktt</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-9</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_grid</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">Lxx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">Ktt</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matvec</span><span class="p">(</span><span class="n">Lxx</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_grid</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_noise</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_grid</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_train</span><span class="p">,),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">idx</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="n">z_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Cauchy</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">z_train</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">z_train</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/vfer_8_0.svg" src="../../../_images/vfer_8_0.svg" /></div>
</div>
<p>We now implement a <code class="docutils literal notranslate"><span class="pre">CauchyLikelihood</span></code> class, which really is just a wrapper around the Cauchy distribution, together with a learnable noise level.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CauchyLikelihood</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">log_noise</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">&quot;binary_classification_likelihood&quot;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># Set log noise parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_noise</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">z</span>
        
    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Cauchy</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">noise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can initialise the full model, and train it on the synthetic data we generated earlier. We will fix the covariance parameters to their ground truth values, and only train the variational parameters. This is a good way to assess the quality of the posterior approximation without confounding this with the quality of the hyperparameters learnt by the model.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We decorate a single gradient descent step with tf.function. On the first</span>
<span class="c1"># call of single_step, tensorflow will compile the computational graph first.</span>
<span class="c1"># After that, all calls to single_step will use the compiled graph which is</span>
<span class="c1"># much faster than the default eager mode execution. In this case, the gain</span>
<span class="c1"># is roughly a x20 speedup (with a CPU), which can be checked by commenting</span>
<span class="c1"># out the decorator and rerunning the training script.</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">single_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimiser</span><span class="p">):</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

        <span class="n">free_energy</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">free_energy</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">free_energy</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimiser</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">free_energy</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span>

<span class="c1"># =============================================================================</span>
<span class="c1"># Define the model</span>
<span class="c1"># =============================================================================</span>


<span class="c1"># Model parameters</span>
<span class="n">trainable_covariance</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">log_coeff</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">log_scales</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">*</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]</span>
<span class="n">log_noise</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.</span>
    
<span class="n">covariance</span> <span class="o">=</span> <span class="n">EQCovariance</span><span class="p">(</span>
    <span class="n">log_coeff</span><span class="o">=</span><span class="n">log_coeff</span><span class="p">,</span>
    <span class="n">log_scales</span><span class="o">=</span><span class="n">log_scales</span><span class="p">,</span>
    <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="n">trainable_covariance</span>
<span class="p">)</span>

<span class="n">Cauchy_likelihood</span> <span class="o">=</span> <span class="n">CauchyLikelihood</span><span class="p">(</span>
    <span class="n">log_noise</span><span class="o">=</span><span class="n">log_noise</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
<span class="p">)</span>

<span class="n">non_conjugate_gp</span> <span class="o">=</span> <span class="n">NonConjugateGP</span><span class="p">(</span>
    <span class="n">x_train</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">covariance</span><span class="o">=</span><span class="n">covariance</span><span class="p">,</span>
    <span class="n">likelihood</span><span class="o">=</span><span class="n">Cauchy_likelihood</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
<span class="p">)</span>

<span class="c1"># =============================================================================</span>
<span class="c1"># Train the model</span>
<span class="c1"># =============================================================================</span>

<span class="c1"># Number of steps to train for</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="c1"># Initialise optimiser</span>
<span class="n">optimiser</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    
<span class="c1"># Set progress bar and suppress warnings</span>
<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">get_logger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="s1">&#39;ERROR&#39;</span><span class="p">)</span>

<span class="c1"># Set tensors for keeping track of quantities of interest</span>
<span class="n">train_vfe</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_cond_lik</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_kl</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Train model</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
        
    <span class="n">free_energy</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">single_step</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">non_conjugate_gp</span><span class="p">,</span>
        <span class="n">optimiser</span><span class="o">=</span><span class="n">optimiser</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        
        <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;VFE </span><span class="si">{</span><span class="n">free_energy</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Cond-lik. </span><span class="si">{</span><span class="n">cond_lik</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;KL </span><span class="si">{</span><span class="n">kl</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        
    <span class="n">train_vfe</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">free_energy</span><span class="p">)</span>
    <span class="n">train_cond_lik</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cond_lik</span><span class="p">)</span>
    <span class="n">train_kl</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kl</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>VFE -19.6, Cond-lik. 2.3, KL 21.9: 100%|██████████| 50000/50000 [00:30&lt;00:00, 1617.46it/s]  
</pre></div>
</div>
</div>
</div>
<p>Before plotting the posterior, let’s quickly verify that the model has trained to convergence, by looking at the curve of the training objective.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Helper for computing moving average</span>
<span class="k">def</span> <span class="nf">moving_average</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    
    <span class="n">cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
    <span class="n">cumsum</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span> <span class="o">=</span> <span class="n">cumsum</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span> <span class="o">-</span> <span class="n">cumsum</span><span class="p">[:</span><span class="o">-</span><span class="n">n</span><span class="p">]</span>
    
    <span class="n">moving_average</span> <span class="o">=</span> <span class="n">cumsum</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="n">n</span>
    
    <span class="k">return</span> <span class="n">moving_average</span>

<span class="c1"># Plot training objective</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">moving_average</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">train_vfe</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50000</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">70</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">60</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# optimisation steps&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;VFE&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/vfer_14_0.svg" src="../../../_images/vfer_14_0.svg" /></div>
</div>
<p>Below we see that the posterior for the noiseless function <span class="math notranslate nohighlight">\(z\)</span> is reasonable. Notably, the model gracefully handles the outliers around the <span class="math notranslate nohighlight">\(x = 1\)</span> region, where instead of overfitting to these points, the model recognises they are outliers and simply increases its uncertainty in <span class="math notranslate nohighlight">\(z\)</span>.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">sample_z</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">non_conjugate_gp</span><span class="o">.</span><span class="n">posterior</span><span class="p">(</span>
    <span class="n">x_pred</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>

<span class="n">median</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">sample_z</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Plot model fit</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">z</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">median</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Median $z$&quot;</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">sample_z</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/vfer_16_0.svg" src="../../../_images/vfer_16_0.svg" /></div>
</div>
</div>
<div class="section" id="binary-classification">
<h3>Binary classification<a class="headerlink" href="#binary-classification" title="Permalink to this headline">¶</a></h3>
<p>The likelihood for a binary classification task with a sigmoid activation has the form</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
p(y_n | z_n) = p_n^{y_n} (1 - p_n)^{1 - y_n}, ~\text{ where }~ p_n = \frac{1}{1 + e^{-z_n}}.
\end{equation}\]</div>
<p>Again, together with a Gaussian prior leads to a non-conjugate model which requires approximations. Below we generate some synthetic data by first sampling <span class="math notranslate nohighlight">\(z\)</span> from a GP with an EQ covariance, passing <span class="math notranslate nohighlight">\(z\)</span> through a sigmoid to obtain the class probablities and sampling the class to which each point belongs.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>

<span class="n">num_grid</span> <span class="o">=</span> <span class="mi">400</span>
<span class="n">num_train</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">x_min</span> <span class="o">=</span> <span class="o">-</span><span class="mf">5.</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="mf">5.</span>

<span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">log_coeff</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">log_scales</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">*</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">covariance</span> <span class="o">=</span> <span class="n">EQCovariance</span><span class="p">(</span>
    <span class="n">log_coeff</span><span class="p">,</span>
    <span class="n">log_scales</span><span class="o">=</span><span class="n">log_scales</span><span class="p">,</span>
    <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">num_grid</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="n">Ktt</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-9</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_grid</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">Lxx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">Ktt</span><span class="p">)</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matvec</span><span class="p">(</span><span class="n">Lxx</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_grid</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_grid</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_train</span><span class="p">,),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">idx</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="n">p_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">p_train</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

<span class="c1"># Plot ground truth and sampled data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">p</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;RdYlGn&quot;</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$p(x)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/vfer_18_0.svg" src="../../../_images/vfer_18_0.svg" /></div>
</div>
<p>We now implement a <code class="docutils literal notranslate"><span class="pre">BernoulliLikelihood</span></code> which, just like the <code class="docutils literal notranslate"><span class="pre">CauchyLikelihood</span></code>, is a wrapper around a tensorflow distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BernoulliLikelihood</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">&quot;binary_classification_likelihood&quot;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can now initialise and train the model. Because we made the <code class="docutils literal notranslate"><span class="pre">NonConjugateGP</span></code> class agnostic to the likelihood model, the only part that’s different to the previous model initialisation is that now we have passed a different likelihood.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We decorate a single gradient descent step with tf.function. On the first</span>
<span class="c1"># call of single_step, tensorflow will compile the computational graph first.</span>
<span class="c1"># After that, all calls to single_step will use the compiled graph which is</span>
<span class="c1"># much faster than the default eager mode execution. In this case, the gain</span>
<span class="c1"># is roughly a x20 speedup (with a CPU), which can be checked by commenting</span>
<span class="c1"># out the decorator and rerunning the training script.</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">single_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimiser</span><span class="p">):</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

        <span class="n">free_energy</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">free_energy</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">free_energy</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimiser</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">free_energy</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span>

<span class="c1"># =============================================================================</span>
<span class="c1"># Define the model</span>
<span class="c1"># =============================================================================</span>


<span class="c1"># Model parameters</span>
<span class="n">trainable_covariance</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">log_coeff</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">log_scales</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">*</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]</span>
    
<span class="n">covariance</span> <span class="o">=</span> <span class="n">EQCovariance</span><span class="p">(</span>
    <span class="n">log_coeff</span><span class="o">=</span><span class="n">log_coeff</span><span class="p">,</span>
    <span class="n">log_scales</span><span class="o">=</span><span class="n">log_scales</span><span class="p">,</span>
    <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="n">trainable_covariance</span>
<span class="p">)</span>

<span class="n">binary_classification_likelihood</span> <span class="o">=</span> <span class="n">BernoulliLikelihood</span><span class="p">(</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
<span class="p">)</span>

<span class="n">non_conjugate_gp</span> <span class="o">=</span> <span class="n">NonConjugateGP</span><span class="p">(</span>
    <span class="n">x_train</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">covariance</span><span class="o">=</span><span class="n">covariance</span><span class="p">,</span>
    <span class="n">likelihood</span><span class="o">=</span><span class="n">binary_classification_likelihood</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
<span class="p">)</span>

<span class="c1"># =============================================================================</span>
<span class="c1"># Train the model</span>
<span class="c1"># =============================================================================</span>


<span class="c1"># Number of training steps</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="c1"># Initialise optimiser</span>
<span class="n">optimiser</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    
<span class="c1"># Set progress bar and suppress warnings</span>
<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">get_logger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="s1">&#39;ERROR&#39;</span><span class="p">)</span>

<span class="c1"># Set tensors for keeping track of quantities of interest</span>
<span class="n">train_vfe</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_cond_lik</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_kl</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Train model</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
        
    <span class="n">free_energy</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">single_step</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">non_conjugate_gp</span><span class="p">,</span>
        <span class="n">optimiser</span><span class="o">=</span><span class="n">optimiser</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        
        <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;VFE </span><span class="si">{</span><span class="n">free_energy</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Cond-lik. </span><span class="si">{</span><span class="n">cond_lik</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;KL </span><span class="si">{</span><span class="n">kl</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        
    <span class="n">train_vfe</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">free_energy</span><span class="p">)</span>
    <span class="n">train_cond_lik</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cond_lik</span><span class="p">)</span>
    <span class="n">train_kl</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kl</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>VFE -23.7, Cond-lik. -17.8, KL 5.9: 100%|██████████| 50000/50000 [00:42&lt;00:00, 1187.53it/s]
</pre></div>
</div>
</div>
</div>
<p>Again, we verify that the model has trained to convergence by looking at the training objective.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot training objective</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">moving_average</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">train_vfe</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50000</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">60</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">60</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# optimisation steps&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;VFE&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/vfer_24_0.svg" src="../../../_images/vfer_24_0.svg" /></div>
</div>
<p>Last we plot the model fit. The model has learnt a reasonable posterior for <span class="math notranslate nohighlight">\(p = \frac{1}{1 + e^{-z}}\)</span>. Note that even in regions where several data are available, the model maintains quite a bit of uncertainty in <span class="math notranslate nohighlight">\(p\)</span>. This is likely because a few binary observations in some region are not enough to pin down <span class="math notranslate nohighlight">\(p\)</span> with enough certainty.</p>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of posterior samples to draw</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">q</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalFullCovariance</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="n">non_conjugate_gp</span><span class="o">.</span><span class="n">mu</span><span class="p">,</span>
    <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">non_conjugate_gp</span><span class="o">.</span><span class="n">q_covariance</span>
<span class="p">)</span>

<span class="n">sample_z</span><span class="p">,</span> <span class="n">sample_p</span> <span class="o">=</span> <span class="n">non_conjugate_gp</span><span class="o">.</span><span class="n">posterior</span><span class="p">(</span>
    <span class="n">x_pred</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span>
<span class="p">)</span>

<span class="n">mean_p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">sample_p</span><span class="p">,</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>

<span class="n">median</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">sample_p</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Plot model fit</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">p</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">median</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Median $p$&quot;</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">sample_p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;RdYlGn&quot;</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$p(x)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/vfer_26_0.svg" src="../../../_images/vfer_26_0.svg" /></div>
</div>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>We saw that when performing VI for models with a Gaussian prior and a factorising likelihood, while using a Gaussian variational posterior, then following Opper and Archembeau, <a class="bibtex reference internal" href="#opper2009variational" id="id4">[OA09]</a> we can write the optimal variational posterior in terms of the prior and an additional <span class="math notranslate nohighlight">\(2N\)</span> variational parameters. We saw that parameterising the variational posterior in this way produces sensible fits in two non-conjugate Gaussian process models.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-content/gp/vfer/vfer-0"><dl class="citation">
<dt class="bibtex label" id="opper2009variational"><span class="brackets">OA09</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>,<a href="#id4">3</a>)</span></dt>
<dd><p>Manfred Opper and Cédric Archambeau. The variational gaussian approximation revisited. <em>Neural computation</em>, 2009.</p>
</dd>
<dt class="bibtex label" id="williams2006gaussian"><span class="brackets"><a class="fn-backref" href="#id2">WR06</a></span></dt>
<dd><p>Christopher K Williams and Carl Edward Rasmussen. <em>Gaussian processes for machine learning</em>. MIT press Cambridge, MA, 2006.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "venv-random-walks"
        },
        kernelOptions: {
            kernelName: "venv-random-walks",
            path: "./content/gp/vfer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'venv-random-walks'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../sparse/gp-sampling.html" title="previous page">Efficiently sampling GP posteriors</a>
    <a class='right-next' id="next-link" href="../../misc/misc.html" title="next page">Miscellaneous</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratos Markou<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>