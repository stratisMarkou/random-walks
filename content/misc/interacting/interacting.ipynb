{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting particle solutions of the FPK\n",
    "\n",
    "One central problem of interest in modelling with Stochastic Differential Equations (SDEs), is solving the Fokker-Planck-Kolmogorov equation (FPK). Given a particle whose state $x$ evolves through time $t$ according to an SDE, the FPK describes how the probability density $p_t(x)$ evolves with time. The FPK is of central interest in many modelling, physical and statistical applications. Unfortunately, with few exceptions such as linear SDEs, the FPK cannot be solved in closed form and must be approximated numerically.\n",
    "\n",
    "Many numerical solution schemes have been developed for solving the FPK, {cite}`sarkka2019applied` including finite element methods and Monte Carlo simulations. Finite element methods are deterministic and typically depend on discretisations through state-space and time and thus scale poorly with the dimensionality of the state and the region being discretised. On the other hand, Monte Carlo simulations - which amount to simulating paths from the SDE and using these as an empirical approximation of $p_t(x)$ - scale better with dimensionality but due to the finite number of sampled paths they induce significant variance in downstream estimators using these paths. Recently, Maoutsa et. al {cite}`maoutsa2020interacting` have developed a numerical approximation scheme which solves for the FPK by simulating deterministic trajectories of an ODE. This method does not involve a discretisation and scales gracefully to higher dimensions. In addition, it is deterministic and therefore the empirically estimated $p_t(x)$ achieves lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.d-flex {\n",
       "    display: none!important;\n",
       "}\n",
       "\n",
       ".bd-toc nav {\n",
       "    opacity: 1;\n",
       "    max-height: 100vh!important;\n",
       "}\n",
       "\n",
       ".bd-toc .nav .nav {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".prev-next-bottom {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       ".footer {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       ".navbar_footer {\n",
       "\tdisplay: none;\n",
       "}\n",
       "\n",
       ".definition {\n",
       "\tbackground-color: rgba(123, 183, 223, 0.33);\n",
       "\tborder-radius: 20px;\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "\tborder-style: solid;\n",
       "\tborder-color: rgba(123, 183, 223, 0.5);\n",
       "}\n",
       "\n",
       ".theorem {\n",
       "\tbackground-color: rgba(240, 213, 75, 0.33);\n",
       "\tborder-radius: 20px;\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "\tborder-style: solid;\n",
       "\tborder-color: rgba(240, 213, 75, 0.5);\n",
       "}\n",
       "\n",
       ".lemma {\n",
       "\tbackground-color: rgba(255, 218, 185, 0.33);\n",
       "\tborder-radius: 20px;\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "\tborder-style: solid;\n",
       "\tborder-color: rgba(255, 218, 185, 0.5);\n",
       "}\n",
       "\n",
       ".observation {\n",
       "\tbackground-color: rgba(178, 234, 188, 0.33);\n",
       "\tborder-radius: 20px;\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "\tborder-style: solid;\n",
       "\tborder-color: rgba(178, 234, 188, 0.5);\n",
       "}\n",
       "\n",
       "details.proof {\n",
       "    border-width: 2px;\n",
       "\tborder-radius: 20px;\n",
       "\tborder-style: solid;\n",
       "\tbackground-color: rgba(128, 128, 128, 0.2);\n",
       "\tborder-color: rgba(128, 128, 128, 0.1);\n",
       "}\n",
       "\n",
       "\n",
       "details.proof div{\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "}\n",
       "\n",
       "details.proof {\n",
       "\tpadding: 0px 30px 0px 30px;\n",
       "}\n",
       "\n",
       "summary {\n",
       "\tmargin-left: -32px;\n",
       "\tpadding-left: 15px;\n",
       "\tpadding-right: 15px;\n",
       "}\n",
       "\n",
       "summary + * {\n",
       "    margin-top: 10px;\n",
       "}\n",
       "\n",
       ".tag_center-output div img {\n",
       "    display:block;\n",
       "    margin:auto;\n",
       "}\n",
       "\n",
       ".container {\n",
       "\twidth : 103% !important;\n",
       "}\n",
       "\n",
       "details > .math.notranslate.nohighlight {\n",
       "\tmargin-top: -50px;\n",
       "\tmargin-bottom: -15px;\n",
       "}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML, set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "css_style = open('../../../_static/custom_style.css', 'r').read()\n",
    "HTML(f'<style>{css_style}</style>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fokker-Planck-Kolmogorov\n",
    "\n",
    "We begin by introducing the problem this method is trying to solve. Given an SDE of the form\n",
    "\n",
    "$$\\begin{align}\n",
    "dx = f(x, t)dt + \\sigma(x, t) d\\beta\n",
    "\\end{align}$$\n",
    "\n",
    "with initial distribution $p_0(x)$, drift function $f : \\mathbb{R}^D \\times \\mathbb{R} \\to \\mathbb{R}^D$ and diffusion function $\\sigma : \\mathbb{R}^D \\times \\mathbb{R} \\to \\mathbb{R}^D$ and a standard Brownian motion $\\beta$, the FPK describes the evolution of the distribution $p_t(x)$ of $x$ at time $t$ as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial p_t(x)}{\\partial t} = - \\sum_{i = 1}^D \\frac{\\partial}{\\partial x_i} [f_i(x, t) p_t(x)] + \\frac{1}{2} \\sum_{i = 1}^D\\sum_{j = 1}^D \\frac{\\partial^2}{\\partial x_i \\partial x_j} \\left[\\sigma_i(x, t) \\sigma_j(x, t)^\\top p_t(x)\\right].\n",
    "\\end{align}$$\n",
    "\n",
    "Here we will constrain ourselves to the special case where the drift is time-independent $f(x, t) = f(x)$ and the diffusion is constant $\\sigma(x, t) = \\sigma$. In this case, the FPK can be written in the form\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial p_t(x)}{\\partial t} = - \\nabla \\cdot \\left[ g(x, t) p_t(x) \\right] \\text{ where } g(x, t) = f(x) - \\frac{\\sigma^2}{2} \\nabla \\log p_t(x).\n",
    "\\end{align}$$\n",
    "\n",
    "This is a deterministic equation, which can be regarded as the Liouville equation of the dynamical system\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{d x}{d t} = g(x, t) \\text{ where } x_0 \\sim p_0(x).\n",
    "\\end{align}$$\n",
    "\n",
    "Therefore, if we sample $x_0 \\sim p_0(x)$ and evolve this up to time $t$ according to this ODE, we obtain $x_t$ distributed according to $p_t(x)$. The difficulty with this is that in order to evolve $x_0$ according to this ODE, we need access to $g(x, t)$. This itself depends on $p_t(x)$ which is the quantity we are trying to solve for, and do not have direct access to. One way to circumvent this problem is to use [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation). This method fits a probability distribution $\\hat{p}_t(x)$ to the empirical samples $x_t$, and $\\hat{p}_t(x)$ is then used as an approximation to $p_t(x)$. However, KDE methods place a restrictive form to the function $\\hat{p}_t(x)$. More generally, we can use any arbitrary function $p_t(x; w)$ parametrised using parameters $w$ and use this to approximate $p_t(x)$ using the log-gradient [gradient-log-density estimator](../score-matching/score-matching) (GLD). {cite}`hyvarinen2005estimation` We can also follow a non-parametric approach for modelling $p_t(x)$, however we do not explore this here -- see {cite}`maoutsa2020interacting` for more details on this.\n",
    "\n",
    "## Score matching objective\n",
    "\n",
    "Here we recapitulate the GLD. Given data that comes from a distribution $p_t(x)$, the GLD can be used to fit a parametrised distribution $p_t(x; w)$ to $p_t(x)$. In many cases of interest, we may know $p_t(x; w)$ only up to a multiplicative normalising constant\n",
    "\n",
    "$$\\begin{align}\n",
    "p_t(x; w) = \\frac{1}{Z} q_t(x; w).\n",
    "\\end{align}$$\n",
    "\n",
    "This occurs for example if we use a complicated parametric function $q_t(x; w)$, which we cannot integrate in closed form to determine the normalising constant $Z$. The GLD estimator uses the objective function\n",
    "\n",
    "$$\\begin{align}\n",
    "J(w) = \\frac{1}{2} \\int p_t(x) || \\psi_t(x; w) - \\psi_t(x) ||^2 dx.\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\psi_t(x; w) = \\nabla \\log p_t(x; w)$ and $\\psi_t(x) = \\nabla \\log p_t(x)$, and estimates the parameters $w$ as the minimiser of $J(w)$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "w_{opt} = \\argmin_{w} J(w).\n",
    "\\end{align}$$\n",
    "\n",
    "[It can be shown](../score-matching/score-matching) that minimising $J(w)$ with respect to $w$ is equivalent to minimising\n",
    "\n",
    "$$\\begin{align}\n",
    "J(w) = \\frac{1}{2} \\int p_t(x) \\left[ \\psi^\\top(x; w)\\psi(x; w) + 2 \\nabla \\cdot \\psi(x; w)  \\right] dx + \\text{const}.\n",
    "\\end{align}$$\n",
    "\n",
    "Unlike the previous form of $J$, this one involves $p_t(x)$ only outside the square brackets. This means that given samples from $p_t(x)$, we can approximate $J$ empirically as\n",
    "\n",
    "$$\\begin{align}\n",
    "J(w) = \\frac{1}{2} \\sum_{n = 1}^N \\left[ \\psi^\\top(x_n; w)\\psi(x_n; w) + 2 \\nabla \\cdot \\psi(x_n; w) \\right] dx \\text{ where } x_n \\sim p_t(x).\n",
    "\\end{align}$$\n",
    "\n",
    "## Explicit minimisation of $J$\n",
    "\n",
    "Choosing a linear in the parameters model\n",
    "\n",
    "$$\\begin{align}\n",
    "\\log p_t(x; w) = \\phi^\\top(x) w_t + \\text{const}.,\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\phi^\\top(x) = [\\phi_1(x), ..., \\phi_K(x)]$ is a collection of $K$ basis functions, and substituting into the gradient-log-density estimator we get\n",
    "\n",
    "$$\\begin{align}\n",
    "J(w_t) &= \\frac{1}{2} \\sum_{n = 1}^N \\left[ w_t^\\top \\Psi(x_n)^\\top \\Psi(x_n) w_t + 2 \\sum_{d = 1}^D g_d^\\top(x_n) w_t  \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\Psi_{ij}(x) = \\partial_i \\phi_j(x)$ and $g_{d}^\\top(x) = \\partial_d^2 \\phi^\\top(x)$. Note that this modelling choice is slightly different to the one employed in {cite}`maoutsa2020interacting`. In the original work, the authors use parametric functions to model the vector function $\\psi_t(x)$, whereas here we have used a parametric function to model the scalar function $\\log p_t(x)$. The benefit of the present approach is that it yields (up to a normalising constant) an explicit function for $\\log p_t(x)$, whereas the original method in the paper does not. Now, using the fact that $J(w)$ is a quadratic form of $w$, we can find its minimiser in closed form\n",
    "\n",
    "$$\\begin{align}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "w_{t, opt} = - \\left(\\sum^N_{n = 1} \\Psi^\\top(x_n) \\Psi(x_n) \\right)^{-1} \\left( \\sum_{d = 1}^D \\partial_d^2 \\phi(x) \\right).\n",
    "\\end{align}$$\n",
    "\n",
    "Note that $\\Psi(x_n)$ is a $D \\times K$ matrix, where $K$ is the number of  for this matrix to be invertible, we must have $N \\geq D$. We can also introduce a regularising term to $J(w)$ to prevent $w_t$ taking extreme values, by modifying the objective to $J(w_t) + \\lambda ||w_t||^2$ and obtaining the optimal $w_t$ as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "w_{t, opt} = - \\left(\\lambda I  + \\sum^N_{n = 1} \\Psi^\\top(x_n) \\Psi(x_n)\\right)^{-1} \\left( \\sum_{d = 1}^D \\partial_d^2 \\phi(x) \\right).\n",
    "\\end{align}$$\n",
    "\n",
    "Lastly, we can substitute $w_{opt}$ in our Liouville equation to obtain the explicit ODE\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{d x_n}{d t} = f(x_n) - \\frac{\\sigma^2}{2} \\phi^\\top(x_n) w_{t, opt}.\n",
    "\\end{align}$$\n",
    "\n",
    "<div class='definition'>\n",
    "    \n",
    "**Algorithm (Interacting particle FPK simulator)** Given an SDE with drift $f(x)$, diffusion $\\sigma$ and initial state distribution $p_0(x)$ the interacting particle simulator produces $N$ approximate samples from $p_t(x)$ by drawing $N$ samples from $x_n \\sim p_0(x)$ and evolving them according to the ODE\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{d x_n}{d t} = f(x_n) - \\frac{\\sigma^2}{2} \\Psi(x_n) w_{t, opt},\n",
    "\\end{align}$$\n",
    "\n",
    "where the time-dependent $w_{t, opt}$ is obtained as described above.\n",
    "    \n",
    "</div>\n",
    "<br>\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "This algorithm uses $N$ particles to empirically estimate the density $p_t(x)$ while evolving them in tandem. The trajectories of the particles are governed by the drift $f(x_n)$ as well as the function $\\Psi(x_n) w$. Since $w$ itself depends on the positions of all particles, it constitutes an interaction term between them. The whole collection of particles determines the empirical estimate of $p_t(x)$, which in turn affects the trajectories of the particles.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Below is an imlpementation of this interacting particle simulator. We first define the basis functions $\\phi_k$ which we will use, which will be Exponentiated Quadratic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import jnp\n",
    "\n",
    "jax.jit(jax.vmap(lambda : jnp.array(np.random.randn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from scipy.integrate import ode as ODE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EQ(x, locs, scales):\n",
    "    \"\"\"\n",
    "    Exponentiated Quadratic basis functions.\n",
    "    \n",
    "    Arguments\n",
    "    x      : tf.Tensor, shape (N, D)\n",
    "    locs   : tf.Tensor, shape (K, D)\n",
    "    scales : tf.Tensor, shape (K, D)\n",
    "    \n",
    "    Returns\n",
    "    Phi    : tf.Tensor, shape (N, K)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape tensors for broadcasting\n",
    "    x = x[:, None, :]\n",
    "    locs = locs[None, :, :]\n",
    "    scales = scales[None, :, :]\n",
    "    \n",
    "    quad = -0.5 * ((x - locs) / scales) ** 2\n",
    "    quad = tf.reduce_sum(quad, axis=2)\n",
    "    \n",
    "    return tf.exp(quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the helper function `optimal_weights`, which given a set of particles $x$, a set of basis functions `phi` and a regularisation costant `lamda`, computes the optimal weights. For this we use automatic differentiation via `GradientTape.batch_jacobian` and decorate the function using `@tf.function` to avoid retracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def optimal_weights(x, phi, lamda):\n",
    "    \n",
    "    # Nested gradient tapes for computing first and second order derivatives\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "    \n",
    "        tape1.watch(x)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "\n",
    "            tape2.watch(x)\n",
    "\n",
    "            phi_ = phi(x)\n",
    "\n",
    "        Psi = tape2.batch_jacobian(phi_, x)\n",
    "        \n",
    "    G = tape1.batch_jacobian(Psi, x)\n",
    "    G = tf.linalg.trace(G)\n",
    "    G = tf.reduce_sum(G, axis=0)\n",
    "    \n",
    "    # Compute [Σ Ψ^Τ Ψ] + λ I\n",
    "    PsiPsi = tf.einsum('nij, nkj -> ik', Psi, Psi)\n",
    "    PsiPsi = PsiPsi + 1e-4 * tf.eye(PsiPsi.shape[0], dtype=PsiPsi.dtype)\n",
    "    \n",
    "    # Compute optimal weights by solving the system of linear equations\n",
    "    optimal_weights = - tf.linalg.solve(PsiPsi, G[:, None])[:, 0]\n",
    "    \n",
    "    return optimal_weights, Psi, PsiPsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the convenience function `augmented_drift` which computes the ODE drift $g(x, t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_drift(x, f, sigma, phi, lamda):\n",
    "    \n",
    "    w, Psi, PsiPsi = optimal_weights(x, phi, lamda)\n",
    "    Phi_w = tf.einsum('nij, i -> nj', Psi, w)\n",
    "    \n",
    "    drift = f(x) - 0.5 * sigma ** 2 * Phi_w\n",
    "    \n",
    "    return drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the interacting particle simulator to the double-well SDE, with drift\n",
    "\n",
    "$$\\begin{align}\n",
    "f(x) = - c x (x - 1) (x + 1),\n",
    "\\end{align}$$\n",
    "\n",
    "and corresponding potential\n",
    "\n",
    "$$\\begin{align}\n",
    "U(x) = \\frac{c}{4} \\left(x^4 - 2 x^2\\right).\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_well_drift(x, c):\n",
    "    return - c * x * (x - 1.) * (x + 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no closed-form for the density $p_t(x)$ for this drift function. We will therefore simulate the density for a sufficiently long time, and then compare the simulated distribution with the stationary distribution, which we can calculate in closed form {cite}`sarkka2019applied`\n",
    "\n",
    "$$\\begin{align}\n",
    "\\pi(x) = \\lim_{t \\to \\infty} p_t(x) \\propto \\exp \\left(- \\frac{2}{\\sigma^2} U(x) \\right).\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv: 0.003960847854614258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-84-fd0b1f560467>\", line 47, in <module>\n",
      "    solution = ode.integrate(ode.t + dt)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/scipy/integrate/_ode.py\", line 432, in integrate\n",
      "    self.f_params, self.jac_params)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/scipy/integrate/_ode.py\", line 1004, in run\n",
      "    y1, t, istate = self.runner(*args)\n",
      "  File \"<ipython-input-84-fd0b1f560467>\", line 31, in <lambda>\n",
      "    g = lambda t, x : tf.reshape(r(t, x), (-1,)).numpy()\n",
      "  File \"<ipython-input-84-fd0b1f560467>\", line 30, in <lambda>\n",
      "    r = lambda t, x : augmented_drift(tf.reshape(x, (N, D)), f, sigma, phi, lamda)\n",
      "  File \"<ipython-input-82-4b19d44bde27>\", line 3, in augmented_drift\n",
      "    w, Psi, PsiPsi = optimal_weights(x, phi, lamda)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 814, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\n",
      "    cancellation_manager=cancellation_manager)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 550, in call\n",
      "    ctx=ctx)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1464, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Set float64 precision\n",
    "dtype = tf.float64\n",
    "\n",
    "# Number of particles, features and problem dimension\n",
    "N = 100\n",
    "K = 100\n",
    "D = 1\n",
    "\n",
    "# Initial/final simulation times and step size\n",
    "t0 = 0.\n",
    "t1 = 1e-1\n",
    "dt = 1e-2\n",
    "\n",
    "# SDE parameters: double well constant and diffusion sigma\n",
    "c = 2.\n",
    "sigma = 1.\n",
    "\n",
    "# Set basis functions and regularisation constant\n",
    "locs = tf.cast(tf.linspace(-2., 2., K), dtype=dtype)[:, None]\n",
    "scales = tf.ones(shape=(K, D), dtype=dtype)\n",
    "\n",
    "phi = lambda x : EQ(x, locs=locs, scales=scales)\n",
    "lamda = 1e-4\n",
    "\n",
    "# Solver tolerance\n",
    "atol = 1e-4\n",
    "\n",
    "# Set up the ODE system\n",
    "f = lambda x : double_well_drift(x, c=c)\n",
    "r = lambda t, x : augmented_drift(tf.reshape(x, (N, D)), f, sigma, phi, lamda)\n",
    "g = lambda t, x : tf.reshape(r(t, x), (-1,)).numpy()\n",
    "\n",
    "# Initial locations of the particles\n",
    "x0 = np.random.normal(0., 1e0, size=(N, D))\n",
    "\n",
    "# Set up the ODE solver\n",
    "ode = ODE(g).set_integrator('vode', atol=atol)\n",
    "ode = ode.set_initial_value(np.reshape(x0, (-1,)), t0)\n",
    "\n",
    "# Array to keep track of the trajectries\n",
    "trajectories = [x0.reshape(-1)]\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# Solve ODE forward\n",
    "while ode.successful and ode.t < t1:\n",
    "    solution = ode.integrate(ode.t + dt)\n",
    "    trajectories.append(solution)\n",
    "\n",
    "trajectories = np.stack(trajectories, axis=0)\n",
    "trajectories = np.reshape(trajectories, (-1, N, D))\n",
    "\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "center-output"
    ]
   },
   "outputs": [],
   "source": [
    "t_plot = np.linspace(t0, t1, trajectories.shape[0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t_plot, trajectories[:, :, 0], color='k', alpha=0.1)\n",
    "\n",
    "# Plot formatting\n",
    "plt.title('Particle trajectories', fontsize=18)\n",
    "plt.xlabel('$t$', fontsize=16)\n",
    "plt.ylabel('$x$', fontsize=16)\n",
    "plt.xlim([t0, t1])\n",
    "plt.ylim([-3, 3])\n",
    "plt.yticks(np.linspace(-3, 3, 5))\n",
    "\n",
    "# Plot estimated pt(x) at t = t1\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Locations to plot pt(x) at\n",
    "x_plot = np.linspace(-2, 2, 100)[:, None]\n",
    "x_range = np.max(x_plot) - np.min(x_plot)\n",
    "\n",
    "# Get optimal weights as given by GLD\n",
    "w, Psi, _ = optimal_weights(trajectories[-1,  :, :], phi, lamda)\n",
    "\n",
    "# Compute density estimated via φT w\n",
    "est_density = np.exp(tf.matmul(phi(x_plot), w[:, None])[:, 0].numpy())\n",
    "est_density = est_density / np.sum(est_density) * (x_plot.shape[0] / x_range)\n",
    "\n",
    "# Compute true stationary density (t = infinity limit)\n",
    "true_density = np.exp(- 2 * c * (0.25 * x_plot ** 4 - 0.5 * x_plot ** 2))\n",
    "true_density = true_density / np.sum(true_density) * (x_plot.shape[0] / 4)\n",
    "\n",
    "# Plot particle histogram (t = t1)\n",
    "plt.hist(trajectories[-1, :, 0],\n",
    "         density=True,\n",
    "         bins=20,\n",
    "         color='b',\n",
    "         alpha=0.3,\n",
    "         edgecolor='black',\n",
    "         label='Particles $x_{t_1}$')\n",
    "\n",
    "# Plot density estimated by GLD (t = t1)\n",
    "plt.plot(x_plot,\n",
    "         est_density,\n",
    "         color='black',\n",
    "         label='Est. $e^{\\phi^T w_{t_1}}$')\n",
    "\n",
    "# Plot true stationary density (t = infinity limit)\n",
    "plt.plot(x_plot,\n",
    "         true_density,\n",
    "         color='red',\n",
    "         label='Exact $p_{\\infty}(x)$')\n",
    "\n",
    "# Format plot\n",
    "plt.ylim([0.0, 0.6])\n",
    "plt.xlabel('$x$', fontsize=16)\n",
    "plt.ylabel('$p(x)$', fontsize=16)\n",
    "plt.xticks(np.linspace(-2., 2., 5))\n",
    "plt.yticks(np.linspace(0., 0.6, 4))\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the particle trakectories in this case quickly converge to some equilibrium distribution. This equilibrium distribution matches the exact stationary distribution of the SDE $p_{\\infty}(x)$ - both the particle histogram (blue) and the GLD-estimated distribution (black) match the stationary distribution. Note how the particle trajectories themselves are smooth, since the only source of randomness is the initial state distribution. If we were to sample the SDE directly, these samples would involve randomness both due to the initial state distribution as well as the random Brownian motion associated with each path. This means that downstream estimators using these deterministic particles to approximate $p_t(x)$ will have smaller variance than paths sampled directly from the SDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import scipy\n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@jax.jit\n",
    "def fast_cg_solve(A, b, x0):\n",
    "    \n",
    "    iAb = jax.scipy.sparse.linalg.cg(A=A,\n",
    "                                     b=b,\n",
    "                                     x0=x0,\n",
    "                                     tol=1e-4,\n",
    "                                     atol=0.)[0]\n",
    "    return iAb\n",
    "\n",
    "\n",
    "class FasterFPKSolver:\n",
    "    \n",
    "    def __init__(self, w0):\n",
    "        \n",
    "        self.cached_weights = tf.reshape(w0, (-1,))\n",
    "        self.solve_times = []\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def PsiPsi_and_G(self, x, phi, lamda):\n",
    "\n",
    "        # Nested gradient tapes for computing first and second order derivatives\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "\n",
    "            tape1.watch(x)\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape2:\n",
    "\n",
    "                tape2.watch(x)\n",
    "\n",
    "                phi_ = phi(x)\n",
    "\n",
    "            Psi = tape2.batch_jacobian(phi_, x)\n",
    "\n",
    "        G = tape1.batch_jacobian(Psi, x)\n",
    "        G = tf.linalg.trace(G)\n",
    "        G = tf.reduce_sum(G, axis=0)\n",
    "\n",
    "        # Compute [Σ Ψ^Τ Ψ] + λ I\n",
    "        PsiPsi = tf.einsum('nij, nkj -> ik', Psi, Psi)\n",
    "        PsiPsi = PsiPsi + 1e-4 * tf.eye(PsiPsi.shape[0], dtype=PsiPsi.dtype)\n",
    "        \n",
    "        return Psi, PsiPsi, G\n",
    "    \n",
    "    \n",
    "    def optimal_weights(self, x, phi, lamda, use_cached_weights=True):\n",
    "\n",
    "        t = time.time()\n",
    "        Psi, PsiPsi, G = self.PsiPsi_and_G(x, phi, lamda)\n",
    "#         print('jac:', time.time() - t)\n",
    "        \n",
    "        PsiPsi = jnp.array(PsiPsi)\n",
    "        G = jnp.array(G)\n",
    "        \n",
    "        w = jnp.array(self.cached_weights)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        w = fast_cg_solve(A=PsiPsi, b=G, x0=w).block_until_ready()\n",
    "        \n",
    "        t2 = time.time()\n",
    "        w = np.linalg.solve(PsiPsi, G)\n",
    "        \n",
    "        t3 = time.time()\n",
    "        \n",
    "        print('inv:', t2 - t1, t3 - t2)\n",
    "        \n",
    "        self.cached_weights = w\n",
    "        \n",
    "        return -w, Psi, PsiPsi\n",
    "    \n",
    "    \n",
    "    def augmented_drift(self, x, f, sigma, phi, lamda):\n",
    "\n",
    "        w, Psi, PsiPsi = self.optimal_weights(x, phi, lamda)\n",
    "        Phi_w = tf.einsum('nij, i -> nj', Psi, w)\n",
    "\n",
    "        drift = f(x) - 0.5 * sigma ** 2 * Phi_w\n",
    "\n",
    "        return drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set float64 precision\n",
    "dtype = tf.float64\n",
    "\n",
    "# Number of particles, features and problem dimension\n",
    "N = 20\n",
    "K = 150\n",
    "D = 1\n",
    "\n",
    "# Initial/final simulation times and step size\n",
    "t0 = 0.\n",
    "t1 = 1e0\n",
    "dt = 1e-2\n",
    "\n",
    "# SDE parameters: double well constant and diffusion sigma\n",
    "c = 2.\n",
    "sigma = 1.\n",
    "\n",
    "# Set basis functions and regularisation constant\n",
    "locs = tf.cast(tf.linspace(-2., 2., K), dtype=dtype)[:, None]\n",
    "scales = tf.ones(shape=(K, D), dtype=dtype)\n",
    "\n",
    "phi = lambda x : EQ(x, locs=locs, scales=scales)\n",
    "lamda = 1e-2\n",
    "\n",
    "# Solver tolerance\n",
    "atol = 1e-4\n",
    "\n",
    "# Initial locations of the particles\n",
    "x0 = np.random.normal(0., 1e0, size=(N, D))\n",
    "w0 = np.zeros(shape=(K, 1))\n",
    "\n",
    "fpk_solver = FasterFPKSolver(w0=w0)\n",
    "\n",
    "# Set up the ODE system\n",
    "f = lambda x : double_well_drift(x, c=c)\n",
    "r = lambda t, x : fpk_solver.augmented_drift(tf.reshape(x, (N, D)), f, sigma, phi, lamda)\n",
    "g = lambda t, x : tf.reshape(r(t, x), (-1,)).numpy()\n",
    "\n",
    "# Set up the ODE solver\n",
    "ode = ODE(g).set_integrator('vode', atol=atol)\n",
    "ode = ode.set_initial_value(np.reshape(x0, (-1,)), t0)\n",
    "\n",
    "# Array to keep track of the trajectries\n",
    "trajectories = [x0.reshape(-1)]\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# Solve ODE forward\n",
    "while ode.successful and ode.t < t1:\n",
    "    solution = ode.integrate(ode.t + dt)\n",
    "    trajectories.append(solution)\n",
    "\n",
    "trajectories = np.stack(trajectories, axis=0)\n",
    "trajectories = np.reshape(trajectories, (-1, N, D))\n",
    "\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py:3135: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in linspace is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"linspace\")\n",
      "/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py:2983: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"asarray\")\n",
      "/Users/stratis/repos/random-walks/venv-random-walks/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py:3044: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"ones\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 145 ms, sys: 6.42 ms, total: 152 ms\n",
      "Wall time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "def jEQ(x, locs, scales):\n",
    "    \"\"\"\n",
    "    Exponentiated Quadratic basis functions.\n",
    "    \n",
    "    Arguments\n",
    "    x      : tf.Tensor, shape (D,)\n",
    "    locs   : tf.Tensor, shape (K, D)\n",
    "    scales : tf.Tensor, shape (K, D)\n",
    "    \n",
    "    Returns\n",
    "    Phi    : tf.Tensor, shape (N, K)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape tensors for broadcasting\n",
    "    x = x[None, :]\n",
    "    \n",
    "    quad = -0.5 * ((x - locs) / scales) ** 2\n",
    "    quad = jnp.sum(quad, axis=1)\n",
    "    \n",
    "    return jnp.exp(quad)\n",
    "\n",
    "N = 500\n",
    "K = 500\n",
    "D = 1\n",
    "\n",
    "x = jnp.array(np.random.normal(0., 1e0, size=(N, D)))\n",
    "locs = jnp.linspace(-2., 2., K, dtype=jnp.float64)[:, None]\n",
    "scales = jnp.ones(shape=(K, D), dtype=jnp.float64)\n",
    "\n",
    "hessian = jax.jit(jax.vmap(lambda x : jax.hessian(jEQ, argnums=0)(x, locs, scales)))\n",
    "\n",
    "%time _ = hessian(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_plot = np.linspace(t0, t1, trajectories.shape[0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t_plot, trajectories[:, :, 0], color='k', alpha=0.1)\n",
    "\n",
    "# Plot formatting\n",
    "plt.title('Particle trajectories', fontsize=18)\n",
    "plt.xlabel('$t$', fontsize=16)\n",
    "plt.ylabel('$x$', fontsize=16)\n",
    "plt.xlim([t0, t1])\n",
    "plt.ylim([-3, 3])\n",
    "plt.yticks(np.linspace(-3, 3, 5))\n",
    "\n",
    "# Plot estimated pt(x) at t = t1\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Locations to plot pt(x) at\n",
    "x_plot = np.linspace(-2, 2, 100)[:, None]\n",
    "x_range = np.max(x_plot) - np.min(x_plot)\n",
    "\n",
    "# Get optimal weights as given by GLD\n",
    "w, Psi, _ = optimal_weights(trajectories[-1,  :, :], phi, lamda)\n",
    "\n",
    "# Compute density estimated via φT w\n",
    "est_density = np.exp(tf.matmul(phi(x_plot), w[:, None])[:, 0].numpy())\n",
    "est_density = est_density / np.sum(est_density) * (x_plot.shape[0] / x_range)\n",
    "\n",
    "# Compute true stationary density (t = infinity limit)\n",
    "true_density = np.exp(- 2 * c * (0.25 * x_plot ** 4 - 0.5 * x_plot ** 2))\n",
    "true_density = true_density / np.sum(true_density) * (x_plot.shape[0] / 4)\n",
    "\n",
    "# Plot particle histogram (t = t1)\n",
    "plt.hist(trajectories[-1, :, 0],\n",
    "         density=True,\n",
    "         bins=20,\n",
    "         color='b',\n",
    "         alpha=0.3,\n",
    "         edgecolor='black',\n",
    "         label='Particles $x_{t_1}$')\n",
    "\n",
    "# Plot density estimated by GLD (t = t1)\n",
    "plt.plot(x_plot,\n",
    "         est_density,\n",
    "         color='black',\n",
    "         label='Est. $e^{\\phi^T w_{t_1}}$')\n",
    "\n",
    "# Plot true stationary density (t = infinity limit)\n",
    "plt.plot(x_plot,\n",
    "         true_density,\n",
    "         color='red',\n",
    "         label='Exact $p_{\\infty}(x)$')\n",
    "\n",
    "# Format plot\n",
    "plt.ylim([0.0, 0.6])\n",
    "plt.xlabel('$x$', fontsize=16)\n",
    "plt.ylabel('$p(x)$', fontsize=16)\n",
    "plt.xticks(np.linspace(-2., 2., 5))\n",
    "plt.yticks(np.linspace(0., 0.6, 4))\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have seen how the FPK can be recast as a Liouville equation, which can in turn be simulated using the GLD estimator. This method evolves a number of particles with randomly sampled initial conditions and deterministic dynamics. The Liouville equation involves an term involving the particle density, which can be interpreted as an interaction term between the particles. This simulation method can be used to obtain lower-variance empirical estimates of the particle density compared to directly sampling paths from the FPK. Recently, this method has been applied {cite}`song2020score` to training large-scale generative models to high-dimensional SDEs, using Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "```{bibliography} ./references.bib\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-random-walks",
   "language": "python",
   "name": "venv-random-walks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
