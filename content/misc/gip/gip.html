
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Global inducing points for BNNs &#8212; Random walks</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://random-walks.org/content/misc/gip/gip.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Interesting reading and websites" href="../../reading-and-links.html" />
    <link rel="prev" title="Gumbel distribution" href="../gumbel/gumbel.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://random-walks.org/content/misc/gip/gip.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Global inducing points for BNNs" />
<meta property="og:description" content="Global inducing points for BNNs  One central challenge in working with Bayesian Neural Networks (BNNs) is handling their posterior distributions. Since the post" />
<meta property="og:image"       content="https://random-walks.org/_static/logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Random walks</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../home.html">
   Random walks
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../prob-intro/intro.html">
   Probability: An introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch01/content.html">
     Events and Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch02/content.html">
     Discrete random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch03/content.html">
     Multivariate discrete distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch04/content.html">
     Probability generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch05/content.html">
     Distribution and density functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch06/content.html">
     Multivariate distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch07/content.html">
     Moment generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch08/content.html">
     Main limit theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch09/content.html">
     Branching processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch10/content.html">
     Random walks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch11/content.html">
     Processes in continuous time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch12/content.html">
     Markov chains
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../cvx/intro.html">
   Convex optimisation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch01.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch02.html">
     Convex sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch03.html">
     Convex functions
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../gp/gp-intro.html">
   Gaussian Processes
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../gp/why-covariances.html">
     Why covariance functions?
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../gp/sparse/sparse-intro.html">
     Sparse Gaussian Processes
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/vfe.html">
       Variational Free Energy GPs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/gp-sampling.html">
       Sampling GP posteriors
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../misc.html">
   Miscellaneous
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../sde/num-sde.html">
     Numerical simulation of SDEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sde-as-gp/sde-as-gp.html">
     VI for diffusion processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../node/node.html">
     Neural ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../optimisation/conjugate-gradients.html">
     Conjugate gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../kalman/kalman.html">
     The Kalman filter and smoother
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ncs/ncs.html">
     Natural cubic splines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ars/ars.html">
     Adaptive rejection sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../score-matching/score-matching.html">
     Estimation by score matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interacting/interacting.html">
     Interacting particle FPK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rff/rff.html">
     Random Fourier features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../svgd/svgd.html">
     Stein variational gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../addgp/addgp.html">
     Additive Gaussian Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gumbel/gumbel.html">
     Gumbel distribution
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Global inducing points for BNNs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reading-and-links.html">
   Interesting reading and websites
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/misc/gip/gip.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/stratisMarkou/random-walks/master?urlpath=tree/./content/misc/gip/gip.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#posterior-over-the-last-layer">
   Posterior over the last layer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#approximate-posterior">
   Approximate posterior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-tight-is-the-gip-elbo">
   How tight is the GIP ELBO?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-experiments">
   More experiments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#effect-of-prior-scale">
     Effect of prior scale
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#effect-of-inducing-points">
     Effect of inducing points
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="global-inducing-points-for-bnns">
<h1>Global inducing points for BNNs<a class="headerlink" href="#global-inducing-points-for-bnns" title="Permalink to this headline">¶</a></h1>
<p>One central challenge in working with Bayesian Neural Networks (BNNs) is handling their posterior distributions. Since the posterior over the parameters of a BNN is analytically intractable, working with it typically requires sampling or making approximations. In Variational Inference (VI), we approximate the exact posterior using another tractable distibution, which can be used for making predictions. Perhaps unsurpsrisingly, VI is sensitive to the choice of approximate posterior, and naive choices can lead to undesirable modelling behaviours.<a class="bibtex reference internal" href="#turner2011problems" id="id1">[TS11]</a> For example, a usual choice is to approximate the posteerior using a factored Gaussian,<a class="bibtex reference internal" href="#blundell2015weight" id="id2">[BCKW15]</a> which typically leads to underfitting and poor estimates of the predictive uncertainty. <a class="bibtex reference internal" href="#foong2019between" id="id3">[FLHLT19]</a> Recently, Ober and Aitchison <a class="bibtex reference internal" href="#ober2021global" id="id4">[OA21a]</a> introduced an approximate posterior which goes beyond the factored approximation and yields improved results. Their approximation is based on inducing points, an idea which is common in the Gaussian Process (GP) literature. In particular, this approximate posterior can be used for Deep GPs (DGPs) as well as BNNs, highlighting the similarities between these models.</p>
<div class="section" id="posterior-over-the-last-layer">
<h2>Posterior over the last layer<a class="headerlink" href="#posterior-over-the-last-layer" title="Permalink to this headline">¶</a></h2>
<p>We first define the model and prior assumptions. Consider a regression task, where we want to learn a mapping from inputs <span class="math notranslate nohighlight">\(\mathbf{X} \subseteq \mathbb{R}^{K \times D_x}\)</span> to the corresponding outputs <span class="math notranslate nohighlight">\(\mathbf{Y} \subseteq \mathbb{R}^{K \times D_y}\)</span>. Consider a fully connected neural network made up of <span class="math notranslate nohighlight">\(L\)</span> hidden layers, each containing <span class="math notranslate nohighlight">\(N_1, \dots, N_L\)</span> hidden units, followed by by a last linear layer which maps from the <span class="math notranslate nohighlight">\(N_L\)</span> units to the dimension of the data <span class="math notranslate nohighlight">\(D_y\)</span>. Let the weights in each layer be</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbf{W} = \{\mathbf{W}_l \in \mathbb{R}^{N_l \times N_{l+1}} \}_{l = 1}^{L+1},
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(N_0 = D_x\)</span> and <span class="math notranslate nohighlight">\(N_{L+1} = D_y\)</span>. Thus the network takes the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbf{F}_1 &amp;= \mathbf{X} \mathbf{W}_1, \\
\mathbf{F}_{l+1} &amp;= \phi(\mathbf{F}_{l}) \mathbf{W}_l, \text{ for } l = 1, \dots, L,
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi\)</span> is a nonlinearity. Note that under this notation, the weights post-multiply the activations rather than pre-multiplying them. Now suppose we place a diagonal Gaussian prior</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(\mathbf{W}) = \prod_{l = 1}^{L+1} \prod_{ij} \mathcal{N}(w_{l, i, j}; 0, \sigma_l^2),
\end{align}\]</div>
<p>over the weights, where <span class="math notranslate nohighlight">\(w_{l, i, j}\)</span> is the weight corresponding to the <span class="math notranslate nohighlight">\((i, j)\)</span> entry of the matrix <span class="math notranslate nohighlight">\(\mathbf{W}_l\)</span>, together with the Gaussian likelihood function</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(\mathbf{Y} | \mathbf{W}, \mathbf{X}) = \prod_{k = 1}^K \mathcal{N}(\mathbf{y}_{k, \cdot}; \mathbf{f}_{L, k, \cdot} \mathbf{W}_{L+1}, \sigma_n^2 I),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{y}_{k, \cdot}\)</span> is the <span class="math notranslate nohighlight">\(k^{th}\)</span> row of the <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> matrix and <span class="math notranslate nohighlight">\(\mathbf{f}_{L, k, \cdot}\)</span> is the <span class="math notranslate nohighlight">\(k^{th}\)</span> row of the <span class="math notranslate nohighlight">\(\mathbf{F}_{L+1}\)</span> matrix. The posterior over the weights of this network is not analytic and must therefore be approximated. However, conditioned on the weights of all previous layers, the posterior over the weights of the last layer is analytic. In particular, we have</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(\mathbf{w}_{L+1, \cdot, d} | \mathbf{W}_{1:L}, \mathbf{X}) \propto \underbrace{p(\mathbf{w}_{L+1, \cdot, d})}_{\text{Prior term}}~\underbrace{\mathcal{N}(y_{\cdot, d}; \mathbf{f}_{L, d, \cdot}^\top \mathbf{w}_{L+1, \cdot, d}, \sigma_n^2 I)}_{\text{Likelihood term}},
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w}_{L+1, \cdot, d}\)</span> is the <span class="math notranslate nohighlight">\(d^{th}\)</span> column of <span class="math notranslate nohighlight">\(\mathbf{W}_{L+1}\)</span>. Note that the features of the last layer depend on the weights of all previous layers, which we are conditioning on. From here we can show that the conditional posterior over <span class="math notranslate nohighlight">\(\mathbf{W}_{L+1, \cdot, d}\)</span> is also Gaussian, and takes the form</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(\mathbf{w}_{L+1, \cdot, d} | \mathbf{W}_{1:L}, \mathbf{X}) = \mathcal{N}\left(\mathbf{w}_{L+1, \cdot, d}; \mathbf{f}_{L, k, \cdot}^\top \mathbf{w}_{L+1, \cdot, d}, (\sigma_n^{-2} I)\right),
\end{align}\]</div>
<p>Ober and Aitchison <a class="bibtex reference internal" href="#ober2021global" id="id5">[OA21a]</a> draw inspiration from this to propose an approximate posterior, in which the weights of a layer given all previous layers are conditionally Gaussian, but the full posterior is not.</p>
</div>
<div class="section" id="approximate-posterior">
<h2>Approximate posterior<a class="headerlink" href="#approximate-posterior" title="Permalink to this headline">¶</a></h2>
<p>While the posterior over the weights of the last layer, conditioned on the weights of all previous layers. This is because the posterior over weights of a linear-in-the parameters model with a Gaussian prior, is also Gaussian. Inspired by this observation, Ober and Aitchison propose using an approximate posterior, where, conditioned on the weights of all previous layers, the weights of a given layer are Gaussian distributed, as in</p>
<div class="math notranslate nohighlight">
\[\begin{align}
q(\mathbf{W}) = q(\mathbf{W}_{L+1} | \mathbf{W}_L, \dots \mathbf{W}_1) \dots q(\mathbf{W}_2 | \mathbf{W}_1) q(\mathbf{W}_1).
\end{align}\]</div>
<p>However, the intermediate layers do not have observations associated with them, and so we have the question of how to parameterise the posterior.
Ober and Aitchison introduce a set of variational parameters, <span class="math notranslate nohighlight">\(\mathbf{v}_l \in \mathbb{R}^{M \times N_l}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_l = \text{diag}(\boldsymbol{\lambda}_l) \in \mathbb{R}^{N_{l+1} \times N_{l+1}}\)</span> for each layer, which play the role of pseudo-observations and pseudo-noise precision for the intermediate layers.
They also introduce a set of corresponding pseudo-inputs <span class="math notranslate nohighlight">\(\mathbb{U}_0 \in \mathbb{R}^{M \times N_1}\)</span>, which they propagate through the network according to</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbf{U}_1 &amp;= \mathbf{U}_0 \mathbf{W}_1, \\
\mathbf{U}_{l+1} &amp;= \phi(\mathbf{U}_{l}) \mathbf{W}_l, \text{ for } l = 1, \dots, L,
\end{align}\end{split}\]</div>
<p>to obtain inducing inputs at every layer of the network.
This is in contrast to approaches such as Doubly Stochastic VI <a class="bibtex reference internal" href="#salimbeni2017doubly" id="id6">[SD17]</a> which use different <span class="math notranslate nohighlight">\(\mathbb{U}_l\)</span> for each layer.
They then pick an approximate posterior that is analogous to the product of a pseudo-likelihood term and the prior term</p>
<div class="math notranslate nohighlight">
\[\begin{align}
q\left(\mathbf{W}_l | \{\mathbf{W}_{l'}\}_{l' = 1}^{l-1}\right) &amp;\propto \underbrace{p(\mathbf{w}_{l, d})}_{\text{Prior term}} \prod_{d = 1}^{D_l} \underbrace{\mathcal{N}\left(\mathbf{v}_{l, \cdot, d}; \phi(\mathbf{U}_{l-1}) \mathbf{w}_{l, \cdot, d}, \boldsymbol{\Lambda}^{-1}_l\right)}_{\text{Pseudo-likelihood terms}}.
\end{align}\]</div>
<p>Note the similarity of this term to the exact posterior. Note also that under the conditional <span class="math notranslate nohighlight">\(q\)</span> is a Gaussian which factorises over weight columns. However, the full posterior, that is the product of the <span class="math notranslate nohighlight">\(q\)</span> terms, is not factorised, neither is it Gaussian. Therefore, this approximation may be able to capture a wide family of distributions, and can be more expressive than a mean-field posterior. Now if we rearrange this expression, to get the normalised distribution over <span class="math notranslate nohighlight">\(\mathbf{w}_d^l\)</span>, we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
q\left(\mathbf{W}_l | \{\mathbf{W}_{l'}\}_{l' = 1}^{l-1}\right) &amp;= \prod_{d = 1}^{D_l} \mathcal{N}\left(\mathbf{w}_{l, \cdot, d}; \boldsymbol{\mu}_{l, d}, \boldsymbol{\Sigma}_l\right), \\
                                          \boldsymbol{\mu}_{l, d}  &amp;= \boldsymbol{\Sigma}_l \phi\left(\mathbf{U}_{l-1}\right)^\top \boldsymbol{\Lambda}_l \mathbf{v}_{l, \cdot, d}, \\
                                       \boldsymbol{\Sigma}_l  &amp;= \left( D_l \mathbf{I} + \phi\left(\mathbf{U}_{l-1}\right)^\top \boldsymbol{\Lambda}_l \phi\left(\mathbf{U}_{l-1}\right) \right)^{-1}.
\end{align}\end{split}\]</div>
<p>We can train this model by maximising the Evidence Lower Bound (ELBO).
We can estimate the ELBO in the following recursive manner.
First, we evaluate <span class="math notranslate nohighlight">\(q\)</span> at the <span class="math notranslate nohighlight">\(l^{th}\)</span> layer given all previous inducing inputs and weights, we sample <span class="math notranslate nohighlight">\(\mathbf{W}_l\)</span> and use these weights to propagate the network activations and inducing inputs to obtain <span class="math notranslate nohighlight">\(\mathbf{F}_l\)</span> and <span class="math notranslate nohighlight">\(\mathbf{U}_l\)</span>.
As we propagate this information through the network, we also need to keep track of the empirical KL divergence between the approximate posterior <span class="math notranslate nohighlight">\(q\)</span> and the prior <span class="math notranslate nohighlight">\(p\)</span> over the weights of the layer.
At the last layer, we can use sampled weights to evaluate the likelihood of the data, conditioned on the sampled weights.
Summing the KL contribution from all layers together with the likelihood of the data, we obtain an unbiased estimate of the ELBO.</p>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>We will define a <code class="docutils literal notranslate"><span class="pre">GlobalInducingDenseLayer</span></code>, which handdles propagating the data activations <span class="math notranslate nohighlight">\(\mathbf{F}_l\)</span>, the inducing activations <span class="math notranslate nohighlight">\(\mathbf{U}_l\)</span> and computes the contribution of the layer to the total KL divergence.</p>
<div class="cell tag_hide-input tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GlobalInducingDenseLayer</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_input</span><span class="p">,</span>
                 <span class="n">num_output</span><span class="p">,</span>
                 <span class="n">num_inducing</span><span class="p">,</span>
                 <span class="n">nonlinearity</span><span class="p">,</span>
                 <span class="n">prior_scale_factor</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">&quot;global_inducing_fully_connected_layer&quot;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">num_input</span> <span class="o">=</span> <span class="n">num_input</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_output</span> <span class="o">=</span> <span class="n">num_output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_inducing</span> <span class="o">=</span> <span class="n">num_inducing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_scale_factor</span> <span class="o">=</span> <span class="n">prior_scale_factor</span>
        
        <span class="c1"># Set nonlinearity for the layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">nonlinearity</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> \
                            <span class="nb">getattr</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        
        <span class="c1"># Set up prior mean, scale and distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_input</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_input</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_scale</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_input</span><span class="o">**</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_scale_factor</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_mean</span><span class="p">,</span>
            <span class="n">scale_diag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_scale</span>
        <span class="p">)</span>
        
        <span class="c1"># Set up pseudo observation means and variances</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pseudo_means</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_inducing</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_output</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pseudo_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pseudo_means</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">pseudo_log_prec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_inducing</span><span class="p">,),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pseudo_log_prec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pseudo_log_prec</span><span class="p">)</span>
        
        
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pseudo_precision</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pseudo_log_precision</span><span class="p">)</span>
    
        
    <span class="k">def</span> <span class="nf">q_prec_cov_chols</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Uin</span><span class="p">):</span>
        
        <span class="n">phiU</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">Uin</span><span class="p">)</span>
        <span class="n">pseudo_prec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pseudo_log_prec</span><span class="p">)</span>
        
        <span class="c1"># Compute precision matrix of multivariate normal</span>
        <span class="n">phiT_lambda_phi</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;mi, m, mj -&gt; ij&quot;</span><span class="p">,</span> <span class="n">phiU</span><span class="p">,</span> <span class="n">pseudo_prec</span><span class="p">,</span> <span class="n">phiU</span><span class="p">)</span>
        
        <span class="n">q_prec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_scale</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">**-</span><span class="mf">2.</span><span class="p">)</span> <span class="o">+</span> <span class="n">phiT_lambda_phi</span>
        
        <span class="c1"># Compute cholesky of approximate posterior precision</span>
        <span class="n">q_prec_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">q_prec</span><span class="p">)</span>
        
        <span class="c1"># Compute cholesky of approximate posterior covariance</span>
        <span class="n">iq_prec_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">triangular_solve</span><span class="p">(</span>
            <span class="n">q_prec_chol</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">q_prec_chol</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="n">lower</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        
        <span class="n">q_cov</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">iq_prec_chol</span><span class="p">,</span> <span class="n">iq_prec_chol</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">q_cov</span> <span class="o">=</span> <span class="n">q_cov</span> <span class="o">+</span> <span class="mf">1e-5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">q_cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">q_cov_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">q_cov</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">q_prec_chol</span><span class="p">,</span> <span class="n">q_cov_chol</span>
    
    
    <span class="k">def</span> <span class="nf">q_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Uin</span><span class="p">,</span> <span class="n">prec_chol</span><span class="p">):</span>
        
        <span class="n">phiU</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">Uin</span><span class="p">)</span>
        <span class="n">pseudo_prec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pseudo_log_prec</span><span class="p">)</span>
        
        <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
            <span class="n">phiU</span><span class="p">,</span>
            <span class="n">pseudo_prec</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pseudo_mean</span><span class="p">,</span>
            <span class="n">transpose_a</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        
        <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="n">prec_chol</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">mean</span>
        
        
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Fin</span><span class="p">,</span> <span class="n">Uin</span><span class="p">):</span>
        
        <span class="c1"># Augment input features with ones to absorb bias</span>
        <span class="n">Fones</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">Fin</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">Fin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">Fin</span><span class="p">,</span> <span class="n">Fones</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">Uones</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">Uin</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">Uin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">Uin</span><span class="p">,</span> <span class="n">Uones</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">Din</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_input</span>
        <span class="n">Dout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_output</span>
        <span class="n">M</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_inducing</span>
        
        <span class="c1"># Check shape of input features Fin and pseudo-means</span>
        <span class="n">check_shape</span><span class="p">(</span>
            <span class="p">[</span><span class="n">Fin</span><span class="p">,</span> <span class="n">Uin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pseudo_means</span><span class="p">],</span>
            <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">Din</span><span class="p">),</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">Din</span><span class="p">),</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">Dout</span><span class="p">)]</span>
        <span class="p">)</span>
        
        <span class="c1"># Compute cholesky factors of q precision and covariance.</span>
        <span class="c1"># These are common between all weight columns, i.e. the covariance</span>
        <span class="c1"># between weights leading to a neuron in the next layer is shared</span>
        <span class="c1"># between all next neurons.</span>
        <span class="n">q_prec_chol</span><span class="p">,</span> <span class="n">q_cov_chol</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_prec_cov_chols</span><span class="p">(</span><span class="n">Uin</span><span class="p">)</span>
        
        <span class="n">check_shape</span><span class="p">(</span>
            <span class="p">[</span><span class="n">q_prec_chol</span><span class="p">,</span> <span class="n">q_cov_chol</span><span class="p">],</span>
            <span class="p">[(</span><span class="n">Din</span><span class="p">,</span> <span class="n">Din</span><span class="p">),</span> <span class="p">(</span><span class="n">Din</span><span class="p">,</span> <span class="n">Din</span><span class="p">)]</span>
        <span class="p">)</span>
        
        <span class="c1"># Compute means of q. There is a different mean vector for</span>
        <span class="c1"># each column of weights.</span>
        <span class="n">q_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_mean</span><span class="p">(</span><span class="n">Uin</span><span class="p">,</span> <span class="n">q_prec_chol</span><span class="p">)</span>
        
        <span class="n">check_shape</span><span class="p">(</span><span class="n">q_mean</span><span class="p">,</span> <span class="p">(</span><span class="n">Dout</span><span class="p">,</span> <span class="n">Din</span><span class="p">))</span>
        
        <span class="c1"># Sample approximate posterior for the weights</span>
        <span class="n">q_cov_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">q_cov_chol</span><span class="p">]</span><span class="o">*</span><span class="n">Dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalTriL</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">q_mean</span><span class="p">,</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">q_cov_chol</span><span class="p">)</span>
        <span class="n">wT</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">wT</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        
        <span class="n">check_shape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">(</span><span class="n">Din</span><span class="p">,</span> <span class="n">Dout</span><span class="p">))</span>
        
        <span class="c1"># Compute contibution to ELBO</span>
        <span class="n">kl_term</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">)</span>
        <span class="n">kl_term</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">kl_term</span><span class="p">)</span>
        
        <span class="c1"># Compute log-probability of weights under prior</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">wT</span><span class="p">)</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">log_p</span><span class="p">)</span>
        
        <span class="c1"># Compute log-probability of weights under approximate posterior</span>
        <span class="n">log_q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">wT</span><span class="p">)</span>
        <span class="n">log_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">log_q</span><span class="p">)</span>
        
        <span class="c1"># Compute Fout and Uout and return</span>
        <span class="n">Fout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">Fin</span><span class="p">),</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">Uout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">Uin</span><span class="p">),</span> <span class="n">w</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">Fout</span><span class="p">,</span> <span class="n">Uout</span><span class="p">,</span> <span class="n">kl_term</span><span class="p">,</span> <span class="n">log_p</span><span class="p">,</span> <span class="n">log_q</span>
</pre></div>
</div>
</div>
</div>
<p>We can then stack a few <code class="docutils literal notranslate"><span class="pre">GlobalInducingDenseLayers</span></code> to form a <code class="docutils literal notranslate"><span class="pre">GlobalInducingFullyConnectedNetwork</span></code>. We use an architecture using two hidden layers, each using <span class="math notranslate nohighlight">\(50\)</span> units, as done by Ober and Aitchinson.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GlobalInducingFullyConnectedNetwork</span><span class="p">(</span><span class="n">tfk</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_input</span><span class="p">,</span>
                 <span class="n">num_output</span><span class="p">,</span>
                 <span class="n">inducing_points</span><span class="p">,</span>
                 <span class="n">nonlinearity</span><span class="p">,</span>
                 <span class="n">prior_scale_factor</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">&quot;global_inducing_fully_connected&quot;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">num_input</span> <span class="o">=</span> <span class="n">num_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_output</span> <span class="o">=</span> <span class="n">num_output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inducing_points</span> <span class="o">=</span> <span class="n">inducing_points</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_inducing</span> <span class="o">=</span> <span class="n">inducing_points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior_scale_factor</span> <span class="o">=</span> <span class="n">prior_scale_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
        
        
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">inducing_points</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inducing_points</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">GlobalInducingDenseLayer</span><span class="p">(</span>
            <span class="n">num_input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_input</span><span class="p">,</span>
            <span class="n">num_output</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">num_inducing</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_inducing</span><span class="p">,</span>
            <span class="n">nonlinearity</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">prior_scale_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_scale_factor</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">GlobalInducingDenseLayer</span><span class="p">(</span>
            <span class="n">num_input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">num_output</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_hidden</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">num_inducing</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_inducing</span><span class="p">,</span>
            <span class="n">nonlinearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">,</span>
            <span class="n">prior_scale_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_scale_factor</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">l3</span> <span class="o">=</span> <span class="n">GlobalInducingDenseLayer</span><span class="p">(</span>
            <span class="n">num_input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_hidden</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">num_output</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_output</span><span class="p">,</span>
            <span class="n">num_inducing</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_inducing</span><span class="p">,</span>
            <span class="n">nonlinearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">,</span>
            <span class="n">prior_scale_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior_scale_factor</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>
        
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">noise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span><span class="p">)</span>
        
    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="n">F1</span><span class="p">,</span> <span class="n">U1</span><span class="p">,</span> <span class="n">kl1</span><span class="p">,</span> <span class="n">log_p1</span><span class="p">,</span> <span class="n">log_q1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inducing_points</span><span class="p">)</span>
        <span class="n">F2</span><span class="p">,</span> <span class="n">U2</span><span class="p">,</span> <span class="n">kl2</span><span class="p">,</span> <span class="n">log_p2</span><span class="p">,</span> <span class="n">log_q2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">F1</span><span class="p">,</span> <span class="n">U1</span><span class="p">)</span>
        <span class="n">F3</span><span class="p">,</span> <span class="n">U3</span><span class="p">,</span> <span class="n">kl3</span><span class="p">,</span> <span class="n">log_p3</span><span class="p">,</span> <span class="n">log_q3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l3</span><span class="p">(</span><span class="n">F2</span><span class="p">,</span> <span class="n">U2</span><span class="p">)</span>
        
        <span class="n">means</span> <span class="o">=</span> <span class="n">F3</span>
        <span class="n">scales</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">F3</span><span class="p">)</span>
        
        <span class="n">kl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">([</span><span class="n">kl1</span><span class="p">,</span> <span class="n">kl2</span><span class="p">,</span> <span class="n">kl3</span><span class="p">])</span>
        
        <span class="n">log_p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">([</span><span class="n">log_p1</span><span class="p">,</span> <span class="n">log_p2</span><span class="p">,</span> <span class="n">log_p3</span><span class="p">])</span>
        <span class="n">log_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">([</span><span class="n">log_q1</span><span class="p">,</span> <span class="n">log_q2</span><span class="p">,</span> <span class="n">log_q3</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">means</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">log_p</span><span class="p">,</span> <span class="n">log_q</span>
    
    <span class="k">def</span> <span class="nf">elbo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

        <span class="n">means</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">cond_lik</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">means</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scales</span><span class="p">)</span>
        <span class="n">cond_lik</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">cond_lik</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

        <span class="n">elbo</span> <span class="o">=</span> <span class="n">cond_lik</span> <span class="o">-</span> <span class="n">kl</span>

        <span class="k">return</span> <span class="n">elbo</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span>

    <span class="k">def</span> <span class="nf">iwbo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>

        <span class="n">iwbo</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>

            <span class="n">means</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">log_p</span><span class="p">,</span> <span class="n">log_q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">cond_lik</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">means</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scales</span><span class="p">)</span>
            <span class="n">cond_lik</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">cond_lik</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

            <span class="n">iwbo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cond_lik</span> <span class="o">+</span> <span class="n">log_p</span> <span class="o">-</span> <span class="n">log_q</span><span class="p">)</span>

        <span class="n">iwbo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">iwbo</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">iwbo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_logsumexp</span><span class="p">(</span><span class="n">iwbo</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">iwbo</span>
</pre></div>
</div>
</div>
</div>
<p>We now want a dataset to fit tthis model to. We’ll use the same one-dimensional toy example that Ober and Aitchinson used, as shown below. This is an interesting example, because of the gap in the middle of the dataset. Recently Foong et al.<a class="bibtex reference internal" href="#foong2019between" id="id7">[FLHLT19]</a> have proved that applying mean-field VI to a, somewhat limited, class of neural networks, fails to capture predictive uncertainty in between different clusters of datapoints. The GIP approximate posterior is not a mean-field posterior, and may represent in-between uncertainty.</p>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_data</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_input</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">std_noise</span> <span class="o">=</span> <span class="mf">3.</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">minval</span><span class="o">=-</span><span class="mf">4.</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=-</span><span class="mf">2.</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_data</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">minval</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_data</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">3.</span> <span class="o">+</span> <span class="n">std_noise</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Figure to plot on </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Plot data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;black&quot;</span>
<span class="p">)</span>

<span class="c1"># Format plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/gip_7_0.svg" src="../../../_images/gip_7_0.svg" /></div>
</div>
<p>Now we can train the model. We’ll use a standard training procedure with Adam, and no special initialisation or scheduling tricks. We’ll also train the model for quite a while to ensure it has converged.</p>
<div class="cell tag_hide-input tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We decorate a single gradient descent step with tf.function. On the first</span>
<span class="c1"># call of single_step, tensorflow will compile the computational graph first.</span>
<span class="c1"># After that, all calls to single_step will use the compiled graph which is</span>
<span class="c1"># much faster than the default eager mode execution. In this case, the gain</span>
<span class="c1"># is roughly a x20 speedup (with a CPU), which can be checked by commenting</span>
<span class="c1"># out the decorator and rerunning the training script.</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">single_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimiser</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

        <span class="n">elbo</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">elbo</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimiser</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">elbo</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span>

<span class="c1"># Set model constants</span>
<span class="n">num_input</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_output</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_inducing</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
<span class="n">nonlinearity</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span>
<span class="n">prior_scale_factor</span> <span class="o">=</span> <span class="mf">2.</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)</span>

<span class="c1"># Initialise inducing points at subset of training points</span>
<span class="n">inducing_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[:</span><span class="n">num_inducing</span><span class="p">]</span>
<span class="n">inducing_points</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">inducing_idx</span><span class="p">)</span>

<span class="c1"># Initialise model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GlobalInducingFullyConnectedNetwork</span><span class="p">(</span>
    <span class="n">num_input</span><span class="o">=</span><span class="n">num_input</span><span class="p">,</span>
    <span class="n">num_output</span><span class="o">=</span><span class="n">num_output</span><span class="p">,</span>
    <span class="n">inducing_points</span><span class="o">=</span><span class="n">inducing_points</span><span class="p">,</span>
    <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
    <span class="n">prior_scale_factor</span><span class="o">=</span><span class="n">prior_scale_factor</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
<span class="p">)</span>

<span class="c1"># Initialise optimiser</span>
<span class="n">optimiser</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
    
<span class="c1"># Set progress bar and suppress warnings</span>
<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">get_logger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="s1">&#39;ERROR&#39;</span><span class="p">)</span>

<span class="c1"># Set tensors for keeping track of quantities of interest</span>
<span class="n">train_elbo</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_cond_lik</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_kl</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Train model</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
        
    <span class="n">elbo</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">single_step</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">optimiser</span><span class="o">=</span><span class="n">optimiser</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="n">y</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        
        <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;ELBO </span><span class="si">{</span><span class="n">elbo</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Cond-lik. </span><span class="si">{</span><span class="n">cond_lik</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;KL </span><span class="si">{</span><span class="n">kl</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        
    <span class="n">train_elbo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elbo</span><span class="p">)</span>
    <span class="n">train_cond_lik</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cond_lik</span><span class="p">)</span>
    <span class="n">train_kl</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kl</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can verify that training has converged by looking at the ELBO. Training further will likely not improve the model parameters or inducing points.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Helper for computing moving average</span>
<span class="k">def</span> <span class="nf">moving_average</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    
    <span class="n">cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
    <span class="n">cumsum</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span> <span class="o">=</span> <span class="n">cumsum</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span> <span class="o">-</span> <span class="n">cumsum</span><span class="p">[:</span><span class="o">-</span><span class="n">n</span><span class="p">]</span>
    
    <span class="n">moving_average</span> <span class="o">=</span> <span class="n">cumsum</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="n">n</span>
    
    <span class="k">return</span> <span class="n">moving_average</span>

<span class="c1"># Plot ELBO during optimisation</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">moving_average</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">train_elbo</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>

<span class="c1"># Format plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# training steps&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;ELBO&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="o">*</span><span class="n">num_steps</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">15.</span><span class="p">,</span> <span class="mf">85.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/gip_11_0.svg" src="../../../_images/gip_11_0.svg" /></div>
</div>
<p>Checking the plot below, we see that the model has fit the data well and produced reasonable uncertainty estimates. This is quite pleasing since we haven’t had to use any tricks in the training procedure.</p>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/gip_13_0.svg" src="../../../_images/gip_13_0.svg" /></div>
</div>
</div>
<div class="section" id="how-tight-is-the-gip-elbo">
<h2>How tight is the GIP ELBO?<a class="headerlink" href="#how-tight-is-the-gip-elbo" title="Permalink to this headline">¶</a></h2>
<p>We can also check how tight the ELBO of the GIP approximate posterior is, using importance sampling. We can estimate the same importance weighed lower bound of the marginal likelihood that importance weighed autoencoders {cite}<code class="docutils literal notranslate"> <span class="pre">burda2015importance</span></code> use. This importance weighted lower bound (IWBO) requires multiple samples from the approximate posterior. When using a single sample, the IWBO is identical to the ELBO, and as we increase the number of samples, the IWBO approaches the marginal likelihood. To get reliable estimates, we will evaluate the ELBO and the IWBO <code class="docutils literal notranslate"><span class="pre">num_repetitions</span> <span class="pre">=</span> <span class="pre">10</span></code> times each. For the IWBO, we will use <code class="docutils literal notranslate"><span class="pre">num_iwbo_samples</span> <span class="pre">=</span> <span class="pre">1000</span></code> samples for each estimate.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Helper for computing the ELBO and IWBO</span>
<span class="k">def</span> <span class="nf">evaluate_elbo_and_iwbo</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_repetitions</span><span class="p">,</span> <span class="n">num_iwbo_samples</span><span class="p">):</span>
    
    <span class="c1"># Compute ELBO and IWBO num_repetitions times</span>
    <span class="n">elbos</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">model</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_repetitions</span><span class="p">)</span>
    <span class="p">]</span>
    
    <span class="n">iwbos</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">model</span><span class="o">.</span><span class="n">iwbo</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_iwbo_samples</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_repetitions</span><span class="p">)</span>
    <span class="p">]</span>
    
    <span class="c1"># Compute mean and error estimate for the mean, of the ELBO and IWBO</span>
    <span class="n">elbo_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">elbos</span><span class="p">)</span>
    <span class="n">elbo_std</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">elbos</span><span class="p">)</span><span class="o">/</span><span class="n">num_repetitions</span><span class="o">**</span><span class="mf">0.5</span>
    
    <span class="n">iwbo_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">iwbos</span><span class="p">)</span>
    <span class="n">iwbo_std</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">iwbos</span><span class="p">)</span><span class="o">/</span><span class="n">num_repetitions</span><span class="o">**</span><span class="mf">0.5</span>
    
    <span class="k">return</span> <span class="n">elbo_mean</span><span class="p">,</span> <span class="n">elbo_std</span><span class="p">,</span> <span class="n">iwbo_mean</span><span class="p">,</span> <span class="n">iwbo_std</span>


<span class="c1"># Number of times to estimate the ELBO/IWBO</span>
<span class="c1"># and number of samples to draw for IWBO</span>
<span class="n">num_repetitions</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_iwbo_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Compute mean and standard deviation for each</span>
<span class="n">elbo_mean</span><span class="p">,</span> <span class="n">elbo_std</span><span class="p">,</span> <span class="n">iwbo_mean</span><span class="p">,</span> <span class="n">iwbo_std</span> <span class="o">=</span> <span class="n">evaluate_elbo_and_iwbo</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">num_repetitions</span><span class="o">=</span><span class="n">num_repetitions</span><span class="p">,</span>
    <span class="n">num_iwbo_samples</span><span class="o">=</span><span class="n">num_iwbo_samples</span>
<span class="p">)</span>

<span class="c1"># Print results</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;ELBO: </span><span class="si">{</span><span class="n">elbo_mean</span><span class="si">:</span><span class="s2"> 7.3f</span><span class="si">}</span><span class="s2"> +/- </span><span class="si">{</span><span class="mf">2.</span><span class="o">*</span><span class="n">elbo_std</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;(estimated with </span><span class="si">{</span><span class="n">num_repetitions</span><span class="si">}</span><span class="s2"> ELBO samples)&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;IWBO: </span><span class="si">{</span><span class="n">iwbo_mean</span><span class="si">:</span><span class="s2"> 7.3f</span><span class="si">}</span><span class="s2"> +/- </span><span class="si">{</span><span class="mf">2.</span><span class="o">*</span><span class="n">iwbo_std</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;(estimated with </span><span class="si">{</span><span class="n">num_repetitions</span><span class="si">}</span><span class="s2"> IWBO samples, &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;each using </span><span class="si">{</span><span class="n">num_iwbo_samples</span><span class="si">}</span><span class="s2"> weight samples)&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ELBO:  61.173 +/- 3.931 (estimated with 10 ELBO samples)
IWBO:  66.156 +/- 0.166 (estimated with 10 IWBO samples, each using 1000 weight samples)
</pre></div>
</div>
</div>
</div>
<p>We see that there is a statistically significant gap between the IWBO (<span class="math notranslate nohighlight">\(\approx\)</span> marginal likelihood). However, considering there are <span class="math notranslate nohighlight">\(100\)</span> datapoints in the dataset, the per-datapoint gap is relatively small. This suggests that the variational posterior is a good approximation to the true posterior. For a full comparison with existing approaches, it would be good to compare this with baselines such as mean-field VI<a class="bibtex reference internal" href="#blundell2015weight" id="id8">[BCKW15]</a> or other structured approximations. Recently, Thang Bui performed a more extensive comparison<a class="bibtex reference internal" href="#bui2021biases" id="id9">[Bui21]</a> of various VI approximations for neural networks, and found that GIP produced ELBOs close to the marginal log-likelihood, as estimated by Annealed Importance Sampling.<a class="bibtex reference internal" href="#neal2001annealed" id="id10">[Nea01]</a></p>
</div>
<div class="section" id="more-experiments">
<h2>More experiments<a class="headerlink" href="#more-experiments" title="Permalink to this headline">¶</a></h2>
<p>We next look at two factors which could affect the performance of GIP, the scale of the prior and the number of inducing points in the variational posterior. For convenience, we’ll define a helper function, which will train a GIP BNN some parameters, which we’ll then systematically sweep over.</p>
<div class="cell tag_hide-input tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">prior_scale_factor</span><span class="p">,</span>
                <span class="n">num_inducing</span><span class="p">,</span>
                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
                <span class="n">num_steps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">),</span>
                <span class="n">num_eval_reps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">num_iwbo_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>

    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
    <span class="k">def</span> <span class="nf">single_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimiser</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

            <span class="n">elbo</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">elbo</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">optimiser</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">elbo</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span>
    
    <span class="c1"># Dictionary for holding experiment results</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;train_elbo&quot;</span>     <span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;train_cond_lik&quot;</span> <span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;train_kl&quot;</span>       <span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;elbo_mean&quot;</span>      <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;elbo_std&quot;</span>       <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;iwbo_mean&quot;</span>      <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;iwbo_std&quot;</span>       <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;model&quot;</span>          <span class="p">:</span> <span class="kc">None</span>
    <span class="p">}</span>

    <span class="c1"># Set model constants</span>
    <span class="n">num_input</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">num_output</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">nonlinearity</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span>

    <span class="c1"># Initialise inducing points at subset of training points</span>
    <span class="n">inducing_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[:</span><span class="n">num_inducing</span><span class="p">]</span>
    <span class="n">inducing_points</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">inducing_idx</span><span class="p">)</span>

    <span class="c1"># Initialise model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GlobalInducingFullyConnectedNetwork</span><span class="p">(</span>
        <span class="n">num_input</span><span class="o">=</span><span class="n">num_input</span><span class="p">,</span>
        <span class="n">num_output</span><span class="o">=</span><span class="n">num_output</span><span class="p">,</span>
        <span class="n">inducing_points</span><span class="o">=</span><span class="n">inducing_points</span><span class="p">,</span>
        <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
        <span class="n">prior_scale_factor</span><span class="o">=</span><span class="n">prior_scale_factor</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
    <span class="p">)</span>

    <span class="c1"># Initialise optimiser</span>
    <span class="n">optimiser</span> <span class="o">=</span> <span class="n">tfk</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="c1"># Set progress bar and suppress warnings</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">get_logger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="s1">&#39;ERROR&#39;</span><span class="p">)</span>

    <span class="c1"># Set tensors for keeping track of quantities of interest</span>
    <span class="n">train_elbo</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_cond_lik</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_kl</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Train model</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>

        <span class="n">elbo</span><span class="p">,</span> <span class="n">cond_lik</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">single_step</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optimiser</span><span class="o">=</span><span class="n">optimiser</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">y</span>
        <span class="p">)</span>

        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_elbo&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elbo</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_cond_lik&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cond_lik</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_kl&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kl</span><span class="p">)</span>
        
    <span class="n">evaluation_results</span> <span class="o">=</span> <span class="n">evaluate_elbo_and_iwbo</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">num_repetitions</span><span class="o">=</span><span class="n">num_eval_reps</span><span class="p">,</span>
        <span class="n">num_iwbo_samples</span><span class="o">=</span><span class="n">num_iwbo_samples</span>
    <span class="p">)</span>
    
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;elbo_mean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">evaluation_results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;elbo_std&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">evaluation_results</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;iwbo_mean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">evaluation_results</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;iwbo_std&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">evaluation_results</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span>
    
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="effect-of-prior-scale">
<h3>Effect of prior scale<a class="headerlink" href="#effect-of-prior-scale" title="Permalink to this headline">¶</a></h3>
<p>The choice of prior heavily affects any Bayesian model. In fact, and perhaps unsurprisingly, in preliminary tests the prior scale seemed to have a substantial effect on the quality of the fit.</p>
<div class="cell tag_hide-input tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_prior_scales</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">min_log10_prior_scale</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span>
<span class="n">max_log10_prior_scale</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">num_inducing</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)</span>
<span class="n">num_eval_reps</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_iwbo_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">prior_scales</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
    <span class="n">min_log10_prior_scale</span><span class="p">,</span>
    <span class="n">max_log10_prior_scale</span><span class="p">,</span>
    <span class="n">num_prior_scales</span><span class="o">+</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Set progress bar and suppress warnings</span>
<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">prior_scales</span><span class="p">)))</span>

<span class="c1"># List for holding all experimental results</span>
<span class="n">prior_results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">prior_scale_factor</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>

    <span class="n">experiment_results</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
        <span class="n">prior_scale_factor</span><span class="o">=</span><span class="n">prior_scale_factor</span><span class="p">,</span>
        <span class="n">num_inducing</span><span class="o">=</span><span class="n">num_inducing</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span>
        <span class="n">num_eval_reps</span><span class="o">=</span><span class="n">num_eval_reps</span><span class="p">,</span>
        <span class="n">num_iwbo_samples</span><span class="o">=</span><span class="n">num_iwbo_samples</span>
    <span class="p">)</span>

    <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Experiment </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s2">02</span><span class="si">}</span><span class="s2"> Prior scale </span><span class="si">{</span><span class="n">prior_scale_factor</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    
    <span class="n">prior_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experiment_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">elbo_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;elbo_mean&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">prior_results</span><span class="p">])</span>
<span class="n">elbo_stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;elbo_std&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">prior_results</span><span class="p">])</span>

<span class="n">iwbo_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;iwbo_mean&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">prior_results</span><span class="p">])</span>
<span class="n">iwbo_stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;iwbo_std&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">prior_results</span><span class="p">])</span>

<span class="c1"># Figure to plot on</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot ELBO and error bars</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">prior_scales</span><span class="p">,</span>
    <span class="n">elbo_means</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;^&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;ELBO&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">prior_scales</span><span class="p">,</span>
    <span class="n">elbo_means</span> <span class="o">-</span> <span class="mf">2.</span><span class="o">*</span><span class="n">elbo_stds</span><span class="p">,</span>
    <span class="n">elbo_means</span> <span class="o">+</span> <span class="mf">2.</span><span class="o">*</span><span class="n">elbo_stds</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span>
<span class="p">)</span>

<span class="c1"># Plot IWBO and error bars</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">prior_scales</span><span class="p">,</span>
    <span class="n">iwbo_means</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;pink&quot;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;^&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;IWBO&quot;</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">prior_scales</span><span class="p">,</span>
    <span class="n">iwbo_means</span> <span class="o">-</span> <span class="mf">2.</span><span class="o">*</span><span class="n">iwbo_stds</span><span class="p">,</span>
    <span class="n">iwbo_means</span> <span class="o">+</span> <span class="mf">2.</span><span class="o">*</span><span class="n">iwbo_stds</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;pink&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span>
<span class="p">)</span>

<span class="c1"># Format figure</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;major&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;minor&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">prior_scales</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">prior_scales</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">160</span><span class="p">,</span> <span class="mi">110</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Prior scale $\sigma_p$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;ELBO and IWBO&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/gip_21_0.svg" src="../../../_images/gip_21_0.svg" /></div>
</div>
<p>We see that the scale of the prior has a dramatic effect on the model performance. For small scales, the model performs very poorly. Plotting the fit for small prior scales shows that the model collapses to a constant or linear function and explains most, if not all, the variation in the data as noise. As the prior scale increases, the model performs much better, presumably because the prior no longer forces the weights close to zero. We also note that the gap between the ELBO and the IWBO remains relatively small throughout, reinforcing the conclusion that this approximate posterior is a good approximation of the true posterior.</p>
</div>
<div class="section" id="effect-of-inducing-points">
<h3>Effect of inducing points<a class="headerlink" href="#effect-of-inducing-points" title="Permalink to this headline">¶</a></h3>
<p>Another interesting question is how the model performance depends on the number of inducing points. In variational approximations to GPs, the number of inducing points can heavily influence the quality of the approximation, and it would be no surprise if it did so here. However, the dependence of the approximate posterior and the predictive on the inducing points is less clear for GIP BNNs than it is for GPs. Here we vary the number of inducing points from a handful of points up to one inducing point per datapoint.</p>
<div class="cell tag_hide-input tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_inducing_increment</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">min_inducing_points</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">max_inducing_points</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">prior_scale_factor</span> <span class="o">=</span> <span class="mf">2.</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)</span>
<span class="n">num_eval_reps</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_iwbo_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">nums_inducing_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
    <span class="n">min_inducing_points</span><span class="p">,</span>
    <span class="n">max_inducing_points</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_inducing_increment</span>
<span class="p">)</span>

<span class="n">nums_inducing_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">nums_inducing_points</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Set progress bar and suppress warnings</span>
<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">nums_inducing_points</span><span class="p">)))</span>

<span class="c1"># List for holding all experimental results</span>
<span class="n">inducing_results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">num_inducing</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>

    <span class="n">experiment_results</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
        <span class="n">prior_scale_factor</span><span class="o">=</span><span class="n">prior_scale_factor</span><span class="p">,</span>
        <span class="n">num_inducing</span><span class="o">=</span><span class="n">num_inducing</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span>
        <span class="n">num_eval_reps</span><span class="o">=</span><span class="n">num_eval_reps</span><span class="p">,</span>
        <span class="n">num_iwbo_samples</span><span class="o">=</span><span class="n">num_iwbo_samples</span>
    <span class="p">)</span>

    <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Experiment </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s2">02</span><span class="si">}</span><span class="s2"> # inducing </span><span class="si">{</span><span class="n">num_inducing</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    
    <span class="n">inducing_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experiment_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">elbo_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;elbo_mean&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">inducing_results</span><span class="p">])</span>
<span class="n">elbo_stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;elbo_std&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">inducing_results</span><span class="p">])</span>

<span class="n">iwbo_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;iwbo_mean&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">inducing_results</span><span class="p">])</span>
<span class="n">iwbo_stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;iwbo_std&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">inducing_results</span><span class="p">])</span>

<span class="c1"># Figure to plot on</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot ELBO and error bars</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">nums_inducing_points</span><span class="p">,</span>
    <span class="n">elbo_means</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;^&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;ELBO&quot;</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">nums_inducing_points</span><span class="p">,</span>
    <span class="n">elbo_means</span> <span class="o">-</span> <span class="mf">2.</span><span class="o">*</span><span class="n">elbo_stds</span><span class="p">,</span>
    <span class="n">elbo_means</span> <span class="o">+</span> <span class="mf">2.</span><span class="o">*</span><span class="n">elbo_stds</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span>
<span class="p">)</span>

<span class="c1"># Plot IWBO and error bars</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">nums_inducing_points</span><span class="p">,</span> <span class="n">iwbo_means</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;pink&quot;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;^&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;IWBO&quot;</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">nums_inducing_points</span><span class="p">,</span> 
    <span class="n">iwbo_means</span> <span class="o">-</span> <span class="mf">2.</span><span class="o">*</span><span class="n">iwbo_stds</span><span class="p">,</span> 
    <span class="n">iwbo_means</span> <span class="o">+</span> <span class="mf">2.</span><span class="o">*</span><span class="n">iwbo_stds</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;pink&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span>
<span class="p">)</span>

<span class="c1"># Format figure</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">nums_inducing_points</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nums_inducing_points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">110</span><span class="p">,</span> <span class="mi">110</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">nums_inducing_points</span><span class="p">[</span><span class="mi">3</span><span class="p">:],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# inducing points&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;ELBO and IWBO&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/gip_25_0.svg" src="../../../_images/gip_25_0.svg" /></div>
</div>
<p>With very few inducing points, the model does not perform well at all. As we increase the number of inducing points, the model performance increases accordingly, until it reaches a plateau, after which it does not improve further.</p>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>The GIP method is a variational method for approximating the posterior over weights of BNNs. Ober and Aitchison also show that GIP posteriors are also applicable to deep Gaussian processes (DGPs), though we have not looked at this here. From the experiments done here, we have a few takeaways. GIP posteriors can represent in-between uncertainty between different clusters of data, overcoming a common pathology of mean-field VI. They are also relatively easy to train, requiring no particular initialisation or training tricks, which are used in other methods. Further, GIP posteriors seem to be fairly good approximations of the ground truth predictive posteriors which they are approximating. This was evidenced by the small gap between the ELBO and the IWBO in the experiments above. We also saw that the prior scale has a significant effect on the model fit, as is reasonable to expect from any Bayesian model. Last, we saw that GIP posteriors are fairly robust to the number of inducing points used, performing fairly well with even a relatively small number on inducing inputs. Ober and Aitchison include more extensive evaluations of GIP and competing methods on UCI datasets, for both BNNs and DGPs. They have also followed up GIP with subsequent work on deep kernel processes <a class="bibtex reference internal" href="#aitchison2021deep" id="id11">[AYO21]</a> and deep Wishart processes.<a class="bibtex reference internal" href="#ober2021variational" id="id12">[OA21b]</a> Overall, this reproduction shows that GIP is a fairly easy method to work with, and gives fairly good results in the context of BNNs.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-content/misc/gip/gip-0"><dl class="citation">
<dt class="bibtex label" id="aitchison2021deep"><span class="brackets"><a class="fn-backref" href="#id11">AYO21</a></span></dt>
<dd><p>Laurence Aitchison, Adam X. Yang, and Sebastian W. Ober. Deep kernel processes. 2021. <a class="reference external" href="https://arxiv.org/abs/2010.01590">arXiv:2010.01590</a>.</p>
</dd>
<dt class="bibtex label" id="blundell2015weight"><span class="brackets">BCKW15</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In <em>International Conference on Machine Learning</em>. PMLR, 2015.</p>
</dd>
<dt class="bibtex label" id="bui2021biases"><span class="brackets"><a class="fn-backref" href="#id9">Bui21</a></span></dt>
<dd><p>Thang D Bui. Biases in variational bayesian neural networks. <em>Bayesian Deep Learning workshop</em>, 2021.</p>
</dd>
<dt class="bibtex label" id="foong2019between"><span class="brackets">FLHLT19</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Andrew YK Foong, Yingzhen Li, Jose Miguel Hernandez-Lobato, and Richard E Turner. In-between uncertainty in bayesian neural networks. <em>arXiv preprint arXiv:1906.11537</em>, 2019.</p>
</dd>
<dt class="bibtex label" id="neal2001annealed"><span class="brackets"><a class="fn-backref" href="#id10">Nea01</a></span></dt>
<dd><p>Radford M Neal. Annealed importance sampling. <em>Statistics and computing</em>, 2001.</p>
</dd>
<dt class="bibtex label" id="ober2021global"><span class="brackets">OA21a</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>Sebastian W Ober and Laurence Aitchison. Global inducing point variational posteriors for bayesian neural networks and deep gaussian processes. In <em>International Conference on Machine Learning</em>, 8248–8259. PMLR, 2021.</p>
</dd>
<dt class="bibtex label" id="ober2021variational"><span class="brackets"><a class="fn-backref" href="#id12">OA21b</a></span></dt>
<dd><p>Sebastian W. Ober and Laurence Aitchison. A variational approximate posterior for the deep wishart process. 2021. <a class="reference external" href="https://arxiv.org/abs/2107.10125">arXiv:2107.10125</a>.</p>
</dd>
<dt class="bibtex label" id="salimbeni2017doubly"><span class="brackets"><a class="fn-backref" href="#id6">SD17</a></span></dt>
<dd><p>Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian processes. 2017. <a class="reference external" href="https://arxiv.org/abs/1705.08933">arXiv:1705.08933</a>.</p>
</dd>
<dt class="bibtex label" id="turner2011problems"><span class="brackets"><a class="fn-backref" href="#id1">TS11</a></span></dt>
<dd><p>R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series models. In <em>Bayesian Time series models</em>. Cambridge University Press, 2011.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "venv-random-walks"
        },
        kernelOptions: {
            kernelName: "venv-random-walks",
            path: "./content/misc/gip"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'venv-random-walks'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../gumbel/gumbel.html" title="previous page">Gumbel distribution</a>
    <a class='right-next' id="next-link" href="../../reading-and-links.html" title="next page">Interesting reading and websites</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratos Markou<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>