{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stein variational gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.d-flex {\n",
       "    display: none!important;\n",
       "}\n",
       "\n",
       ".bd-toc nav {\n",
       "    opacity: 1;\n",
       "    max-height: 100vh!important;\n",
       "}\n",
       "\n",
       ".bd-toc .nav .nav {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".prev-next-bottom {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       ".footer {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       ".navbar_footer {\n",
       "\tdisplay: none;\n",
       "}\n",
       "\n",
       ".definition {\n",
       "\tbackground-color: rgba(123, 183, 223, 0.33);\n",
       "\tborder-radius: 20px;\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "\tborder-style: solid;\n",
       "\tborder-color: rgba(123, 183, 223, 0.5);\n",
       "}\n",
       "\n",
       ".theorem {\n",
       "\tbackground-color: rgba(240, 213, 75, 0.33);\n",
       "\tborder-radius: 20px;\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "\tborder-style: solid;\n",
       "\tborder-color: rgba(240, 213, 75, 0.5);\n",
       "}\n",
       "\n",
       ".lemma {\n",
       "\tbackground-color: rgba(255, 218, 185, 0.33);\n",
       "\tborder-radius: 20px;\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "\tborder-style: solid;\n",
       "\tborder-color: rgba(255, 218, 185, 0.5);\n",
       "}\n",
       "\n",
       ".observation {\n",
       "\tbackground-color: rgba(178, 234, 188, 0.33);\n",
       "\tborder-radius: 20px;\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "\tborder-style: solid;\n",
       "\tborder-color: rgba(178, 234, 188, 0.5);\n",
       "}\n",
       "\n",
       "details.proof {\n",
       "    border-width: 2px;\n",
       "\tborder-radius: 20px;\n",
       "\tborder-style: solid;\n",
       "\tbackground-color: rgba(128, 128, 128, 0.2);\n",
       "\tborder-color: rgba(128, 128, 128, 0.1);\n",
       "}\n",
       "\n",
       "\n",
       "details.proof div{\n",
       "\tpadding: 20px 20px 10px 20px;\n",
       "}\n",
       "\n",
       "details.proof {\n",
       "\tpadding: 0px 30px 0px 30px;\n",
       "}\n",
       "\n",
       "summary {\n",
       "\tmargin-left: -32px;\n",
       "\tpadding-left: 15px;\n",
       "\tpadding-right: 15px;\n",
       "}\n",
       "\n",
       "summary + * {\n",
       "    margin-top: 10px;\n",
       "}\n",
       "\n",
       ".tag_center-output div img {\n",
       "    display:block;\n",
       "    margin:auto;\n",
       "}\n",
       "\n",
       ".container {\n",
       "\twidth : 103% !important;\n",
       "}\n",
       "\n",
       "details > .math.notranslate.nohighlight {\n",
       "\tmargin-top: -50px;\n",
       "\tmargin-bottom: -15px;\n",
       "}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import HTML, set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "css_style = open('../../../_static/custom_style.css', 'r').read()\n",
    "HTML(f'<style>{css_style}</style>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theorem\">\n",
    "    \n",
    "**Theorem (Gradient of KL is the KSD)** Let $x \\sim q(x)$, and $T(x) = x + \\epsilon \\phi(x)$, where $\\phi$ is a smooth function. Then\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\nabla_{\\epsilon}\\text{KL}(q_{[T]} || p) \\big|_{\\epsilon = 0} = - \\mathbb{E}_{x \\sim q}\\left[\\mathcal{A}_p \\phi(x)\\right],\n",
    "\\end{align}$$\n",
    "    \n",
    "where $q_{[T]}$ is the density of $T(x)$ and\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\mathcal{A}_p \\phi(x) = \\nabla_x \\log p(x)\\phi^\\top(x) + \\nabla_x \\phi(x).\n",
    "\\end{align}$$\n",
    "    \n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<details class=\"proof\">\n",
    "<summary>Proof: Gradient of KL</summary>\n",
    "    \n",
    "Let $p_{\\left[T^{-1}\\right]}(x)$ denote the density of $z = T^{-1}(x)$ when $x \\sim p(x)$. By changing the variable of integration from $z$ to $x = T^{-1}(x)$, we obtain\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\text{KL}(q_{[T]} || p) &= \\int q_{[T]}(z) \\log \\frac{q_{[T]}(z)}{p(z)} dz \\\\\n",
    "                        &= \\int q(x) \\left[ \\log q(x) - \\log p_{\\left[T^{-1}\\right]}(x) + \\log  \\right] dx.\n",
    "\\end{align}$$\n",
    "    \n",
    "This change of variables is convenient because now only one term in the integral depends on $\\epsilon$, that is $p_{\\left[T^{-1}\\right]}(x)$. Now taking the derivative with respect to $\\epsilon$ we obtain\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\nabla_{\\epsilon} \\text{KL}(q_{[T]} || p) &= - \\int q(x) \\nabla_{\\epsilon} \\log p_{\\left[T^{-1}\\right]}(x) dx, \\\\\n",
    "                                          &= - \\int q(x) \\nabla_{\\epsilon} \\log p_{\\left[T^{-1}\\right]}(x) dx,\n",
    "\\end{align}$$\n",
    "    \n",
    "and using the fact that\n",
    "\n",
    "$$\\begin{align}\n",
    "\\log p_{\\left[T^{-1}\\right]}(x) &= \\log p(T(x)) + \\log |\\nabla_x T(x)|,\n",
    "\\end{align}$$\n",
    "    \n",
    "we obtain the expression\n",
    "\n",
    "$$\\begin{align}\n",
    "\\nabla_{\\epsilon} \\log p_{\\left[T^{-1}\\right]}(x) &= \\nabla \\log p(T(x))^\\top \\nabla_\\epsilon T(x) + \\nabla_\\epsilon \\log |\\nabla_x T(x)|, \\\\\n",
    "                                                  &= \\nabla \\log p(T(x))^\\top \\nabla_\\epsilon T(x) + \\text{trace}\\left[(\\nabla_x T(x))^{-1} \\nabla_\\epsilon \\nabla_x T(x)\\right],\n",
    "\\end{align}$$\n",
    "    \n",
    "where we used the identity\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\nabla_{\\epsilon} \\log |\\det A| = \\text{trace} A^{-1} \\nabla_{\\epsilon} A,\n",
    "\\end{align}$$\n",
    "    \n",
    "we arrive at the following expression for the derivative\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\nabla_{\\epsilon} \\text{KL}(q_{[T]} || p) &= - \\mathbb{E}_{x \\sim q} \\left[\\nabla \\log p(T(x))^\\top \\nabla_\\epsilon T(x) + \\text{trace} (\\nabla_x T(x))^{-1} \\nabla_\\epsilon \\nabla_x T(x)\\right].\n",
    "\\end{align}$$\n",
    "    \n",
    "Setting $T(x) = x + \\epsilon \\phi(x)$ yields the result\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\nabla_{\\epsilon} \\text{KL}(q_{[T]} || p) &= - \\mathbb{E}_{x \\sim q} \\left[\\nabla \\log p(x)^\\top \\phi(x) + \\text{trace}\\left[\\nabla_x \\phi(x) \\right]\\right], \\\\\n",
    "                                          &= - \\mathbb{E}_{x \\sim q} \\left[\\text{trace} \\mathcal{A}_p \\phi(x) \\right].\n",
    "\\end{align}$$\n",
    "    \n",
    "</details>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"theorem\">\n",
    "    \n",
    "**Theorem (Direction of steepest descent of the KL)** The direction $\\phi^* \\in \\mathcal{H}_D$ of steepest descent of the KL-divergence is given by the expression\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\phi^*(\\cdot) = \\mathbb{E}_{x \\sim q}\\left[ k(x, \\cdot) \\nabla_x \\log p(x) + \\nabla_x k(x, \\cdot)\\right].\n",
    "\\end{align}$$\n",
    "    \n",
    "</div>\n",
    "<br>\n",
    "\n",
    "\n",
    "<details class=\"proof\">\n",
    "<summary>Proof: Direction of steepest descent of the KL</summary>\n",
    "    \n",
    "For $f \\in \\mathcal{H}_D$ we have the following equality\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\langle f, \\phi^* \\rangle_{\\mathcal{H}_D} &= \\sum_{d = 1}^D \\langle f_d(\\cdot), \\phi^* \\rangle_{\\mathcal{H}} \\\\\n",
    "                                          &= \\sum_{d = 1}^D \\left \\langle f_d(\\cdot), \\mathbb{E}_{x \\sim q}\\left[k(x, \\cdot) \\nabla_{x_d} \\log p(x) + \\nabla_{x_d} k(x, \\cdot)\\right] \\right\\rangle_{\\mathcal{H}} \\\\\n",
    "                                          &= \\sum_{d = 1}^D \\mathbb{E}_{x \\sim q}\\left[\\nabla_{x_d} \\log p(x) \\langle f_d(\\cdot), k(x, \\cdot) \\rangle + \\langle f_d(\\cdot), \\nabla_{x_d} k(x, \\cdot) \\rangle \\right] \\rangle_{\\mathcal{H}} \\\\\n",
    "                                          &= \\sum_{d = 1}^D \\mathbb{E}_{x \\sim q}\\left[\\nabla_{x_d} \\log p(x) f_d(x) + \\nabla_{x_d} f_d(x) \\right] \\\\\n",
    "                                          &= \\mathbb{E}_{x \\sim p}\\left[\\mathcal{A}_q f(x)\\right].\n",
    "\\end{align}$$\n",
    "    \n",
    "Therefore, the $f \\in \\mathcal{H}_D$ which maximises $\\mathbb{E}_{x \\sim p}\\left[\\mathcal{A}_q f(x)\\right]$ is the one which maximises the inner product $\\langle f, \\phi^* \\rangle_{\\mathcal{H}_D}$, which occurs when $f$ is parallel to $\\phi^*$.\n",
    "    \n",
    "</details>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"definition\">\n",
    "    \n",
    "**Algorithm (Stein variational gradient descent)** Given a distribution $p(x)$, a postive definite kernel $k(x, x')$ and a set of particles with initial positions $\\{x_n\\}_{n=1}^N$, Stein variational gradient descent evolves the particles according to\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\frac{d x_m}{dt} = \\sum_{n = 1}^N \\left[ k(x_n, x_m) \\nabla_x \\log p(x)|_{x_n} + \\nabla_x k(x_m, x) |_{x_n}\\right].\n",
    "\\end{align}$$\n",
    "    \n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(x, x_, lengthscale):\n",
    "    \n",
    "    diff = x[:, None, :] - x_[None, :, :]\n",
    "    quad = np.sum((diff / lengthscale) ** 2, axis=2)\n",
    "    exp = np.exp(-0.5 * quad)\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mog_logprob(locs, scales, weights):\n",
    "    \n",
    "    def logprob(x):\n",
    "        \n",
    "        diff = x[:, None, :] - locs[None, :, :]\n",
    "        quad = np.sum((diff / scales) ** 2, axis=2)\n",
    "        \n",
    "        coeffs = \n",
    "        exp = np.exp(-0.5 * quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-random-walks",
   "language": "python",
   "name": "venv-random-walks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
