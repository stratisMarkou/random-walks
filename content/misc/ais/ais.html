
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Annealed Importance Sampling &#8212; Random walks</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://random-walks.org/content/misc/ais/ais.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Interesting reading and websites" href="../../reading-and-links.html" />
    <link rel="prev" title="Global inducing points for BNNs" href="../gip/gip.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://random-walks.org/content/misc/ais/ais.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Annealed Importance Sampling" />
<meta property="og:description" content="Annealed Importance Sampling  Simulating samples from distributions is a central problem in Statistics and Machine Learning, because it enables estimating impor" />
<meta property="og:image"       content="https://random-walks.org/_static/logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Random walks</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../home.html">
   Random walks
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../prob-intro/intro.html">
   Probability: An introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch01/content.html">
     Events and Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch02/content.html">
     Discrete random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch03/content.html">
     Multivariate discrete distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch04/content.html">
     Probability generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch05/content.html">
     Distribution and density functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch06/content.html">
     Multivariate distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch07/content.html">
     Moment generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch08/content.html">
     Main limit theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch09/content.html">
     Branching processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch10/content.html">
     Random walks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch11/content.html">
     Processes in continuous time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch12/content.html">
     Markov chains
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../cvx/intro.html">
   Convex optimisation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch01.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch02.html">
     Convex sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch03.html">
     Convex functions
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../gp/gp-intro.html">
   Gaussian Processes
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../gp/why-covariances.html">
     Why covariance functions?
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../gp/sparse/sparse-intro.html">
     Sparse Gaussian Processes
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/vfe.html">
       Variational Free Energy GPs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/gp-sampling.html">
       Sampling GP posteriors
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../gp/vfer/vfer.html">
     Variational Inference Revisited
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../misc.html">
   Miscellaneous
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../sde/num-sde.html">
     Numerical simulation of SDEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sde-as-gp/sde-as-gp.html">
     VI for diffusion processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../node/node.html">
     Neural ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../optimisation/conjugate-gradients.html">
     Conjugate gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../kalman/kalman.html">
     The Kalman filter and smoother
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ncs/ncs.html">
     Natural cubic splines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ars/ars.html">
     Adaptive rejection sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../score-matching/score-matching.html">
     Estimation by score matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interacting/interacting.html">
     Interacting particle FPK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rff/rff.html">
     Random Fourier features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../svgd/svgd.html">
     Stein variational gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../addgp/addgp.html">
     Additive Gaussian Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gumbel/gumbel.html">
     Gumbel distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gip/gip.html">
     Global inducing points for BNNs
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Annealed Importance Sampling
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reading-and-links.html">
   Interesting reading and websites
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/misc/ais/ais.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/stratisMarkou/random-walks/master?urlpath=tree/./content/misc/ais/ais.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#importance-sampling">
   Importance sampling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#importance-weighted-mcmc">
   Importance-weighted MCMC
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Annealed Importance Sampling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#toy-experiment">
     Toy experiment
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="annealed-importance-sampling">
<h1>Annealed Importance Sampling<a class="headerlink" href="#annealed-importance-sampling" title="Permalink to this headline">¶</a></h1>
<p>Simulating samples from distributions is a central problem in Statistics and Machine Learning, because it enables estimating important quantities such as integrals. For example, we are often interested in evaluating integrals of the form</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I = \int p(x) f(x) dx,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is a probability density and <span class="math notranslate nohighlight">\(f\)</span> is a function of interest. If we have acccess to the cumulative density function <span class="math notranslate nohighlight">\(p\)</span>, we can easily draw samples from it using <a class="reference external" href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inverse transform sampling</a>. However, in most cases, the cumulative density function does not have an analytically tractable closed form, so sampling via the inverse transform does not apply. For example, non-conjugate Bayesian models typically involve intractable distributions of this kind. In such cases, we must resort to approximation methods, such as Monte Carlo.</p>
<p>A standard Monte Carlo method for handling intractable integrals is importance sampling. Importance sampling gets around the intractability of <span class="math notranslate nohighlight">\(p\)</span> by introducing another tractable distribution <span class="math notranslate nohighlight">\(q\)</span> and drawing samples from <span class="math notranslate nohighlight">\(q\)</span> insted of <span class="math notranslate nohighlight">\(p\)</span>. It then corrects for the bias in the samples, to account for the fact that these now come from <span class="math notranslate nohighlight">\(q\)</span> rather than <span class="math notranslate nohighlight">\(p\)</span>, by weighing them appropriately, using so called importance weights. In this way, importance sampling yields an unbiased estimate of downstream integrals, while circumventing the intractability of <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>Unfortunately, if <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(p\)</span> are very dissimilar, the importance sampling estimator has a large random error because, as we will see, the variance in the random weights is large. Annealed importance sampling<a class="bibtex reference internal" href="#neal2001annealed" id="id1">[Nea01]</a> is a method which circumvents this issue by using an annealing procedure which produces samples with more equally distributed importance weights while remaining unbiased, and can greatly reduce the variance of the resulting estimates.</p>
<div class="section" id="importance-sampling">
<h2>Importance sampling<a class="headerlink" href="#importance-sampling" title="Permalink to this headline">¶</a></h2>
<p>Suppose we wish to evaluate an integral of the form</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I = \int p(x) f(x) dx.
\end{equation}\]</div>
<p>If we could draw samples from <span class="math notranslate nohighlight">\(p\)</span>, we could estimate this integral by Monte Carlo, by computing</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I \approx \frac{1}{N} \sum_{n=1}^N f(x_n) ~~\text{ where }~~ x_n \sim p.
\end{equation}\]</div>
<p>However, in many applications of interest, we cannot draw samples from <span class="math notranslate nohighlight">\(p\)</span> directly. Importance sampling gets around this by introducing another tractable distribution <span class="math notranslate nohighlight">\(q\)</span>, and rewriting this integral as</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I = \int q(x) \frac{p(x)}{q(x)} f(x) dx.
\end{equation}\]</div>
<p>A technical, but important, requirement here is that <span class="math notranslate nohighlight">\(q\)</span> should be non-zero whenever <span class="math notranslate nohighlight">\(p\)</span> is non-zero, for the integral above to be well defined. Hereafter we assume this to be the case. Since the distribution <span class="math notranslate nohighlight">\(q\)</span> is tractable, we can estimate the value of the integral by drawing samples from it and computing</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
I \approx \frac{1}{N} \sum_{n=1}^N \underbrace{\frac{p(x_n)}{q(x_n)}}_{w_n} f(x_n) ~~\text{ where }~~ x_n \sim q.
\end{equation}\]</div>
<p>The ratios <span class="math notranslate nohighlight">\(w_n\)</span> are called importance weights, since they weigh the contribution of each <span class="math notranslate nohighlight">\(f(x_n)\)</span> term in the sum. Since <span class="math notranslate nohighlight">\(q\)</span> proposes the samples to be used in the Monte Carlo sum, it is commonly referred to it as the proposal distribution. We also refer to <span class="math notranslate nohighlight">\(p\)</span> as the target distribution, since this is the one we are ultimately interested in. This is called the importance sampling estimator, and is unbiased, meaning that in expectation it is equal to the original integral</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\mathbb{E}\left[ \frac{1}{N} \sum_{n=1}^N \frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{1}{N} \sum_{n=1}^N \mathbb{E}\left[\frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{1}{N} \sum_{n=1}^N I = I.
\end{equation}\]</div>
<p>However, in practice we always end up using a finite number of samples, so the importance sampling estimate will not be exactly equal to <span class="math notranslate nohighlight">\(I\)</span>, but will be off by some random error, whose magnitude is captured by the variance of the estimator. In particular, if the variance of the estimator using a single sample is <span class="math notranslate nohighlight">\(V\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
V = \mathbb{V}\left[\sum_{n=1}^N \frac{p(x_n)}{q(x_n)} f(x_n) \right] \geq 0,
\end{equation}\]</div>
<p>then using <span class="math notranslate nohighlight">\(N\)</span> samples reduces the variance by a factor of <span class="math notranslate nohighlight">\(N\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\mathbb{V}\left[\frac{1}{N}\sum_{n=1}^N \frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{1}{N^2}\mathbb{V}\left[\sum_{n=1}^N \frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{1}{N^2}\sum_{n=1}^N \mathbb{V}\left[\frac{p(x_n)}{q(x_n)} f(x_n) \right] = \frac{V}{N}.
\end{equation}\]</div>
<p>Using more samples reduces the overall variance, but increases the computational cost. A common issue that arises with the importance sampling estimator is that its variance, <span class="math notranslate nohighlight">\(V\)</span>, can be very large. This means that lots of samples, and thus lots of computate, must be used to obtain a reasonable estimate.</p>
<p>Let’s look at a simple example of Importance Sampling. Suppose <span class="math notranslate nohighlight">\(f(x) = x^3\)</span> and let <span class="math notranslate nohighlight">\(p\)</span> be a mixture of Gaussians</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
p(x) = \pi \mathcal{N}(x; \mu_1, \sigma_1^2) + (1 - \pi) \mathcal{N}(x; \mu_2, \sigma_2^2).
\end{equation}\]</div>
<p>Of course, we can in fact draw samples from a mixture of Gaussians directly, but let’s pretend we can’t. Now, define <span class="math notranslate nohighlight">\(q\)</span> to be a Gaussian</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
q(x) = \mathcal{N}(x; \mu_q, \sigma_q^2).
\end{equation}\]</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mf">3.</span>

<span class="n">x_plot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">m1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">s1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">m2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">s2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">mq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">sq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">mix_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">p1</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">m1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">m2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s2</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Mixture</span><span class="p">(</span><span class="n">cat</span><span class="o">=</span><span class="n">tfd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">mix_probs</span><span class="p">),</span> <span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">])</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mq</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sq</span><span class="p">)</span>

<span class="n">p_plot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x_plot</span><span class="p">))</span>
<span class="n">q_plot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x_plot</span><span class="p">))</span>
<span class="n">f_plot</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_plot</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/ais_4_0.svg" src="../../../_images/ais_4_0.svg" /></div>
</div>
<p>Using importance sampling, we can obtain an estimate of this intergral.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the random seed for repeatable results</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># How many samples to draw</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Draw samples</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">num_samples</span><span class="p">])</span>

<span class="c1"># Compute importance weights and importance weighted integral</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">-</span> <span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">f</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Importance Sampling estimate I = </span><span class="si">{</span><span class="n">I</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Importance Sampling estimate I = -5.40
</pre></div>
</div>
</div>
</div>
<p>Note that the exact value of the integral is <span class="math notranslate nohighlight">\(I = 0\)</span>, which we can see because <span class="math notranslate nohighlight">\(p(x)\)</span> is symmetric around <span class="math notranslate nohighlight">\(x = 0\)</span> while <span class="math notranslate nohighlight">\(f(x) = x^3\)</span> is antisymmetric. However the estimate was <span class="math notranslate nohighlight">\(I \approx -5.46\)</span> which seems a bit off. Repeating this experiment <span class="math notranslate nohighlight">\(100\)</span> times we obtain:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># How many samples to draw</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">100</span>
    
<span class="c1"># Draw samples</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">num_trials</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">])</span>

<span class="c1"># Compute importance weights and importance weighted integral</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">-</span> <span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">f</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">mean_I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
<span class="n">std_I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Importance Sampling over </span><span class="si">{</span><span class="n">num_trials</span><span class="si">}</span><span class="s2"> trials &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;I = </span><span class="si">{</span><span class="n">mean_I</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> +/- </span><span class="si">{</span><span class="mf">2.</span><span class="o">*</span><span class="n">std_I</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">.&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Importance Sampling over 100 trials I = 0.10 +/- 13.309.
</pre></div>
</div>
</div>
</div>
<p>So even though the estimator is unbiased, it has a large variance. Before we go further, let’s compare the importance sampling estimate to a Monte Carlo estimate using samples directly from <span class="math notranslate nohighlight">\(p\)</span>. Note that in practice this is not possible, because <span class="math notranslate nohighlight">\(p\)</span> is not in general available in closed form, but that’s why we picked <span class="math notranslate nohighlight">\(p\)</span> to be a mixture of Gaussians. The point of this additional Monte Carlo estimate is to give us an idea of how much random error we should expect if we had access to exact samples from <span class="math notranslate nohighlight">\(p\)</span>, which we cannot expect to beat by simply reducing the variance in the importance weights.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># How many samples to draw</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">100</span>
    
<span class="c1"># Draw samples</span>
<span class="n">px</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">num_trials</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">])</span>

<span class="c1"># Compute importance weights and importance weighted integral</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">f</span><span class="p">(</span><span class="n">px</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">mean_I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
<span class="n">std_I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Monte Carlo with samples from p over </span><span class="si">{</span><span class="n">num_trials</span><span class="si">}</span><span class="s2"> trials &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;I = </span><span class="si">{</span><span class="n">mean_I</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> +/- </span><span class="si">{</span><span class="mf">2.</span><span class="o">*</span><span class="n">std_I</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">.&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Monte Carlo with samples from p over 100 trials I = -0.07 +/- 1.848.
</pre></div>
</div>
</div>
</div>
<p>The Importance sampling estimator has seven times larger random error than the Monte Carlo estimator. To see why, this occurs, let’s plot the raw samples drawn from <span class="math notranslate nohighlight">\(q\)</span> (in green) as well as the samples weighted according to their importance weights (in blue).</p>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the random seed for repeatable results</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># How many samples to draw</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Draw samples and compute importance weights</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">num_samples</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">-</span> <span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot proposal</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">p_plot</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">)</span>

<span class="c1"># Plot target</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">q_plot</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:green&quot;</span><span class="p">)</span>

<span class="c1"># Plot raw samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:green&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Samples from $q$&quot;</span>
<span class="p">)</span>

<span class="c1"># Plot samples weighted by their importance weights</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="n">w</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Weighted samples from $q$&quot;</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Target $p$, proposal $q$, function $f$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$p(x),~ q(x)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">twin_axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>

<span class="c1"># Plot function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">f_plot</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$f(x)$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$f(x)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">,</span> <span class="mf">60.</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/ais_12_0.svg" src="../../../_images/ais_12_0.svg" /></div>
</div>
<p>First off, this plot illustrates how importance sampling works. Although <span class="math notranslate nohighlight">\(q\)</span> proposes many samples in the middle, these are down-weighted by the importance weights, since <span class="math notranslate nohighlight">\(p\)</span> has small density there. Note that the samples in the middle are still there in the blue histogram, but they have just been down-weighted by the importance weights. Conversely, whenever <span class="math notranslate nohighlight">\(q\)</span> proposes a sample at a point where <span class="math notranslate nohighlight">\(p\)</span> has high density, the importance weight becomes much larger. In this way, importance sampling moulds the empirical distribution of samples from <span class="math notranslate nohighlight">\(q\)</span> (in green) to resemble the target distribution <span class="math notranslate nohighlight">\(p\)</span> (in blue).</p>
<p>However, the samples which receive a high importance are relatively infrequent, which means that sometimes we may get sample with a large importance weight in one of the two Gaussian modes, but not in the other mode. Since this sample has a large importance weight, it greatly affects the overall estimate, introducing lots of random error. Looking at a histogram of the importance weights we see that most weights are very small, and it’s only a few large weights which dominate the value of the integral (note the <span class="math notranslate nohighlight">\(x\)</span>-axis has a log-scale).</p>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot distributions and samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot importance weights</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">w</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Importance weights&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Empirical frequency of importance weights&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Importance weight $w$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Empirical frequency&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/ais_14_0.svg" src="../../../_images/ais_14_0.svg" /></div>
</div>
<p>Going a little further, note that the expectation of the value of an importance weight is always equal to one since</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\int q(x) \frac{p(x)}{q(x)} dx = 1 \implies \mathbb{E}_{x \sim q(x)}\left[\frac{p(x)}{q(x)} \right] = 1,
\end{equation}\]</div>
<p>This means that if <span class="math notranslate nohighlight">\(q\)</span> proposes samples with small importance weights most of the time, it must also propose some samples with very large importance weights to make up for the small importance weights: the integrand cannot always be smaller than <span class="math notranslate nohighlight">\(1\)</span> or always larger than <span class="math notranslate nohighlight">\(1\)</span>, otherwise it would not integrate to <span class="math notranslate nohighlight">\(1\)</span>. The quantity which affects the amount of random error in an importance sampling estimator is the variability of the importance weights around this mean, that is the variance of the importance weights. It is reasonable to expect that the more dissimilar <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(p\)</span> are, the larger the variance will be. In partricular, we can show that the variance of the importance weights can be lower bounded by a quantity that scales exponentially with the KL divergence.</p>
<div class="lemma">
<p><strong>Lemma (Lower bound to importance weight variance)</strong> Given distributions <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>, it holds that</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\mathbb{V}_{x \sim q}\left[\frac{p(x)}{q(x)}\right] \geq e^K - 1, \text{ where } K = \max\left(D_{KL}(p || q),~ 2 D_{KL}(q || p)\right) 
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(D_{KL}\)</span> denotes the Kullback-Leibler diverence, measured in nats.</p>
</div>
<br>
<details class="proof">
<summary>Derivation: Lower bound to importance weight variance</summary>
<br>
<p>The variance in the importance weights can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{V}_{x \sim q}\left[\frac{p(x)}{q(x)}\right] &amp;= \mathbb{E}_{x \sim q}\left[\left(\frac{p(x)}{q(x)}\right)^2\right] - \mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)}\right]^2, \\
&amp;= \mathbb{E}_{x \sim q}\left[\left(\frac{p(x)}{q(x)}\right)^2\right] - 1, \\
&amp;= \mathbb{E}_{x \sim p}\left[\frac{p(x)}{q(x)}\right] - 1, \\
\end{align}\end{split}\]</div>
<p>By applying Jensen’s inequality once, we can get a lower bound to the expectation above, to obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{V}_{x \sim q}\left[\frac{p(x)}{q(x)}\right] &amp;= \exp\left(\log\left(\mathbb{E}_{x \sim q}\left[\frac{p(x)^2}{q(x)^2}\right] \right)\right) - 1 \\
&amp;\geq \exp\left(\mathbb{E}_{x \sim p}\left[2 \log \frac{p(x)}{q(x)} \right]\right) - 1 \\
&amp;= e^{2 D_{KL}(q || p)} - 1,
\end{align}\end{split}\]</div>
<p>and similarly</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{V}_{x \sim q}\left[\frac{p(x)}{q(x)}\right] &amp;= \exp\left(\log\left( \mathbb{E}_{x \sim p}\left[\frac{p(x)}{q(x)}\right] \right)\right) - 1 \\
&amp;\geq \exp\left(\mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q(x)} \right]\right) - 1 \\
&amp;= e^{D_{KL}(p || q)} - 1,
\end{align}\end{split}\]</div>
<p>which is the lower bound in the lemma.</p>
</details>
<br>
<p>Note that when <span class="math notranslate nohighlight">\(q = p\)</span>, this lower bound becomes <span class="math notranslate nohighlight">\(0\)</span>. This is in agreement with the fact that when <span class="math notranslate nohighlight">\(q = p\)</span>, all importance weights are equal to <span class="math notranslate nohighlight">\(1\)</span> and their variance is <span class="math notranslate nohighlight">\(0\)</span>. As <span class="math notranslate nohighlight">\(q\)</span> becomes more and more dissimilar to <span class="math notranslate nohighlight">\(p\)</span>, <span class="math notranslate nohighlight">\(D_{KL}(q || p)\)</span> increases and so does the lower bound. Therefore the variance also increases and in fact it increases at least exponentially with <span class="math notranslate nohighlight">\(D_{KL}(q || p)\)</span>. Of course, this is only a lower bound, and the actual variance can be even larger than this. On the flipside, this lower bound tells us that if we want to reduce the variance in the importance weights we must draw our samples from a proposal distribution which is as similar as possible with the target distribution, in the sense of having a small KL divergence.</p>
<p>This is where Annealed Importance Sampling (AIS) becomes useful. AIS is an importance sampling method which uses an annealing procedure based on Markov Chain Monte Carlo (MCMC), to produce samples whose distribution is closer to <span class="math notranslate nohighlight">\(p\)</span>. To achieve this, instead of using samples from <span class="math notranslate nohighlight">\(q\)</span> directly in the importance sampling estimator, AIS gradually transforms the samples, such that the distribution of the transformed samples is closer to <span class="math notranslate nohighlight">\(p\)</span>. This reduces the variability in the importance samples and thus the random error in the importance sampling estimator.</p>
</div>
<div class="section" id="importance-weighted-mcmc">
<h2>Importance-weighted MCMC<a class="headerlink" href="#importance-weighted-mcmc" title="Permalink to this headline">¶</a></h2>
<p>Motivated by the above intuition, given some initial samples from <span class="math notranslate nohighlight">\(q\)</span>, we want to transform them in a way such that the distribution of the transformed samples is as close as possible to <span class="math notranslate nohighlight">\(p\)</span>. This would reduce the variance of the importance weights and thus the error in our estimator. Markov Chain Monte Carlo (MCMC) is a standard class of methods geared towards solving this type of problem: an MCMC algorithm begins from an arbitrarily initialised distribution, and proceeds to transform this distribution acording to a randomised rule, such that the resulting distribution is closer to a target distribution. So we could, in principle, use MCMC within an importance-weighted estimator to reduce its variance. The following algorithm is based on this intuition.</p>
<div class="definition">
<p><strong>Algorithm (An importance weighted MCMC algorithm)</strong> Given a proposal density <span class="math notranslate nohighlight">\(q\)</span>, a target density <span class="math notranslate nohighlight">\(p\)</span> and a sequence of transition kernels <span class="math notranslate nohighlight">\(T_1(x, x'), \dots, T_K(x, x')\)</span> be a sequence of transition kernels such that <span class="math notranslate nohighlight">\(T_k\)</span> leaves <span class="math notranslate nohighlight">\(p\)</span> invariant. Sampling <span class="math notranslate nohighlight">\(x_0 \sim q(x)\)</span> followed by</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
x_k \sim T_k(x_{k-1}, x_k) ~\text{ for }~ k = 1, \dots, K,
\end{equation}\]</div>
<p>and return the sample <span class="math notranslate nohighlight">\(x_K\)</span> with an appropriately chosen importance weight <span class="math notranslate nohighlight">\(w\)</span> such that the resulting estimator is unbiased.</p>
</div>
<br>
<p>Note that the only requirement on the transition kernels <span class="math notranslate nohighlight">\(T_k(x, x')\)</span> is that they leave <span class="math notranslate nohighlight">\(p\)</span> invariant, and do not need to result in an ergodic Markov Chain, which is a usual requirement in standard MCMC. The distribution which results after drawing an intial sample drawn from the proposal <span class="math notranslate nohighlight">\(q\)</span> and applying the transition kernels <span class="math notranslate nohighlight">\(T_1(x, x'), \dots, T_K(x, x')\)</span> can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
q_K(x_K) = \int q(x_0) T_1(x_0, x_1) \dots T_K(x_{K-1}, x_K) dx_0 \dots dx_{K-1},
\end{align}\]</div>
<p>and gets closer to <span class="math notranslate nohighlight">\(p\)</span> as we increase <span class="math notranslate nohighlight">\(K\)</span>. Note that this algorithm does not specify how to select <span class="math notranslate nohighlight">\(w\)</span>. However, while we could in principle draw <span class="math notranslate nohighlight">\(x_K \sim q_K\)</span> and return this sample together with the importance weight</p>
<div class="math notranslate nohighlight">
\[\begin{align}
w = \frac{p(x_K)}{q_K(x_K)},
\end{align}\]</div>
<p>each of the nested integrals above is intractable, which means we cannot compute <span class="math notranslate nohighlight">\(q_K\)</span> in closed form, and by extension we cannot compute <span class="math notranslate nohighlight">\(w\)</span> in closed form either. One way to get around this issue, is to consider the joint distribution of <span class="math notranslate nohighlight">\((x_0, x_1, \dots, x_K)\)</span> under the Markov Chain, which has density</p>
<div class="math notranslate nohighlight">
\[\begin{align}
h(x_0, x_1, \dots, x_K) = q(x_0) T_1(x_0, x_1) \dots T_K(x_{K-1}, x_K).
\end{align}\]</div>
<p>We can then define the reverse transition kernels <span class="math notranslate nohighlight">\(\tilde{T}_k\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\tilde{T}_k(x, x') = T_k(x', x)\frac{p(x')}{p(x)}.
\end{align}\]</div>
<p>Because <span class="math notranslate nohighlight">\(T_k\)</span> leaves <span class="math notranslate nohighlight">\(p\)</span> invariant, it holds that</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\int \tilde{T}_k(x, x') dx' = \int T_k(x', x)\frac{p(x')}{p(x)} dx' = \frac{1}{p(x)}\int T_k(x', x)p(x') dx' = 1,
\end{align}\]</div>
<p>so the reverse kernels integrate to <span class="math notranslate nohighlight">\(1\)</span> and are also valid transition kernels. We can now consider a reversed Markov chain which starts with the target distribution <span class="math notranslate nohighlight">\(p\)</span> as its intial distribution and applies the reverse kernels <span class="math notranslate nohighlight">\(\tilde{T}_k\)</span>. This Markov chain has corresponding joint distribution</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\tilde{h}(x_0, x_1, \dots, x_K) = p(x_K) \tilde{T}_K(x_K, x_{K-1}) \dots \tilde{T}_1(x_1, x_0).
\end{align}\]</div>
<p>Now, consider performing importance sampling in this augmented space, with <span class="math notranslate nohighlight">\(h\)</span> as the proposal and <span class="math notranslate nohighlight">\(\tilde{h}\)</span> as the target distributions. Specifically, we draw draw <span class="math notranslate nohighlight">\((x_{n, 0}, \dots, x_{n, K}) \sim h\)</span> for <span class="math notranslate nohighlight">\(n = 1, \dots, N\)</span> and compute the importance weight</p>
<div class="math notranslate nohighlight">
\[\begin{align}
w_n = \frac{\tilde{h}(x_{n, 0}, \dots, x_{n, K})}{h(x_{n, 0}, \dots, x_{n, K})}.
\end{align}\]</div>
<p>The importance weights <span class="math notranslate nohighlight">\(w_n\)</span> ensure that for any function <span class="math notranslate nohighlight">\(g\)</span> of the augmented sample, the importance weighted estimator</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\int g(x_0, \dots, x_K) \tilde{h}(x_0, \dots, x_K) dx_0 \dots dx_K \approx \frac{1}{N}\sum_{n=1}^N w_n g(x_{n, 0}, \dots, x_{n, K}),
\end{align}\]</div>
<p>is unbiased. Therefore, if we set <span class="math notranslate nohighlight">\(g(x_{n, 0}, \dots, x_{n, K}) = f(x_{n, K})\)</span>, we obtain the estimator</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\int f(x_K) p(x_K) dx_K \approx \frac{1}{N}\sum_{n=1}^N w_n g(x_{n, 0}, \dots, x_{n, K}),
\end{align}\]</div>
<p>which is also unbiased. Crucially, the importance weights <span class="math notranslate nohighlight">\(w_n\)</span> can now be computed in closed form. Using the definition of the reverse transition kernels we have</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(x_K) \tilde{T}_K(x_K, x_{K-1}) \dots \tilde{T}_1(x_1, x_0) = p(x_0) T_1(x_0, x_1) \dots T_K(x_{K-1}, x_K).
\end{align}\]</div>
<p>and all terms coming from the transition kernels cancel in the importance weight ratio, yielding</p>
<div class="math notranslate nohighlight">
\[\begin{align}
w_n = \frac{\tilde{h}(x_{n, 0}, \dots, x_{n, K})}{h(x_{n, 0}, \dots, x_{n, K})} = \frac{p(x_0)}{q(x_0)}.
\end{align}\]</div>
<p>By performing importance sampling in this augmented space, we have got around the issue of intractable importance weights, by cancelling out a load of terms. However, unfortunately these importance weight are exactly the same as the importance weights of the standard importance sampling estimator, so this algorithm does not improve on the variance of the standard estimator at all! However, it is possible to modify this algorithm to obtain better importance weights, while still taking advantage of the cancellation of the transition kernels.</p>
</div>
<div class="section" id="id2">
<h2>Annealed Importance Sampling<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>Given a sequnece <span class="math notranslate nohighlight">\(0 = \beta_0 \leq \dots \leq \beta_K = 1\)</span>, AIS introduces a sequence of distributions with densities</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\pi_k(x) = p(x)^{\beta_k} q(x)^{(1 - \beta_k)}.
\end{equation}\]</div>
<p>These distributions interpolate between <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(p\)</span> as we vary <span class="math notranslate nohighlight">\(\beta\)</span>. AIS then proceeds in a similar way to the importance weighted MCMC algorithm we highlighted above, except that it requires that each <span class="math notranslate nohighlight">\(T_k\)</span> leaves <span class="math notranslate nohighlight">\(\pi_k\)</span>, instead of <span class="math notranslate nohighlight">\(p\)</span>, invariant.</p>
<div class="definition">
<p><strong>Definition (Annealed Importance Sampling)</strong> Given a target density <span class="math notranslate nohighlight">\(p\)</span>, a proposal density <span class="math notranslate nohighlight">\(q\)</span> and a sequence <span class="math notranslate nohighlight">\(0 = \beta_0 \leq \dots \leq \beta_K = 1\)</span>, define</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\pi_k(x) = p(x)^{\beta_n} q(x)^{(1 - \beta_k)},
\end{equation}\]</div>
<p>and let <span class="math notranslate nohighlight">\(T_1(x, x'), \dots, T_K(x, x')\)</span> be a sequence of transition kernels such that <span class="math notranslate nohighlight">\(T_k\)</span> leaves <span class="math notranslate nohighlight">\(\pi_k\)</span> invariant. Annealed Impoprtance Sampling amounts to drawing <span class="math notranslate nohighlight">\(x_0 \sim \pi_0(x)\)</span> followed by</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
x_k \sim T_k(x_{k-1}, x_k) ~\text{ for }~ k = 1, \dots, K,
\end{equation}\]</div>
<p>and return the sample <span class="math notranslate nohighlight">\(x_K\)</span> together with the importance weight</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
w = \frac{\pi_1(x_1)}{\pi_0(x_1)} \dots \frac{\pi_N(x_N)}{\pi_{N-1}(x_N)}.
\end{equation}\]</div>
</div>
<br>
<p>Note that drawing samples according to this algorithm and setting <span class="math notranslate nohighlight">\(g(x_{n, 0}, \dots, x_{n, K}) = f(x_K)\)</span> still results in an unbiased estimator of <span class="math notranslate nohighlight">\(I\)</span>. This is because the augmented sample <span class="math notranslate nohighlight">\((x_{n, 0}, \dots, x_{n, K})\)</span> is distributed acccording to <span class="math notranslate nohighlight">\(h\)</span> whose marginal, after integrating out all but the last variable, is <span class="math notranslate nohighlight">\(q\)</span>. Let’s implement this and see how it performs.</p>
<div class="section" id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h3>
<p>To implement this procedure, we need to specify the transition kernels, and the sequence of annealing parameters <span class="math notranslate nohighlight">\(\beta_k\)</span>. For the transition kernel, we will use a standard <a class="reference external" href="https://en.wikipedia.org/wiki/Metropolis-Hastings_algorithm">Metropolis-Hastings</a> transition kernel, which itself uses a Gaussian distribution as its proposal distribution (not to be confused with the initial proposal distribution <span class="math notranslate nohighlight">\(p\)</span>). We’ll leave the <span class="math notranslate nohighlight">\(\beta_k\)</span> to be specified by the user.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransitionKernel</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">pass</span>

    
<span class="k">class</span> <span class="nc">GaussianTransitionKernel</span><span class="p">(</span><span class="n">TransitionKernel</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">distribution</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        
        <span class="c1"># Create forward proposal distribution and propose next point</span>
        <span class="n">forward</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">next_x</span> <span class="o">=</span> <span class="n">forward</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        
        <span class="c1"># Create reverse proposal distribution</span>
        <span class="n">reverse</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">next_x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        
        <span class="c1"># Compute acceptance probability</span>
        <span class="n">log_prob_1</span> <span class="o">=</span> <span class="n">forward</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">next_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">distribution</span><span class="p">(</span><span class="n">next_x</span><span class="p">)</span>
        <span class="n">log_prob_2</span> <span class="o">=</span> <span class="n">reverse</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">distribution</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">log_prob_ratio</span> <span class="o">=</span> <span class="n">log_prob_1</span> <span class="o">-</span> <span class="n">log_prob_2</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="n">log_prob_ratio</span><span class="p">]))</span>
        
        <span class="c1"># Accept reject step</span>
        <span class="n">accept</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span>
            <span class="p">[[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">p</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)]],</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        
        <span class="n">x_accept</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">next_x</span><span class="p">])[</span><span class="n">accept</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">x_accept</span><span class="p">,</span> <span class="n">accept</span>
</pre></div>
</div>
</div>
</div>
<p>We can then put together an <code class="docutils literal notranslate"><span class="pre">AnnealedImportanceSampler</span></code>, which accepts an initial and a target distribution, a transition kernel and a list containing a schedule for <span class="math notranslate nohighlight">\(\beta_k\)</span>, to perform AIS.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AnnealedImportanceSampler</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">initial_distribution</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
            <span class="n">target_distribution</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
            <span class="n">transition_kernel</span><span class="p">:</span> <span class="n">TransitionKernel</span><span class="p">,</span>
            <span class="n">betas</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">initial_distribution</span> <span class="o">=</span> <span class="n">initial_distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_distribution</span> <span class="o">=</span> <span class="n">target_distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transition_kernel</span> <span class="o">=</span> <span class="n">transition_kernel</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">betas</span> <span class="o">=</span> <span class="n">betas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_steps</span> <span class="o">=</span> <span class="n">betas</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        
        <span class="c1"># Draw samples from intial distribution</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">num_samples</span><span class="p">])</span>
        
        <span class="c1"># Run AIS chain on the initial samples</span>
        <span class="n">samples_and_log_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">map_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_chain</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x0</span><span class="p">,</span> <span class="n">samples_and_log_weights</span>

    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">jit_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">run_chain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        
        <span class="c1"># Initialise chain history and current distribution</span>
        <span class="n">chain_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="n">annealed_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_distribution</span><span class="o">.</span><span class="n">log_prob</span>
        
        <span class="c1"># Initialise log importance weight</span>
        <span class="n">log_w</span> <span class="o">=</span> <span class="o">-</span> <span class="n">annealed_log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_steps</span><span class="p">):</span>
            
            <span class="c1"># Create next annealed distribution</span>
            <span class="n">next_annealed_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_geometric_mixture</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">log_w</span> <span class="o">=</span> <span class="n">log_w</span> <span class="o">+</span> <span class="n">next_annealed_log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="c1"># Propose next point</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">accept</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transition_kernel</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                <span class="n">distribution</span><span class="o">=</span><span class="n">next_annealed_log_prob</span><span class="p">,</span>
            <span class="p">)</span>
            
            <span class="n">log_w</span> <span class="o">=</span> <span class="n">log_w</span> <span class="o">-</span> <span class="n">next_annealed_log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="n">annealed_log_prob</span> <span class="o">=</span> <span class="n">next_annealed_log_prob</span>
            <span class="n">chain_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
        <span class="n">log_w</span> <span class="o">=</span> <span class="n">log_w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">log_w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
            
        
    <span class="k">def</span> <span class="nf">log_geometric_mixture</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
        
        <span class="k">def</span> <span class="nf">_log_geometric_mixture</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
            
            <span class="n">log_prob_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">log_prob_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="p">((</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">log_prob_1</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">log_prob_2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">_log_geometric_mixture</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="toy-experiment">
<h3>Toy experiment<a class="headerlink" href="#toy-experiment" title="Permalink to this headline">¶</a></h3>
<p>Now we have an AIS sampler which can be used with arbitrary annealing parameters <span class="math notranslate nohighlight">\(\beta_k\)</span>. For this example, let’s set</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\beta_k = \frac{1}{1 + e^{\gamma_k}},
\end{equation}\]</div>
<p>where we start from large and negative values to large and positive values. This gives <span class="math notranslate nohighlight">\(\beta_k\)</span> which gradually interpolate from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>

<span class="n">transition_scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Intialise transition kernel</span>
<span class="n">transition_kernel</span> <span class="o">=</span> <span class="n">GaussianTransitionKernel</span><span class="p">(</span>
    <span class="n">scale</span><span class="o">=</span><span class="n">transition_scale</span>
<span class="p">)</span>

<span class="c1"># Initialise betas</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="mf">10.</span> <span class="o">*</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">))</span>

<span class="c1"># Initialise AIS sampler</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">AnnealedImportanceSampler</span><span class="p">(</span>
    <span class="n">initial_distribution</span><span class="o">=</span><span class="n">q</span><span class="p">,</span>
    <span class="n">target_distribution</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
    <span class="n">transition_kernel</span><span class="o">=</span><span class="n">transition_kernel</span><span class="p">,</span>
    <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s draw some samples on the same problem we considered earlier, and visualise them as before.</p>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the random seed for repeatable results</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># How many samples to draw</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Draw samples and compute importance weights</span>
<span class="n">x0</span><span class="p">,</span> <span class="n">samples_and_log_weights</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">samples_and_log_weights</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">log_w</span> <span class="o">=</span> <span class="n">samples_and_log_weights</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_w</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot proposal</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">p_plot</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">)</span>

<span class="c1"># Plot target</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">q_plot</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:green&quot;</span><span class="p">)</span>

<span class="c1"># Plot initial samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">x0</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:green&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Initial samples from $q$&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Plot samples after samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:purple&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;AIS samples&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Plot samples weighted by their importance weights</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="n">w</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Weighted AIS samples&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;AIS samples&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$p(x),~ q(x)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">twin_axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">f_plot</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$f(x)$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$f(x)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">,</span> <span class="mf">60.</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/ais_23_0.svg" src="../../../_images/ais_23_0.svg" /></div>
</div>
<p>We observe that the distribution of the AIS samples (purple bars) is similar to the target distribution <span class="math notranslate nohighlight">\(p\)</span> (blue line). Further, weighing the AIS samples by their importance weights (blue bars) does not appear to significantly change the resulting distribution. This would suggest that moost of the importance weights are close to <span class="math notranslate nohighlight">\(1\)</span>, which we can verify by plotting them on a histogram.</p>
<div class="cell tag_center-output tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../../_images/ais_25_0.svg" src="../../../_images/ais_25_0.svg" /></div>
</div>
<p>As expected, most of the AIS importance weights are close to <span class="math notranslate nohighlight">\(1\)</span>. Further, they have a clearly smaller variance than standard importance sampling. This has a significant impact on the variance of AIS estimates. In the running example integral we’ve been using thus far, AIS reduces the variance of our Monte Carlo estimate by a factor of <span class="math notranslate nohighlight">\(\approx 6\)</span> over standard importance sampling, and gets close to the variance of an estimator that uses direct i.i.d. samples from <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># How many samples to draw</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">100</span>
    
<span class="c1"># Draw samples</span>
<span class="n">samples_and_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">sampler</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">)]</span>
<span class="n">samples_and_log_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">samples_and_log_weights</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">samples_and_log_weights</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">log_weights</span> <span class="o">=</span> <span class="n">samples_and_log_weights</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>

<span class="c1"># Compute importance weights and importance weighted integral</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">f</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">mean_I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
<span class="n">std_I</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Annealed Importance Sampling over </span><span class="si">{</span><span class="n">num_trials</span><span class="si">}</span><span class="s2"> trials &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;I = </span><span class="si">{</span><span class="n">mean_I</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> +/- </span><span class="si">{</span><span class="mf">2.</span><span class="o">*</span><span class="n">std_I</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">.&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Annealed Importance Sampling over 100 trials I = 0.30 +/- 2.218.
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>In importance sampling, using a proposal distribution that is significantly different from the target distribution results in a large variance of the resulting importance weights. Large variance in the importance weights typically induces large variance in downstream estimates obtained using these weights. AIS is a method which helps address this issue. AIS introduces a sequence of annealed distributions, which interpolate between the target and proposal distribution, iteratively transforming samples from the proposal distribution using a sequence of Markov transition kernels which preserve the interpolating (annealed) distributions. In this way, AIS typically reduces the variance in the importance weights, which in turn reduces the variance in downstream Monte Carlo estimates. For more details, Radford Neal’s original paper introducing AIS<a class="bibtex reference internal" href="#neal2001annealed" id="id3">[Nea01]</a> is a classic worth reading.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-content/misc/ais/ais-0"><dl class="citation">
<dt class="bibtex label" id="neal2001annealed"><span class="brackets">Nea01</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>Radford M Neal. Annealed importance sampling. <em>Statistics and computing</em>, 11:125–139, 2001.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "venv-random-walks"
        },
        kernelOptions: {
            kernelName: "venv-random-walks",
            path: "./content/misc/ais"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'venv-random-walks'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../gip/gip.html" title="previous page">Global inducing points for BNNs</a>
    <a class='right-next' id="next-link" href="../../reading-and-links.html" title="next page">Interesting reading and websites</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratos Markou<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>