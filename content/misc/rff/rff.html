
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Random Fourier features &#8212; Random walks</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://random-walks.org/content/misc/rff/rff.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Stein variational gradient descent" href="../svgd/svgd.html" />
    <link rel="prev" title="Interacting particle solutions of the FPK" href="../interacting/interacting.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://random-walks.org/content/misc/rff/rff.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Random Fourier features" />
<meta property="og:description" content="Random Fourier features  One central difficulty with Gaussian Processes (GPs), and more generally all kernel methods such as Support Vector Machines (SVMs), is " />
<meta property="og:image"       content="https://random-walks.org/_static/logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Random walks</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../home.html">
   Random walks
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../prob-intro/intro.html">
   Probability: An introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch01/content.html">
     Events and Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch02/content.html">
     Discrete random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch03/content.html">
     Multivariate discrete distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch04/content.html">
     Probability generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch05/content.html">
     Distribution and density functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch06/content.html">
     Multivariate distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch07/content.html">
     Moment generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch08/content.html">
     Main limit theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch09/content.html">
     Branching processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch10/content.html">
     Random walks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch11/content.html">
     Processes in continuous time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch12/content.html">
     Markov chains
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../cvx/intro.html">
   Convex optimisation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch01.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch02.html">
     Convex sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch03.html">
     Convex functions
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../gp/gp-intro.html">
   Gaussian Processes
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../gp/why-covariances.html">
     Why covariance functions?
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../gp/sparse/sparse-intro.html">
     Sparse Gaussian Processes
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/vfe.html">
       Variational Free Energy GPs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/gp-sampling.html">
       Sampling GP posteriors
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../gp/vfer/vfer.html">
     Variational Inference Revisited
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../misc.html">
   Miscellaneous
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../sde/num-sde.html">
     Numerical simulation of SDEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sde-as-gp/sde-as-gp.html">
     VI for diffusion processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../node/node.html">
     Neural ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../optimisation/conjugate-gradients.html">
     Conjugate gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../kalman/kalman.html">
     The Kalman filter and smoother
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ncs/ncs.html">
     Natural cubic splines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ars/ars.html">
     Adaptive rejection sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../score-matching/score-matching.html">
     Estimation by score matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interacting/interacting.html">
     Interacting particle FPK
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Random Fourier features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../svgd/svgd.html">
     Stein variational gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../addgp/addgp.html">
     Additive Gaussian Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gumbel/gumbel.html">
     Gumbel distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gip/gip.html">
     Global inducing points for BNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ais/ais.html">
     Annealed Importance Sampling
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reading-and-links.html">
   Interesting reading and websites
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/misc/rff/rff.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/stratisMarkou/random-walks/master?urlpath=tree/./content/misc/rff/rff.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-rff-approximation">
   The RFF approximation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rff-and-bayesian-regression">
     RFF and Bayesian regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rates-of-convergence">
     Rates of convergence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampling-from-the-prior">
     Sampling from the prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-with-rff-features">
     Regression with RFF features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance-starvation">
     Variance starvation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="random-fourier-features">
<h1>Random Fourier features<a class="headerlink" href="#random-fourier-features" title="Permalink to this headline">¶</a></h1>
<p>One central difficulty with Gaussian Processes (GPs), and more generally all kernel methods such as Support Vector Machines (SVMs), is their computational cost. Exact GP regression scales cubically <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> in the number of datapoints <span class="math notranslate nohighlight">\(N\)</span>, which is prohibitive for even modestly large datasets. Therefore we typically have to make approximations, of which there is a wealth of possible options. This page presents Random Fourier Features (RFF), <a class="bibtex reference internal" href="#rahimi2007random" id="id1">[RR+07]</a> an approximation which is applicable to stationary kernels.</p>
<p>RFF relies on the fact that kernels of stationary processes can be expressed as the Fourier transform of a probability density function. This Fourier transform can then be efficiently approximated by Monte Carlo. The samples drawn to approximate this integral are called Random Fourier Features and can be used as features for a linear-in-the-parameters Bayesian regression model, which approximates exact GP regression. The computational cost of Bayesian linear regression is <span class="math notranslate nohighlight">\(\mathcal{O}(\min\{N^3, M^3\})\)</span> where <span class="math notranslate nohighlight">\(M\)</span> is the number of regression features, so by using <span class="math notranslate nohighlight">\(M &lt; N\)</span> we can significantly reduce the cost of the algorithm. Further, one can prove that the covariance function induced by RFF closely approximates that of the exact GP with a relatively small number of samples. Lastly, RFF is rather easy to implement in practice.</p>
<div class="section" id="the-rff-approximation">
<h2>The RFF approximation<a class="headerlink" href="#the-rff-approximation" title="Permalink to this headline">¶</a></h2>
<p>The starting point for deriving RFF is Bochner’s theorem, which relates stationary kernels with probability distributions over frequencies via the Fourier transform.</p>
<div class="theorem">
<p><strong>Theorem (Bochner’s theorem)</strong> A continuous function of the form <span class="math notranslate nohighlight">\(k(x, y) = k(x - y)\)</span> is positive definite if and only if <span class="math notranslate nohighlight">\(k(\delta)\)</span> is the Fourier transform of a non-negative measure.</p>
</div>
<br>
<p>Note that this statement slightly abuses the <span class="math notranslate nohighlight">\(k\)</span> symbol, using it to denote both the kernel <span class="math notranslate nohighlight">\(k(x, y)\)</span> as well as its writing in an explicitly translation-invariant form <span class="math notranslate nohighlight">\(k(x - y)\)</span> - the implied use is clear from context. From Bochner’s theorem, we see that we can express any positive definite measure as the Fourier transform of a probability measure, rather than simply a non-negative measure, by introducing an appropriate scaling constant <span class="math notranslate nohighlight">\(\sigma^2\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
k(x - y) = \sigma^2 \int p(\omega) e^{-i \omega^\top (x - y)} d\omega = \sigma^2 \mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right].
\end{align}\]</div>
<p>Thus, we can get an unbiased estimate of <span class="math notranslate nohighlight">\(k(x - y)\)</span> by sampling <span class="math notranslate nohighlight">\(\omega \sim p(\omega)\)</span> and computing <span class="math notranslate nohighlight">\(\zeta_{\omega}(x)\zeta_{\omega}^*(y)\)</span>. Note however, that even though <span class="math notranslate nohighlight">\(\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right]\)</span> is real, the sampled <span class="math notranslate nohighlight">\(\zeta_{\omega}(x)\zeta_{\omega}^*(y)\)</span> will in general be complex. This is an issue if we want to use <span class="math notranslate nohighlight">\(\zeta_{\omega}\)</span> to represent real functions. To resolve this issue, we can write the integral as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right] &amp;= \text{Re}\left[\mathbb{E}_{\omega}\left[e^{-i \omega^\top (x - y)}\right]\right] \\
                                                                     &amp;= \mathbb{E}_{\omega}\left[\text{Re}\left[e^{-i \omega^\top (x - y)}\right]\right] \\
                                                                     &amp;= \mathbb{E}_{\omega}\left[\cos(\omega^\top (x - y))\right],
\end{align}\end{split}\]</div>
<p>which has a real-valued integrand, so a Monte Carlo estimate of it will always be real valued. Next, we would like to manipulate the expression above into an expectation of the form <span class="math notranslate nohighlight">\(\mathbb{E}_{\omega}\left[r_{\omega}(x)r_{\omega}(y)\right]\)</span> rather than <span class="math notranslate nohighlight">\(\mathbb{E}\left[r_{\omega}(x - y)\right]\)</span>. In this way, we will be able to interpret this expectation as the covariance of a Bayesian regression model - this will be clarified shortly. Using the fact that</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}_{\phi}\left[\cos(z + n\phi)\right] = 0,
\end{align}\]</div>
<p>for all <span class="math notranslate nohighlight">\(z \in \mathbb{R}, n \in \mathbb{N}^+\)</span>, where <span class="math notranslate nohighlight">\(\phi \sim \text{Uniform}[0, 2\pi]\)</span>, we can re-write the expectation as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right] &amp;= \mathbb{E}_{\omega, \phi}\left[\cos\left(\omega^\top (x - y)\right) + \cos\left(\omega^\top (x + y\right) + 2b)\right] \\
                                                                     &amp;= \mathbb{E}_{\omega, \phi}\left[2 \cos\left(\omega^\top x + b\right) \cos\left(\omega^\top y + b\right)\right].
\end{align}\end{split}\]</div>
<p>We can therefore get an unbiased, real valued estimate of <span class="math notranslate nohighlight">\(k(x - y)\)</span> by sampling <span class="math notranslate nohighlight">\(\omega, \phi\)</span> and computing</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right] \approx z_{\omega, b}(x) z_{\omega, b}(y), \text{ where } z_{\omega, b}(x) = \sqrt{2} \cos(\omega^\top x + b).
\end{align}\]</div>
<p>In fact, we can go a bit further by drawing <span class="math notranslate nohighlight">\(M\)</span> independent pairs of <span class="math notranslate nohighlight">\(\omega, b\)</span> and computing the estimate</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}_{\omega}\left[\zeta_{\omega}(x)\zeta_{\omega}^*(y)\right] \approx \frac{1}{M} \sum_{m = 1}^M z_{\omega_m, \phi_m}(x) z_{\omega_m, \phi_m}(y).
\end{align}\]</div>
<p>This is also an unbiased estimate of the kernel, however its variance is lower than in the <span class="math notranslate nohighlight">\(M = 1\)</span> case, since the variance of the average of the sum of <span class="math notranslate nohighlight">\(K\)</span> i.i.d. random variables is lower than the variance of a single one of the variables.
We therefore arrive at the following algorithm for estimating <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="definition">
<p><strong>Algorithm (Random Fourier Features)</strong> Given a translation invariant kernel <span class="math notranslate nohighlight">\(k\)</span> that is the Fourier transform of a probability measure <span class="math notranslate nohighlight">\(p\)</span>, we have the unbiased real-valued estimator</p>
<div class="math notranslate nohighlight">
\[\begin{align}
k(x - y) \approx \frac{1}{M} \sum_{m = 1}^M z_{\omega_m, \phi_m}(x) z_{\omega_m, \phi_m}(y) = z^\top(x)z(y),
\end{align}\]</div>
<p>where we have used the notation <span class="math notranslate nohighlight">\(z(x) = \left[ z_{\omega_1, \phi_1}(x), ..., z_{\omega_M, \phi_M}(x) \right]^\top\)</span> and <span class="math notranslate nohighlight">\(\omega_m \sim p(\omega), \phi_m \sim \text{Uniform}[0, 2\pi]\)</span> are independent and identically distributed samples.</p>
</div>
<br>
<div class="section" id="rff-and-bayesian-regression">
<h3>RFF and Bayesian regression<a class="headerlink" href="#rff-and-bayesian-regression" title="Permalink to this headline">¶</a></h3>
<p>Now we are in a position to interpret this expression in terms of a Bayesian regression model. Consider the regressor</p>
<div class="math notranslate nohighlight">
\[\begin{align}
f(x) = z^\top(x)w, \text{ where } w \sim \mathcal{N}(0, I),
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(z(x)\)</span> is as defined above. This is a Bayesian regression model which uses Random Fourier Features. Therefore we can approximate the exact GP regressor by a Bayesian linear regressor with Random Fourier Features.</p>
</div>
<div class="section" id="rates-of-convergence">
<h3>Rates of convergence<a class="headerlink" href="#rates-of-convergence" title="Permalink to this headline">¶</a></h3>
<p>Now there remains the question of how large the error of the RFF estimator is. In other words, how closely does RFF estimate the exact kernel <span class="math notranslate nohighlight">\(k\)</span>? Since <span class="math notranslate nohighlight">\(-\sqrt{2} \leq z_{\omega, \phi} \leq \sqrt{2}\)</span>, we can use Hoeffding’s inequality<a class="bibtex reference internal" href="#grimmett2020probability" id="id2">[Gri20]</a> to obtain the following high-probability bound on the absolute error on our estimate of <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="lemma">
<p><strong>Result (Hoeffding for RFF)</strong> The RFF estimator of <span class="math notranslate nohighlight">\(k\)</span>, using <span class="math notranslate nohighlight">\(M\)</span> pairs of <span class="math notranslate nohighlight">\(\omega, \phi\)</span>, obeys</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p\big(|z^\top(x)z(y) - k(x, y)| \geq \epsilon \big) \leq 2 \exp\left(-M \frac{\epsilon^2}{4}\right).
\end{align}\]</div>
</div>
<br>
<p>Therefore for any given input pair, the error of the RFF estimator decays exponentially quickly with <span class="math notranslate nohighlight">\(M\)</span>. Note that this is a statement about the closeness of <span class="math notranslate nohighlight">\(z^\top(x)z(y)\)</span> and <span class="math notranslate nohighlight">\(k(x, y)\)</span> for any two input pairs, rather than the closeness of these functions over the whole input space. In fact, it is possible<a class="bibtex reference internal" href="#rahimi2007random" id="id3">[RR+07]</a> to make a stronger statement about the uniform convergence of the estimator.</p>
<div class="lemma">
<p><strong>Result (Uniform convergence of RFF)</strong> Let <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> be a compact subset of <span class="math notranslate nohighlight">\(\mathbb{R}^D\)</span>. Then the RFF estimator of <span class="math notranslate nohighlight">\(k\)</span>, using <span class="math notranslate nohighlight">\(M\)</span> pairs of <span class="math notranslate nohighlight">\(\omega, \phi\)</span> converges uniformly to <span class="math notranslate nohighlight">\(k\)</span> according to</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p~\Bigg(\sup_{x, y \in \mathcal{M}}|z^\top(x)z(y) - k(x, y)| \geq \epsilon \Bigg) \leq \frac{C}{\epsilon^2} \exp\left(-\frac{M\epsilon^2}{4(d + 2)}\right).
\end{align}\]</div>
</div>
<br></div>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>We now turn to an implementation of RFF. We consider approximating the same three kernels which were presented in the RFF paper, namely the EQ (or Gaussian) kernel, the Laplace kernel and the Cauchy kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample_rff</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="n">num_functions</span><span class="p">,</span> <span class="n">num_features</span><span class="p">):</span>
    
    <span class="c1"># Dimension of data space</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">omega_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_functions</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">)</span>
    
    <span class="c1"># Handle each of three possible kernels separately</span>
    <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;eq&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">omega_shape</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;laplace&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_cauchy</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">omega_shape</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;cauchy&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">laplace</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">omega_shape</span><span class="p">)</span>
        
    <span class="c1"># Scale omegas by lengthscale -- same operation for all three kernels</span>
    <span class="n">omega</span> <span class="o">=</span> <span class="n">omega</span> <span class="o">/</span> <span class="n">lengthscale</span>
    
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                               <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                               <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_functions</span><span class="p">,</span> <span class="n">num_features</span><span class="p">))</span>
    
    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                            <span class="n">high</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">),</span>
                            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_functions</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sfd, nd -&gt; sfn&#39;</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">phi</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">num_features</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">features</span> <span class="o">*</span> <span class="n">coefficient</span>
    
    <span class="n">functions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;sf, sfn -&gt; sn&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">functions</span><span class="p">,</span> <span class="n">features</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="sampling-from-the-prior">
<h3>Sampling from the prior<a class="headerlink" href="#sampling-from-the-prior" title="Permalink to this headline">¶</a></h3>
<p>We can now visualise functions sampled from the prior induced by RFF. Each function corresponds to a different sample set <span class="math notranslate nohighlight">\(\{(\omega_m, \phi_m)\}_{m = 1}^M\)</span> as well as a different weight vector <span class="math notranslate nohighlight">\(w\)</span>. The covariance function plots correspond to the RFF estimator of <span class="math notranslate nohighlight">\(k\)</span>, using a single sample of RFF parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Kernels to approximate</span>
<span class="n">kernels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;eq&#39;</span><span class="p">,</span> <span class="s1">&#39;laplace&#39;</span><span class="p">,</span> <span class="s1">&#39;cauchy&#39;</span><span class="p">]</span>

<span class="c1"># Kernel parameters, # of functions to sample, # of features for each function</span>
<span class="n">lengthscale</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">coefficient</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">num_functions</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Input locations</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># Draw random fourier features and functions</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">sample_rff</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> 
                      <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> 
                      <span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">,</span> 
                      <span class="n">coefficient</span><span class="o">=</span><span class="n">coefficient</span><span class="p">,</span> 
                      <span class="n">num_functions</span><span class="o">=</span><span class="n">num_functions</span><span class="p">,</span> 
                      <span class="n">num_features</span><span class="o">=</span><span class="n">num_features</span><span class="p">)</span>
           <span class="k">for</span> <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Titles for sample plots</span>
<span class="n">sample_titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;EQ RFF samples&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;Laplace RFF samples&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;Cauchy RFF samples&#39;</span><span class="p">]</span>

<span class="c1"># Titles for kernel plots</span>
<span class="n">kernel_titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;EQ RFF covariance&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;Laplace RFF covariance&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;Cauchy RFF covariance&#39;</span><span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;pink&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">]</span>

<span class="c1"># Plot samples and linear model covariances</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>

    <span class="n">functions</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="c1"># Figure to plot on</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Plot sampled functions</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">functions</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    
    <span class="c1"># Format plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$f$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">sample_titles</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">19</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">4</span><span class="p">)</span>

    <span class="c1"># Plot covariances</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij, ik -&gt; jk&#39;</span><span class="p">,</span> <span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;coolwarm&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    
    <span class="c1"># Format plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$x&#39;$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">kernel_titles</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/rff_7_0.svg" src="../../../_images/rff_7_0.svg" /></div>
</div>
</div>
<div class="section" id="regression-with-rff-features">
<h3>Regression with RFF features<a class="headerlink" href="#regression-with-rff-features" title="Permalink to this headline">¶</a></h3>
<p>First we generate a toy dataset from a noisy sinusoid. We place a gap in the data to observe the quality of the uncertainty estimates of the RFF regressor. We purposefully use a large number of datapoints, <span class="math notranslate nohighlight">\(N = 5000\)</span>, which would be quite slow to process with an exact GP model.</p>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of datapoints to generate</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="mi">4000</span>

<span class="c1"># Generate sinusoidal data with a gap in input space</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">num_data</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_data</span><span class="p">[:(</span><span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">4</span><span class="p">)],</span>
                         <span class="n">x_data</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">4</span><span class="p">):]],</span>
                        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        
<span class="c1"># Plot data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    
<span class="c1"># Plot data points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Format plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Toy sinusoidal data&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">x_data</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">x_data</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/rff_9_0.svg" src="../../../_images/rff_9_0.svg" /></div>
</div>
<p>The function <code class="docutils literal notranslate"><span class="pre">rff_posterior</span></code> below implements Bayesian linear regression with randomly sampled Fourier features. For more details on Bayesian linear regression see Chapter 3 of Bishop’s PRML book.<a class="bibtex reference internal" href="#bishop2006pattern" id="id4">[Bis06]</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rff_posterior</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
    
    <span class="n">num_data</span> <span class="o">=</span> <span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">x_data</span><span class="p">])</span>
    
    <span class="n">_</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">sample_rff</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_full</span><span class="p">,</span> 
                             <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> 
                             <span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">,</span> 
                             <span class="n">coefficient</span><span class="o">=</span><span class="n">coefficient</span><span class="p">,</span> 
                             <span class="n">num_functions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                             <span class="n">num_features</span><span class="o">=</span><span class="n">num_features</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">features_pred</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="n">num_data</span><span class="p">]</span>
    <span class="n">features_data</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span> <span class="o">-</span><span class="n">num_data</span><span class="p">:]</span>
    
    <span class="n">iS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">features_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">features_data</span> <span class="o">@</span> <span class="n">features_data</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">noise</span> <span class="o">**</span> <span class="o">-</span><span class="mi">2</span>

    <span class="n">mean_pred</span> <span class="o">=</span> <span class="n">noise</span> <span class="o">**</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">features_pred</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">iS</span><span class="p">,</span> <span class="n">features_data</span> <span class="o">@</span> <span class="n">y_data</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
    
    <span class="n">var_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;fn, fn -&gt; n&#39;</span><span class="p">,</span>
                         <span class="n">features_pred</span><span class="p">,</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">iS</span><span class="p">,</span> <span class="n">features_pred</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">mean_pred</span><span class="p">,</span> <span class="n">var_pred</span>
</pre></div>
</div>
</div>
</div>
<p>Now we perform Bayesian linear regression with RFF features for each of the three kernels. The two helpers below, implement exact GP regression. The <code class="docutils literal notranslate"><span class="pre">covariance</span></code> helper implements the EQ, Laplace and Cauchy covariance functions and the <code class="docutils literal notranslate"><span class="pre">exact_gp_posterior</span></code> helper performs exact GP regression. <a class="bibtex reference internal" href="#rasmussen2003gaussian" id="id5">[Ras03]</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">covariance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    
    <span class="n">diff</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">x_</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
    
    <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;eq&#39;</span><span class="p">:</span>
        <span class="n">l2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">diff</span> <span class="o">/</span> <span class="n">lengthscale</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">coefficient</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">l2</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;laplace&#39;</span><span class="p">:</span>
        <span class="n">l1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span> <span class="o">/</span> <span class="n">lengthscale</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">coefficient</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">l1</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;cauchy&#39;</span><span class="p">:</span>
        <span class="n">l2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">diff</span> <span class="o">/</span> <span class="n">lengthscale</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">coefficient</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">l2</span><span class="p">)</span>
        
    <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">cov</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">cov</span>


<span class="k">def</span> <span class="nf">exact_gp_posterior</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
    
    <span class="n">Kdd</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
    <span class="n">Kpd</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">Kpp</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">lengthscale</span><span class="p">,</span> <span class="n">coefficient</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    
    <span class="n">mean</span> <span class="o">=</span> <span class="n">Kpd</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Kdd</span><span class="p">,</span> <span class="n">y_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Kpp</span> <span class="o">-</span> <span class="n">Kpd</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Kdd</span><span class="p">,</span> <span class="n">Kpd</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span>
</pre></div>
</div>
</div>
</div>
<p>Timing the computation shows that the RFF implementation takes miliseconds to execute, as opposed to exact GP regression which takes much longer.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timing exact GP regression:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="o">%</span><span class="k">time</span> exact_posteriors = [exact_gp_posterior(x_data,      \
                                             <span class="n">y_data</span><span class="p">,</span>      \
                                             <span class="n">x_pred</span><span class="p">,</span>      \
                                             <span class="n">kernel</span><span class="p">,</span>      \
                                             <span class="n">lengthscale</span><span class="p">,</span> \
                                             <span class="n">coefficient</span><span class="p">,</span> \
                                             <span class="n">noise</span><span class="p">)</span>       \
                          <span class="k">for</span> <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Timing exact GP regression:

CPU times: user 11.6 s, sys: 988 ms, total: 12.5 s
Wall time: 4.55 s
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Timing exact approximate RFF regression:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="o">%</span><span class="k">time</span> approx_posteriors = [rff_posterior(x_data,       \
                                         <span class="n">y_data</span><span class="p">,</span>       \
                                         <span class="n">x_pred</span><span class="p">,</span>       \
                                         <span class="n">kernel</span><span class="p">,</span>       \
                                         <span class="n">lengthscale</span><span class="p">,</span>  \
                                         <span class="n">coefficient</span><span class="p">,</span>  \
                                         <span class="n">num_features</span><span class="p">,</span> \
                                         <span class="n">noise</span><span class="p">)</span>        \
                           <span class="k">for</span> <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Timing exact approximate RFF regression:

CPU times: user 295 ms, sys: 90.2 ms, total: 385 ms
Wall time: 99.8 ms
</pre></div>
</div>
</div>
</div>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Titles for sample plots</span>
<span class="n">posterior_titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;EQ RFF posterior&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;Laplace RFF posterior&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;Cauchy RFF posterior&#39;</span><span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;pink&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>

    <span class="n">approx_mean</span><span class="p">,</span> <span class="n">approx_var</span> <span class="o">=</span> <span class="n">approx_posteriors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">exact_mean</span><span class="p">,</span> <span class="n">exact_var</span> <span class="o">=</span> <span class="n">exact_posteriors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Plot approximate posterior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">approx_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                     <span class="n">approx_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">approx_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">approx_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">approx_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    
    <span class="c1"># Plot approximate posterior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">exact_mean</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">exact_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">exact_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">exact_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">exact_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    
    <span class="c1"># Plot data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    
    <span class="c1"># Format plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">posterior_titles</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">x_pred</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x_pred</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/rff_18_0.svg" src="../../../_images/rff_18_0.svg" /></div>
</div>
<p>RFF has produced sensible regressors in each case, significantly faster than exact GP regression. The approximate posteriors roughly match the exact posteriors, while being significantly quicker to compute.</p>
</div>
<div class="section" id="variance-starvation">
<h3>Variance starvation<a class="headerlink" href="#variance-starvation" title="Permalink to this headline">¶</a></h3>
<p>One issue with the RFF is that - like all other finte basis function models - is that it has a limited amount of degrees of freedom. Therefore, in some situations the datapoints may be such that they pin down the RFF model and significantly reducing the variance of the approximate regressor. To illustrate this, we sample a slightly different dataset, with a smaller gap in between the two data clumps.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of datapoints to generate</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="mi">3000</span>

<span class="c1"># Generate sinusoidal data with a gap in input space</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">num_data</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_data</span><span class="p">[:(</span><span class="mi">2</span><span class="o">*</span><span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">5</span><span class="p">)],</span>
                         <span class="n">x_data</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">5</span><span class="p">):]],</span>
                        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        
<span class="c1"># Plot data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    
<span class="c1"># Plot data points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Format plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Toy sinusoidal data (second dataset)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">x_data</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">x_data</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/rff_20_0.svg" src="../../../_images/rff_20_0.svg" /></div>
</div>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Titles for sample plots</span>
<span class="n">posterior_titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;EQ RFF posterior&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;Laplace RFF posterior&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;Cauchy RFF posterior&#39;</span><span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;pink&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>

    <span class="n">approx_mean</span><span class="p">,</span> <span class="n">approx_var</span> <span class="o">=</span> <span class="n">approx_posteriors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">exact_mean</span><span class="p">,</span> <span class="n">exact_var</span> <span class="o">=</span> <span class="n">exact_posteriors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Plot approximate posterior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">approx_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                     <span class="n">approx_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">approx_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">approx_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">approx_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    
    <span class="c1"># Plot approximate posterior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">exact_mean</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">exact_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">exact_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">exact_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">exact_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    
    <span class="c1"># Plot data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    
    <span class="c1"># Format plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">posterior_titles</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">x_pred</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x_pred</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/rff_22_0.svg" src="../../../_images/rff_22_0.svg" /></div>
</div>
<p>The variance of the approximate estimator in between the data is (in some cases), signiticantly smaller than that of the exact posterior. So the speedup that the RFF gives does not come without a cost. In certain cases, we can end up with approximate posteriors which are significantly overfitted. This can be alleviated by increasing the number of RFF features. However this increases the computational cost of performing regression and may defeat the purpose of using RFF features in the first place.</p>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>Random Fourier Features are a cheap and efficient way to sample from a Gaussian Process (GP) prior. By using a finite number of features <span class="math notranslate nohighlight">\(M\)</span>, smaller than the number of datapoints <span class="math notranslate nohighlight">\(N\)</span>, allows us to perform approximate GP regression, reducing the computational complexity from <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(NM^2)\)</span>, where <span class="math notranslate nohighlight">\(M\)</span> is the number of random features. Some interesting recent work<a class="bibtex reference internal" href="#wilson2020efficiently" id="id6">[WBT+20]</a> combines RFF with sparse GP approximations to produce approximate GP regressors which are both cheap to train as well as to sample form their posteriors.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-content/misc/rff/rff-0"><dl class="citation">
<dt class="bibtex label" id="bishop2006pattern"><span class="brackets"><a class="fn-backref" href="#id4">Bis06</a></span></dt>
<dd><p>Christopher M Bishop. <em>Pattern recognition and machine learning</em>. springer, 2006.</p>
</dd>
<dt class="bibtex label" id="grimmett2020probability"><span class="brackets"><a class="fn-backref" href="#id2">Gri20</a></span></dt>
<dd><p>David Grimmett, Geoffrey Stirzaker. <em>Probability and random processes</em>. Oxford university press, 2020.</p>
</dd>
<dt class="bibtex label" id="rahimi2007random"><span class="brackets">RR+07</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>Ali Rahimi, Benjamin Recht, and others. Random features for large-scale kernel machines. In <em>NIPS</em>. 2007.</p>
</dd>
<dt class="bibtex label" id="rasmussen2003gaussian"><span class="brackets"><a class="fn-backref" href="#id5">Ras03</a></span></dt>
<dd><p>Carl Edward Rasmussen. Gaussian processes in machine learning. In <em>Summer school on machine learning</em>, 63–71. Springer, 2003.</p>
</dd>
<dt class="bibtex label" id="wilson2020efficiently"><span class="brackets"><a class="fn-backref" href="#id6">WBT+20</a></span></dt>
<dd><p>James T. Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Peter Deisenroth. Efficiently sampling functions from gaussian process posteriors. 2020.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "venv-random-walks"
        },
        kernelOptions: {
            kernelName: "venv-random-walks",
            path: "./content/misc/rff"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'venv-random-walks'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../interacting/interacting.html" title="previous page">Interacting particle solutions of the FPK</a>
    <a class='right-next' id="next-link" href="../svgd/svgd.html" title="next page">Stein variational gradient descent</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratos Markou<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>