{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global inducing points for BNNs\n",
    "\n",
    "One central challenge with Bayesian Neural Networks (BNNs) is handling the posteriors over their parameters. In Variational Inference (VI), we approximate the exact posterior using another tractable distibution, which can be used for making predictions. Unfortunately, common choices such as factored approximate posteriors {cite}`blundell2015weight` are typically a poor approximation to the true posterior. For example, a usual choice for the approximate posterior is a factored Gaussian, which typically leads to underfitting, and poor estimates of the predictive uncertainty. {cite}`foong2019between` Recently, Ober and Aitchison {cite}`ober2021global` introduced an approximate posterior which goes beyond the factored approximation and yields improved results. Their approximation is based on inducing points, an idea which is common in the Gaussian Process (GP) literature. In particular, this approximate posterior can be used for Deep GPs (DGPs) as well as BNNs, highlighting the similarities between these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from check_shape import check_shape\n",
    "\n",
    "tfk = tf.keras\n",
    "tfd = tfp.distributions\n",
    "\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior model\n",
    "\n",
    "Suppose we wish to perform a regression task, to learn a map from inputs $\\mathbf{X} \\in \\mathbb{R}^{K \\times D_x}$ to the corresponding outputs $\\mathbf{Y} \\in \\mathbb{R}^{K \\times D_y}$. Consider a fully connected neural network for regression, made up of $L$ hidden layers, each containing $N_1, \\dots, N_L$ hidden units, followed by by a last linear layer which maps from the $N_L$ units to the dimension of the data $D_y$. Let the weights in each layer be\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{W} = \\{\\mathbf{W}_l \\in \\mathbb{R}^{N_l \\times N_{l+1}} \\}_{l = 1}^{L+1},\n",
    "\\end{align}$$\n",
    "\n",
    "where $N_0 = D_x$ and $N_{L+1} = D_y$. Thus the network takes the form\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{F}_1 &= \\mathbf{X} \\mathbf{W}_1, \\\\\n",
    "\\mathbf{F}_l &= \\phi(\\mathbf{F}_{l - 1}) \\mathbf{W}_l,\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\phi$ is a nonlinearity. Note that under this notation, the weights post-multiply the activations rather than pre-multiplying them. Now suppose we place a prior $p(\\mathbf{W})$ over the weights, together with a Gaussian likelihood function\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mathbf{Y} | \\mathbf{W}, \\mathbf{X}) = \\prod_{k = 1}^K \\mathcal{N}(\\mathbf{y}_k; \\mathbf{f}_k \\mathbf{W}_{L+1}, \\sigma_n^2 I),\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\mathbf{y}_k$ and $\\mathbf{f}_{L, k}$ correspond to the $k^{th}$ row of the $\\mathbf{Y}$ and $\\mathbf{F}_{L+1}$ matrices. The posterior over the weights of this network is not analytic and must therefore be approximated. \n",
    "\n",
    "However, conditioned on the weights of all previous layers, the posterior over the weights of the last layer is analytic. In particular, we have\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mathbf{w}_{L+1, d} | \\mathbf{W}_{1:L}, \\mathbf{X}) \\propto \\mathcal{N}(y_{\\cdot, d}; \\mathbf{f}_{\\cdot, d} \\mathbf{w}_{L+1, \\cdot, d}, \\sigma_n^2 I) p(\\mathbf{w}_{L+1, d}),\n",
    "\\end{align}$$\n",
    "\n",
    "where $y_{\\cdot, d}$ is the is the $d^{th}$ column of $\\mathbf{Y}$ and . Note that the features $\\mathbf{f}_n$ are dependent on $\\mathbf{W}_{1:L}$. Therefore, $\\mathbf{w}_{L+1,d}$ has conditional posterior\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mathbf{w}_{L+1, d} | \\mathbf{W}_{1:L}, \\mathbf{X}) = \\mathcal{N}(\\mathbf{w}_{L+1, d}; , (\\sigma_n^{-2} I)),\n",
    "\\end{align}$$\n",
    "\n",
    "Ober and Aitchison {cite}`ober2021global` draw inspiration from this to propose an approximate posterior, in which the weights of a layer given all previous layers are conditionally Gaussian, but the full posterior is not.\n",
    "\n",
    "## Approximate posterior\n",
    "\n",
    "The approximate posterior takes the form\n",
    "\n",
    "$$\\begin{align}\n",
    "q\\left(\\mathbf{W}_l | \\{\\mathbf{W}_{l'}\\}_{l' = 1}^{l-1}\\right) &\\propto \\prod_{d = 1}^{D_l} \\mathcal{N}\\left(\\mathbf{v}^l_d; \\phi(\\mathbf{F}_{l-1}) \\mathbf{w}^l_d, \\boldsymbol{\\Lambda}^{-1}_l\\right) p(\\mathbf{w}^l_d),\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "which, if we rearrange to explicitly be in the form of a distrubution over the $\\mathbf{w}_d^l$ weights, becomes\n",
    "\n",
    "$$\\begin{align}\n",
    "q\\left(\\mathbf{W}_l | \\{\\mathbf{W}_{l'}\\}_{l' = 1}^{l-1}\\right) &= \\prod_{d = 1}^{D_l} \\mathcal{N}\\left(\\mathbf{w}^l_d; \\boldsymbol{\\mu}_l^w, \\boldsymbol{\\Sigma}_l^w\\right), \\\\\n",
    "                                          \\boldsymbol{\\mu}_l^w  &= \\boldsymbol{\\Sigma}_l^w \\phi\\left(\\mathbf{F}_{l-1}\\right)^\\top \\boldsymbol{\\Lambda}_l \\mathbf{v}_d^l, \\\\\n",
    "                                       \\boldsymbol{\\Sigma}_l^w  &= \\left( D_l \\mathbf{I} + \\phi\\left(\\mathbf{F}_{l-1}\\right)^\\top \\boldsymbol{\\Lambda}_l \\phi\\left(\\mathbf{F}_{l-1}\\right) \\right)^{-1}.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a `GlobalInducingDenseLayer`, which handdles propagating the data activations $\\mathbf{F}_l$, the inducing activations $\\mathbf{U}_l$ and computes the contribution of the layer to the total KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalInducingDenseLayer(tfk.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_input,\n",
    "                 num_output,\n",
    "                 num_inducing,\n",
    "                 nonlinearity,\n",
    "                 dtype,\n",
    "                 name=\"global_inducing_fully_connected_layer\",\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "        \n",
    "        self.num_input = num_input + 1\n",
    "        self.num_output = num_output\n",
    "        self.num_inducing = num_inducing\n",
    "        \n",
    "        # Set nonlinearity for the layer\n",
    "        self.nonlinearity = (lambda x: x) if nonlinearity is None else \\\n",
    "                            getattr(tf.nn, nonlinearity)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Set up prior mean, scale and distribution\n",
    "        self.prior_mean = tf.zeros(\n",
    "            shape=(self.num_output, self.num_input),\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.prior_scale = tf.ones(\n",
    "            shape=(self.num_output, self.num_input),\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.prior_scale = self.prior_scale / self.num_input**0.5\n",
    "        \n",
    "        self.prior = tfd.MultivariateNormalDiag(\n",
    "            loc=self.prior_mean,\n",
    "            scale_diag=self.prior_scale\n",
    "        )\n",
    "        \n",
    "        # Set up pseudo observation means and variances\n",
    "        self.pseudo_means = tf.zeros(\n",
    "            shape=(self.num_inducing, self.num_output),\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.pseudo_mean = tf.Variable(self.pseudo_means)\n",
    "        \n",
    "        self.pseudo_log_prec = tf.zeros(\n",
    "            shape=(self.num_inducing,),\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.pseudo_log_prec = tf.Variable(self.pseudo_log_prec)\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def pseudo_precision(self):\n",
    "        return tf.math.exp(self.pseudo_log_precision)\n",
    "    \n",
    "        \n",
    "    def q_prec_cov_chols(self, Uin):\n",
    "        \n",
    "        phiU = self.nonlinearity(Uin)\n",
    "        pseudo_prec = tf.math.exp(self.pseudo_log_prec)\n",
    "        \n",
    "        # Compute precision matrix of multivariate normal\n",
    "        phiT_lambda_phi = tf.einsum(\"mi, m, mj -> ij\", phiU, pseudo_prec, phiU)\n",
    "        \n",
    "        q_prec = tf.linalg.diag(self.prior_scale[0, :]**-2.) + phiT_lambda_phi\n",
    "        \n",
    "        # Compute cholesky of approximate posterior precision\n",
    "        q_prec_chol = tf.linalg.cholesky(q_prec)\n",
    "        \n",
    "        # Compute cholesky of approximate posterior covariance\n",
    "        iq_prec_chol = tf.linalg.triangular_solve(\n",
    "            q_prec_chol,\n",
    "            tf.eye(q_prec_chol.shape[0]),\n",
    "            lower=True\n",
    "        )\n",
    "        \n",
    "        q_cov = tf.matmul(iq_prec_chol, iq_prec_chol, transpose_a=True)\n",
    "        q_cov_chol = tf.linalg.cholesky(q_cov)\n",
    "        \n",
    "        return q_prec_chol, q_cov_chol\n",
    "    \n",
    "    \n",
    "    def q_mean(self, Uin, prec_chol):\n",
    "        \n",
    "        phiU = self.nonlinearity(Uin)\n",
    "        pseudo_prec = tf.math.exp(self.pseudo_log_prec)\n",
    "        \n",
    "        mean = tf.matmul(\n",
    "            phiU,\n",
    "            pseudo_prec[:, None] * self.pseudo_mean,\n",
    "            transpose_a=True\n",
    "        )\n",
    "        \n",
    "        mean = tf.linalg.cholesky_solve(prec_chol, mean)\n",
    "        mean = tf.transpose(mean, [1, 0])\n",
    "        \n",
    "        return mean\n",
    "        \n",
    "        \n",
    "    def call(self, Fin, Uin):\n",
    "        \n",
    "        # Augment input features with ones to absorb bias\n",
    "        Fones = tf.ones(shape=(Fin.shape[0], 1), dtype=self.dtype)\n",
    "        Fin = tf.concat([Fin, Fones], axis=-1)\n",
    "        \n",
    "        Uones = tf.ones(shape=(Uin.shape[0], 1), dtype=self.dtype)\n",
    "        Uin = tf.concat([Uin, Uones], axis=-1)\n",
    "        \n",
    "        Din = self.num_input\n",
    "        Dout = self.num_output\n",
    "        M = self.num_inducing\n",
    "        \n",
    "        # Check shape of input features Fin and pseudo-means\n",
    "        check_shape(\n",
    "            [Fin, Uin, self.pseudo_means],\n",
    "            [(-1, Din), (M, Din), (M, Dout)]\n",
    "        )\n",
    "        \n",
    "        # Compute cholesky factors of q precision and covariance.\n",
    "        # These are common between all weight columns, i.e. the covariance\n",
    "        # between weights leading to a neuron in the next layer is shared\n",
    "        # between all next neurons.\n",
    "        q_prec_chol, q_cov_chol = self.q_prec_cov_chols(Uin)\n",
    "        \n",
    "        check_shape(\n",
    "            [q_prec_chol, q_cov_chol],\n",
    "            [(Din, Din), (Din, Din)]\n",
    "        )\n",
    "        \n",
    "        # Compute means of q. There is a different mean vector for\n",
    "        # each column of weights.\n",
    "        q_mean = self.q_mean(Uin, q_prec_chol)\n",
    "        \n",
    "        check_shape(q_mean, (Dout, Din))\n",
    "        \n",
    "        # Sample approximate posterior for the weights\n",
    "        q_cov_chol = tf.stack([q_cov_chol]*Dout, axis=0)\n",
    "        q = tfd.MultivariateNormalTriL(loc=q_mean, scale_tril=q_cov_chol)\n",
    "        wT = q.sample()\n",
    "        w = tf.transpose(wT, [1, 0])\n",
    "        \n",
    "        check_shape(w, (Din, Dout))\n",
    "        \n",
    "        # Compute contibution to ELBO\n",
    "        kl_term = q.kl_divergence(self.prior)\n",
    "        kl_term = tf.reduce_sum(kl_term)\n",
    "        \n",
    "        # Compute log-probability of weights under prior\n",
    "        log_p = self.prior.log_prob(wT)\n",
    "        log_p = tf.reduce_sum(log_p)\n",
    "        \n",
    "        # Compute log-probability of weights under approximate posterior\n",
    "        log_q = q.log_prob(wT)\n",
    "        log_q = tf.reduce_sum(log_q)\n",
    "        \n",
    "        # Compute Fout and Uout and return\n",
    "        Fout = tf.matmul(self.nonlinearity(Fin), w)\n",
    "        Uout = tf.matmul(self.nonlinearity(Uin), w)\n",
    "        \n",
    "        return Fout, Uout, kl_term, log_p, log_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then stack a few `GlobalInducingDenseLayers` to form a `GlobalInducingFullyConnectedNetwork`. We use an architecture using two hidden layers, each using $50$ units, as done by Ober and Aitchinson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class GlobalInducingFullyConnectedNetwork(tfk.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input,\n",
    "                 num_output,\n",
    "                 inducing_points,\n",
    "                 nonlinearity,\n",
    "                 dtype,\n",
    "                 name=\"global_inducing_fully_connected\",\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "        \n",
    "        self.num_input = num_input\n",
    "        self.num_output = num_output\n",
    "        self.inducing_points = inducing_points\n",
    "        self.num_inducing = inducing_points.shape[0]\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.num_hidden = [50, 50]\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.inducing_points = tf.Variable(self.inducing_points)\n",
    "        \n",
    "        self.l1 = GlobalInducingDenseLayer(\n",
    "            num_input=self.num_input,\n",
    "            num_output=self.num_hidden[0],\n",
    "            num_inducing=self.num_inducing,\n",
    "            nonlinearity=None,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.l2 = GlobalInducingDenseLayer(\n",
    "            num_input=self.num_hidden[0],\n",
    "            num_output=self.num_hidden[1],\n",
    "            num_inducing=self.num_inducing,\n",
    "            nonlinearity=self.nonlinearity,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.l3 = GlobalInducingDenseLayer(\n",
    "            num_input=self.num_hidden[1],\n",
    "            num_output=self.num_output,\n",
    "            num_inducing=self.num_inducing,\n",
    "            nonlinearity=self.nonlinearity,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        self.log_noise = tf.Variable(\n",
    "            tf.convert_to_tensor(-2., dtype=self.dtype)\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def noise(self):\n",
    "        return tf.math.exp(self.log_noise)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        \n",
    "        F1, U1, kl1, log_p1, log_q1 = self.l1(x, self.inducing_points)\n",
    "        F2, U2, kl2, log_p2, log_q2 = self.l2(F1, U1)\n",
    "        F3, U3, kl3, log_p3, log_q3 = self.l3(F2, U2)\n",
    "        \n",
    "        means = F3\n",
    "        scales = self.noise * tf.ones_like(F3)\n",
    "        \n",
    "        kl = tf.reduce_sum([kl1, kl2, kl3])\n",
    "        \n",
    "        log_p = tf.reduce_sum([log_p1, log_p2, log_p3])\n",
    "        log_q = tf.reduce_sum([log_q1, log_q2, log_q3])\n",
    "        \n",
    "        return means, scales, kl, log_p, log_q\n",
    "    \n",
    "    \n",
    "    def elbo(self, x, y):\n",
    "\n",
    "        means, scales, kl, _, _ = self(x)\n",
    "\n",
    "        cond_lik = tfd.Normal(loc=means, scale=scales)\n",
    "        cond_lik = tf.reduce_sum(cond_lik.log_prob(y))\n",
    "\n",
    "        elbo = cond_lik - kl\n",
    "\n",
    "        return elbo, cond_lik, kl\n",
    "\n",
    "    \n",
    "    def iwbo(self, x, y, num_samples):\n",
    "        \n",
    "        @tf.function\n",
    "        def call(x):\n",
    "            return self.call(x)\n",
    "\n",
    "        iwbo = []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "\n",
    "            means, scales, kl, log_p, log_q = call(x)\n",
    "\n",
    "            cond_lik = tfd.Normal(loc=means, scale=scales)\n",
    "            cond_lik = tf.reduce_sum(cond_lik.log_prob(y))\n",
    "\n",
    "            iwbo.append(cond_lik + log_p - log_q)\n",
    "\n",
    "        iwbo = tf.stack(iwbo, axis=0)\n",
    "        iwbo = tf.math.reduce_logsumexp(iwbo) - np.log(num_samples)\n",
    "\n",
    "        return iwbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "center-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "num_data = 100\n",
    "num_input = 1\n",
    "std_noise = 3.\n",
    "\n",
    "x1 = tf.random.uniform(minval=-4., maxval=-2., shape=(num_data // 2, 1))\n",
    "x2 = tf.random.uniform(minval=2., maxval=4., shape=(num_data // 2, 1))\n",
    "\n",
    "x = tf.concat([x1, x2], axis=0)\n",
    "y = tf.concat([x1, x2], axis=0) ** 3. + std_noise * tf.random.normal(shape=(num_data, 1))\n",
    "\n",
    "x = (x - tf.reduce_mean(x)) / tf.math.reduce_std(x)\n",
    "y = (y - tf.reduce_mean(y)) / tf.math.reduce_std(y)\n",
    "\n",
    "# Figure to plot on \n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(\n",
    "    x[:, 0],\n",
    "    y[:, 0],\n",
    "    marker=\"+\",\n",
    "    c=\"black\"\n",
    ")\n",
    "\n",
    "# Format plot\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "\n",
    "plt.xticks(np.linspace(-2., 2., 3), fontsize=24)\n",
    "plt.yticks(np.linspace(-3., 3., 3), fontsize=24)\n",
    "\n",
    "plt.xlabel(\"$x$\", fontsize=32)\n",
    "plt.ylabel(\"$y$\", fontsize=32)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decorate a single gradient descent step with tf.function. On the first\n",
    "# call of single_step, tensorflow will compile the computational graph first.\n",
    "# After that, all calls to single_step will use the compiled graph which is\n",
    "# much faster than the default eager mode execution. In this case, the gain\n",
    "# is roughly a x20 speedup (with a CPU), which can be checked by commenting\n",
    "# out the decorator and rerunning the training script.\n",
    "\n",
    "@tf.function\n",
    "def single_step(model, optimiser, x, y):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        elbo, cond_lik, kl = model.elbo(x, y)\n",
    "        loss = - elbo / x.shape[0]\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return elbo, cond_lik, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set model constants\n",
    "num_input = 1\n",
    "num_output = 1\n",
    "num_inducing = 100\n",
    "dtype = tf.float32\n",
    "nonlinearity = \"relu\"\n",
    "num_steps = int(1e5)\n",
    "\n",
    "# Initialise inducing points at subset of training points\n",
    "inducing_idx = tf.random.shuffle(tf.range(x.shape[0]))[:num_inducing]\n",
    "inducing_points = tf.gather(x, inducing_idx)\n",
    "\n",
    "# Initialise model\n",
    "model = GlobalInducingFullyConnectedNetwork(\n",
    "    num_input=num_input,\n",
    "    num_output=num_output,\n",
    "    inducing_points=inducing_points,\n",
    "    nonlinearity=nonlinearity,\n",
    "    dtype=dtype\n",
    ")\n",
    "\n",
    "# Initialise optimiser\n",
    "optimiser = tfk.optimizers.Adam(learning_rate=1e-3)\n",
    "    \n",
    "# Set progress bar and suppress warnings\n",
    "progress_bar = tqdm(range(1, num_steps+1))\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Set tensors for keeping track of quantities of interest\n",
    "train_elbo = []\n",
    "train_cond_lik = []\n",
    "train_kl = []\n",
    "\n",
    "# Train model\n",
    "for i in progress_bar:\n",
    "        \n",
    "    elbo, cond_lik, kl = single_step(\n",
    "        model=model,\n",
    "        optimiser=optimiser,\n",
    "        x=x,\n",
    "        y=y\n",
    "    )\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        \n",
    "        progress_bar.set_description(\n",
    "            f\"ELBO {elbo:.1f}, \"\n",
    "            f\"Cond-lik. {cond_lik:.1f}, \"\n",
    "            f\"KL {kl:.1f}\"\n",
    "        )\n",
    "        \n",
    "    train_elbo.append(elbo)\n",
    "    train_cond_lik.append(cond_lik)\n",
    "    train_kl.append(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "center-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Number of samples to draw, three will be plotted\n",
    "num_samples = 100\n",
    "\n",
    "# Input locations to plot\n",
    "x_plot = tf.linspace(-4., 4., 100)[:, None]\n",
    "\n",
    "# Draw samples from model\n",
    "samples = [model(x_plot)[0] for i in range(num_samples)]\n",
    "\n",
    "# Compute mean and standard deviation of samples\n",
    "mean = tf.reduce_mean(samples, axis=0)\n",
    "std = tf.math.reduce_std(samples, axis=0)\n",
    "\n",
    "# Figure to plot on \n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot epistemic uncertainty\n",
    "plt.fill_between(\n",
    "    x_plot[:, 0],\n",
    "    mean[:, 0] - 2.*std[:, 0],\n",
    "    mean[:, 0] + 2.*std[:, 0],\n",
    "    color=\"tab:gray\",\n",
    "    alpha=0.4,\n",
    "    zorder=1\n",
    ")\n",
    "\n",
    "# Plot three samples\n",
    "for i, color in enumerate([\"tab:red\", \"tab:green\", \"tab:blue\"]):\n",
    "    \n",
    "    plt.plot(\n",
    "        x_plot[:, 0],\n",
    "        samples[i][:, 0],\n",
    "        color=color,\n",
    "        zorder=2\n",
    "    )\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(\n",
    "    model.inducing_points[:, 0],\n",
    "    -3.*tf.ones_like(model.inducing_points[:, 0]),\n",
    "    marker=\"x\",\n",
    "    c=\"tab:purple\",\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(\n",
    "    x[:, 0],\n",
    "    y[:, 0],\n",
    "    marker=\"+\",\n",
    "    c=\"black\",\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Format plot\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-4.5, 4.5])\n",
    "\n",
    "plt.xticks(np.linspace(-3., 3., 3), fontsize=24)\n",
    "plt.yticks(np.linspace(-4., 4., 5), fontsize=24)\n",
    "\n",
    "plt.xlabel(\"$x$\", fontsize=32)\n",
    "plt.ylabel(\"$y$\", fontsize=32)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(array, n):\n",
    "    \n",
    "    cumsum = np.cumsum(array)\n",
    "    cumsum[n:] = cumsum[n:] - cumsum[:-n]\n",
    "    \n",
    "    moving_average = cumsum[n - 1:] / n\n",
    "    \n",
    "    return moving_average\n",
    "\n",
    "plt.plot(moving_average(tf.stack(train_elbo).numpy(), n=1000))\n",
    "# plt.xlim([2000, 100000])\n",
    "plt.ylim([-25, -15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How tight is the GIP ELBO?\n",
    "\n",
    "We can also check how tight the ELBO of the GIP approximate posterior is, using importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_repetitions = 10\n",
    "num_iwbo_samples = 1000\n",
    "\n",
    "elbos = [model.elbo(x=x, y=y)[0] for i in range(num_repetitions)]\n",
    "iwbos = [model.iwbo(x=x, y=y, num_samples=num_iwbo_samples) for i in range(num_repetitions)]\n",
    "\n",
    "print(\n",
    "    f\"ELBO: {tf.reduce_mean(elbos): 7.3f} +/- {2.*tf.math.reduce_std(elbos)/num_repetitions**0.5:.3f} \"\n",
    "    f\"(estimated with {num_repetitions} ELBO samples)\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"IWBO: {tf.reduce_mean(iwbos): 7.3f} +/- {2.*tf.math.reduce_std(iwbos)/num_repetitions**0.5:.3f} \"\n",
    "    f\"(estimated with {num_repetitions} IWBO samples, each using {num_iwbo_samples} weight samples)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography} ./references.bib\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-random-walks",
   "language": "python",
   "name": "venv-random-walks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
