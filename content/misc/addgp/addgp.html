
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Additive Gaussian Processes &#8212; Random walks</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://random-walks.org/content/misc/addgp/addgp.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Interesting reading and websites" href="../../reading-and-links.html" />
    <link rel="prev" title="Stein variational gradient descent" href="../svgd/svgd.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://random-walks.org/content/misc/addgp/addgp.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Additive Gaussian Processes" />
<meta property="og:description" content="Additive Gaussian Processes  One central limitation of Gaussian Processes (GPs) [Ras03] is that they scale poorly with increasing input dimensions. One reason f" />
<meta property="og:image"       content="https://random-walks.org/_static/logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Random walks</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../home.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../prob-intro/intro.html">
   Probability: An introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch01/content.html">
     Events and Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch02/content.html">
     Discrete random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch03/content.html">
     Multivariate discrete distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch04/content.html">
     Probability generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch05/content.html">
     Distribution and density functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch06/content.html">
     Multivariate distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch07/content.html">
     Moment generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch08/content.html">
     Main limit theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch09/content.html">
     Branching processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch10/content.html">
     Random walks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch11/content.html">
     Processes in continuous time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch12/content.html">
     Markov chains
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../gp/gp-intro.html">
   Gaussian Processes
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../gp/why-covariances.html">
     Why covariance functions?
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../gp/sparse/sparse-intro.html">
     Sparse Gaussian Processes
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/vfe.html">
       Variational Free Energy GPs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/gp-sampling.html">
       Sampling GP posteriors
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../misc.html">
   Miscellaneous
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../sde/num-sde.html">
     Numerical simulation of SDEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sde-as-gp/sde-as-gp.html">
     VI for diffusion processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../node/node.html">
     Neural ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../optimisation/conjugate-gradients.html">
     Conjugate gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../kalman/kalman.html">
     The Kalman filter and smoother
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ncs/ncs.html">
     Natural cubic splines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ars/ars.html">
     Adaptive rejection sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../score-matching/score-matching.html">
     Estimation by score matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interacting/interacting.html">
     Interacting particle FPK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rff/rff.html">
     Random Fourier features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../svgd/svgd.html">
     Stein variational gradient descent
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Additive Gaussian Processes
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reading-and-links.html">
   Interesting reading and websites
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/misc/addgp/addgp.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/stratisMarkou/random-walks/master?urlpath=tree/./content/misc/addgp/addgp.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additive-covariances">
   Additive covariances
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sanity-checking">
     Sanity checking
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampling-the-prior">
     Sampling the prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-and-inference">
     Training and inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#two-dimensional-example">
   Two-dimensional example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-on-concrete-data">
   Example on concrete data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="additive-gaussian-processes">
<h1>Additive Gaussian Processes<a class="headerlink" href="#additive-gaussian-processes" title="Permalink to this headline">¶</a></h1>
<p>One central limitation of Gaussian Processes (GPs) <a class="bibtex reference internal" href="../rff/rff.html#rasmussen2003gaussian" id="id1">[Ras03]</a> is that they scale poorly with increasing input dimensions. One reason for the poor scaling is that the choice of covariance function has a great impact on the inferred posterior, and naively chosen covariances have an increasingly adverse effect as the number of dimensions is increased. For example, choosing a covariance involving all input variables <span class="math notranslate nohighlight">\(x_1, ..., x_D\)</span>, such as a member of the RBF family (e.g. EQ or Matern), is equivalent to the prior assumption that the function <span class="math notranslate nohighlight">\(f\)</span> being modelled depends jointly on these variables. To model a function <span class="math notranslate nohighlight">\(f\)</span> involving interactions between all input variables we need lots of data, since the whole input space must be covered to pin the function sufficiently well throughout the input space. However, in many cases of interest, the data may be explainable via functions with low-order interactions, which can be modelled much more efficiently.</p>
<p>Suppose for example that we wish to predict the CO2 emissions per 100km of a car, <span class="math notranslate nohighlight">\(f\)</span>, as a function of its horsepower <span class="math notranslate nohighlight">\(x_1\)</span> and mass <span class="math notranslate nohighlight">\(x_2\)</span>. We might choose to model <span class="math notranslate nohighlight">\(f\)</span> using a covariance function that is joint in <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>. This prior assumption implies that <span class="math notranslate nohighlight">\(f\)</span> itself jointly depends on <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{align}
k = k(x, x'), x = [x_1, x_2] \implies f = f(x_1, x_2).
\end{align}\]</div>
<p>But we may know for example that the horsepower and weight of the car make independent contributions to the emissions, so that the total CO2 emitted is the sum of two univariate functions <span class="math notranslate nohighlight">\(f(x_1, x_2) = f_1(x_1) + f_2(x_2)\)</span>, of the horsepower and weight respectively. Such a prior assumption is captured by a sum-separable covariance function of the form</p>
<div class="math notranslate nohighlight">
\[\begin{align}
k = k(x_1, x_1') + k(x_2, x_2') \implies f = f_1(x_1) + f_2(x_2).
\end{align}\]</div>
<p>Modelling with this covariance means that much fewer data are required to pin down the function. If the region of interest in our car example is a <span class="math notranslate nohighlight">\(W \times H\)</span> rectangle, the first covariance requires <span class="math notranslate nohighlight">\(O(WH)\)</span> data to pin down the function since it needs to uniformly cover the input space. On the contrary, the latter requires only <span class="math notranslate nohighlight">\(\mathcal{O}(W) + \mathcal{O}(H)\)</span> to achieve the same level of certainty, since it only needs to pin down the univariate functions <span class="math notranslate nohighlight">\(f_1\)</span> and <span class="math notranslate nohighlight">\(f_2\)</span> to fully determine <span class="math notranslate nohighlight">\(f\)</span>. In general, we may wish to involve interactions of various degrees in our model. Moreover, we may not know which levels of interactions to include a priori, and may wish to learn these from the data. Additive GPs <a class="bibtex reference internal" href="#duvenaud2011additive" id="id2">[DNR11]</a> provide a way for modelling with covariance functions involving the sum of interaction terms of any order, in a computationally efficient way.</p>
<div class="section" id="additive-covariances">
<h2>Additive covariances<a class="headerlink" href="#additive-covariances" title="Permalink to this headline">¶</a></h2>
<p>As discussed above, we may not know a priori, what additive structure may be present in our data. Additive GPs directly address this issue by placing a prior which is the sum of all possible kinds of interactions between the input variables. Assuming the inputs lie in <span class="math notranslate nohighlight">\(\mathbb{R}^D\)</span> and starting from a collection of univariate <em>base covariances</em> <span class="math notranslate nohighlight">\(k_d(x_d, x_d'), d = 1, ..., D\)</span> for each dimension, additive GPs place a covariance of the form</p>
<div class="math notranslate nohighlight">
\[\begin{align}
k_{\text{add}}(x, x') = \sum_{d = 1}^D \sigma^2_d k_{\text{add}, d}(x, x')
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(k_{\text{add}, d}(x, x')\)</span> itself is of the form</p>
<div class="math notranslate nohighlight">
\[\begin{align}
k_{\text{add}, d}(x, x') &amp;= \sum_{1 \leq i_1 &lt; ... &lt; i_d \leq D} \prod_{n = 1}^N k_{i_n}(x_{i_n}, x_{i_n}').
\end{align}\]</div>
<p>For example, in three dimensions the covariance takes the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
k_{\text{add}}(x, x') = &amp;~~\phantom{+} \sigma^2_1 \left(k_1(x_1, x_1') + k_2(x_2, x_2') + k_3(x_3, x_3') \right) \\
                        &amp; + \sigma^2_2 \left(k_1(x_1, x_1')k_2(x_2, x_2') + k_2(x_2, x_2')k_3(x_3, x_3') + k_1(x_1, x_1')k_3(x_3, x_3')\right) \\
                        &amp; + \sigma^2_3 \left(k_1(x_1, x_1')k_2(x_2, x_2')k_3(x_3, x_3') \right).
\end{align}\end{split}\]</div>
<p>This additive prior allows the GP to model all combinations of interactions of the input variables. The term in the first line models univariate functions, while the terms in the second and third line models functions with all combinations of two and three input variables. The variance coefficients up front allow modulation of each order of interactions, while including private variances within each <span class="math notranslate nohighlight">\(k_d\)</span> controls the magnitude of each base covariance in isolation.</p>
<p>However, attempting to naively compute <span class="math notranslate nohighlight">\(k_{\text{add}}(x, x')\)</span> is prohibitively expensive, since there are <span class="math notranslate nohighlight">\(\mathcal{O}(2^D)\)</span> terms involved in this expression. Fortunately, the Newton-Girard equations provide a much more efficient way to compute the covariance, reducing the cost from <span class="math notranslate nohighlight">\(\mathcal{O}(2^D)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(D^2)\)</span>.</p>
<div class="lemma">
<p><strong>Definition (Newton-Girard formulae)</strong> Let <span class="math notranslate nohighlight">\(z_1, ..., z_D \in \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(s_k\)</span> be the power sum</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p_K(z_1, ..., z_D) = P_K = \sum_{d = 1}^D z_d^k.
\end{align}\]</div>
<p>and <span class="math notranslate nohighlight">\(s_K\)</span> be the symmetric polynomial of order <span class="math notranslate nohighlight">\(K &lt; D\)</span>, written as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
s_K(z_1, ..., z_D) = S_K = \sum_{1 \leq i_1 &lt; ... &lt; i_K \leq K} \prod_{k = 1}^K z_{i_k}.
\end{align}\]</div>
<p>The following recursive relation, known as the Newton-Girard formula, holds</p>
<div class="math notranslate nohighlight">
\[\begin{align}
S_K &amp;= \frac{1}{K} \sum_{k = 1}^{K} (-1)^{k-1} S_{K - k} P_k,
\end{align}\]</div>
<p>where we have defined <span class="math notranslate nohighlight">\(S_0 = 1\)</span>.</p>
</div>
<br>
<p>Several proofs of the Newton-Girard formulae are available. Here we present a short one, adapted from <a class="reference external" href="https://planetmath.org/ProofOfNewtonGirardFormulaForSymmetricPolynomials">planetmath.org</a>.</p>
<details class="proof">
<summary>Derivation: Newton-Girard formulae</summary>
<p>Starting from the polynomial</p>
<div class="math notranslate nohighlight">
\[\begin{align}
f(x) &amp;= \prod_{d = 1}^D (1 - z_d x)
\end{align}\]</div>
<p>and taking its derivative, we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
f'(x) &amp;= - f(x)\sum_{d = 1}^D \frac{z_d}{1 - z_d x} \\
      &amp;= \phantom{-} f(x)\sum_{d = 1}^D \sum_{l = 0}^\infty (-1)^{l+1} z_d^{l + 1} x^l \\
      &amp;= \phantom{-} f(x)\sum_{l = 0}^\infty (-1)^{l+1} P_{l + 1} x^l
\end{align}\end{split}\]</div>
<p>Now, noting that the polynomial can also be written as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
f(x) &amp;=  \sum_{d = 0}^D s_d(z_1, ..., z_D)~x^d \\
     &amp;=  \sum_{d = 0}^D S_d~x^d,
\end{align}\end{split}\]</div>
<p>we take a derivative of this expression to obtain</p>
<div class="math notranslate nohighlight">
\[\begin{align}
f'(x) &amp;= S_1 + 2S_2 x + ... + D S_D x^{D-1}.
\end{align}\]</div>
<p>Now equating the two expressions for the derivatives and rearranging to compare coefficients</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
S_1 + 2S_2 x + ... + D S_D x^{D-1} &amp;= \left[\sum_{d = 0}^D S_d~x^d \right] \left[\sum_{l = 0}^\infty (-1)^{l+1} P_{l + 1} x^l\right] \\
                                   &amp;= \left[\sum_{d = 0}^D \sum_{l = 0}^\infty (-1)^{l+1} S_d P_{l + 1} x^{d + l}\right]\\
                                   &amp;= \left[\sum_{n = 0}^\infty (S_n P_1 - ... + (-1)^n S_0 P_{n + 1}) x^n\right]
\end{align}\end{split}\]</div>
<p>we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{align}
S_K &amp;= \frac{1}{K} \sum_{k = 1}^{K} (-1)^{k-1} S_{K - k} P_k.
\end{align}\]</div>
</details>
<br>
<p>Using the Newton-Girard formula, we can efficiently compute the covariance function up to the desired level of interactions. For a <span class="math notranslate nohighlight">\(D\)</span>-dimensional problem, we can compute the first <span class="math notranslate nohighlight">\(K \leq D\)</span> interactions in <span class="math notranslate nohighlight">\(\mathcal{O}(K^2)\)</span> time. Further, we could also use high-order interactions if we so want, by taking differences of a low-order and a high-order interaction covariance.</p>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>We can now implement the additive covariance function. Note that the individual covariances <span class="math notranslate nohighlight">\(k_i\)</span> can be chosen to have any form we like, and even to have different forms from each other - we could choose for example <span class="math notranslate nohighlight">\(k_1\)</span> to be EQ, <span class="math notranslate nohighlight">\(k_2\)</span> to be periodic and so on. Here we will take all of <span class="math notranslate nohighlight">\(k_i\)</span> to be EQ covariance, implemented below. This <code class="docutils literal notranslate"><span class="pre">EQKernel</span></code> takes <code class="docutils literal notranslate"><span class="pre">dim</span></code> as an argument to specify which entry of the input vector it should use: if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">=</span> <span class="pre">k</span></code> is passed, the covariance will act on the <span class="math notranslate nohighlight">\(k^{th}\)</span> entry of the input vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">class</span> <span class="nc">EQKernel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">lengthscale</span><span class="p">,</span>
                 <span class="n">variance</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;eq_kernel&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">log_lengthscale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lengthscale</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_lengthscale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_lengthscale</span><span class="p">,</span>
                                           <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">log_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_variance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_variance</span><span class="p">,</span>
                                        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">&lt;</span> <span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">x2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="n">diff</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">]</span> <span class="o">-</span> <span class="n">x2</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">]</span>
        <span class="n">exp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_lengthscale</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">exp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_variance</span><span class="p">)</span> <span class="o">*</span> <span class="n">exp</span>
        
        <span class="k">return</span> <span class="n">exp</span>
</pre></div>
</div>
</div>
</div>
<p>We can now implement the additive covariance function. This is a wrapper around a list of first-order covariances, together with a <code class="docutils literal notranslate"><span class="pre">__call__</span></code> function which computes the additive covariance using the Newton-Girard formula presented earlier. The arguments <code class="docutils literal notranslate"><span class="pre">Kmin</span></code> and <code class="docutils literal notranslate"><span class="pre">Kmax</span></code> allow the user to specify the minimum and maximum order of interactions to be included in the covariance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AdditiveCovariance</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">covs</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;addgp&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># dtype for the covariance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        
        <span class="c1"># Set 1st-order covariances and model dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">covs</span> <span class="o">=</span> <span class="n">covs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">covs</span><span class="p">)</span>
        
        <span class="c1"># Set log-variances for each order</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">variances</span><span class="p">),</span>
                                         <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="c1"># Set log-noise for overall covariance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">noise</span><span class="p">),</span>
                                     <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        
        
    <span class="k">def</span> <span class="nf">P</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        
        <span class="c1"># Compute 1st-order covariances</span>
        <span class="n">covs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cov</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="k">for</span> <span class="n">cov</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">covs</span><span class="p">]</span>
        <span class="n">covs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">covs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Powers in which to raise the covariances</span>
        <span class="n">powers</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="c1"># Raise covariances to powers and sum</span>
        <span class="n">cov_powers</span> <span class="o">=</span> <span class="n">covs</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">**</span> <span class="n">powers</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">cov_powers</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">cov_powers</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">cov_powers</span>
        
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">Kmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">Kmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">individual_orders</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        
        <span class="k">assert</span> <span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">x2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span>
        
        <span class="n">Kmin</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">Kmin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">Kmin</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">Kmax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="k">if</span> <span class="n">Kmax</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">Kmax</span>
        
        <span class="c1"># Compute power sums up-front</span>
        <span class="n">P</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        
        <span class="c1"># List of S matices to use for recursive Newton-Girard</span>
        <span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> \
             <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
        
        <span class="c1"># S0 is 1 by defnition</span>
        <span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="c1"># Add kernel contributions by Newton-Girard </span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">Kmax</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
                
                <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">k</span> <span class="o">*</span> <span class="n">S</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">P</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">k</span><span class="p">]</span>
                
            <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">i</span>
            
        <span class="c1"># Take out the first S array -- this was used only for the recursion</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="n">Kmin</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">Kmax</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Multiply by variances for each order</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">S</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_variances</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">Kmin</span><span class="p">:</span><span class="n">Kmax</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">individual_orders</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">S</span>
        
        <span class="c1"># Multiply by variances and sum all orders</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># If required add the diagonal noise term</span>
        <span class="k">if</span> <span class="n">add_noise</span><span class="p">:</span>
            <span class="n">cov</span> <span class="o">=</span> <span class="n">cov</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">cov</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="sanity-checking">
<h3>Sanity checking<a class="headerlink" href="#sanity-checking" title="Permalink to this headline">¶</a></h3>
<p>To sanity check this implementation, we can compute the covariance using the Newton-Girard method and compare this with the direct (but inefficient) approach. The two methods should give identical answers, which in fact they do.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data type to use</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>

<span class="c1"># Number of dimensions to use</span>
<span class="n">num_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Initialise first-order covariances</span>
<span class="n">covs</span> <span class="o">=</span> <span class="p">[</span><span class="n">EQKernel</span><span class="p">(</span><span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                 <span class="n">variance</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_dim</span><span class="p">)]</span>

<span class="c1"># Set variance for each order</span>
<span class="n">variances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_dim</span><span class="p">,))</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="c1"># Initialise additive covariance object</span>
<span class="n">add_cov</span> <span class="o">=</span> <span class="n">AdditiveCovariance</span><span class="p">(</span><span class="n">covs</span><span class="o">=</span><span class="n">covs</span><span class="p">,</span>
                             <span class="n">variances</span><span class="o">=</span><span class="n">variances</span><span class="p">,</span>
                             <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Create random data to pass through covariance</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Compute covariance using Newton Girard</span>
<span class="n">newton_girard_cov</span> <span class="o">=</span> <span class="n">add_cov</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Covariance matrix via Newton-Girard</span><span class="se">\n</span><span class="si">{</span><span class="n">newton_girard_cov</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Compute covariance directly</span>
<span class="n">direct_cov</span> <span class="o">=</span> <span class="n">covs</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">+</span> <span class="n">covs</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">+</span> \
             <span class="n">covs</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">*</span> <span class="n">covs</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Covariance matrix via direct computation</span><span class="se">\n</span><span class="si">{</span><span class="n">direct_cov</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance matrix via Newton-Girard
[[2.68965384 1.46812373 0.35503475]
 [1.34435512 2.3834466  1.89976853]
 [2.80927224 2.65321724 0.69253263]
 [1.49244366 1.39089004 0.32437561]
 [0.64391387 1.62276698 2.27971315]]

Covariance matrix via direct computation
[[2.68965384 1.46812373 0.35503475]
 [1.34435512 2.3834466  1.89976853]
 [2.80927224 2.65321724 0.69253263]
 [1.49244366 1.39089004 0.32437561]
 [0.64391387 1.62276698 2.27971315]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sampling-the-prior">
<h3>Sampling the prior<a class="headerlink" href="#sampling-the-prior" title="Permalink to this headline">¶</a></h3>
<p>We can now draw samples from the prior and visualise them, to illustrate the difference between the various different orders of interactions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of dimensions to use</span>
<span class="n">num_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Initialise first-order covariances</span>
<span class="n">covs</span> <span class="o">=</span> <span class="p">[</span><span class="n">EQKernel</span><span class="p">(</span><span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                 <span class="n">variance</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_dim</span><span class="p">)]</span>

<span class="c1"># Set variance for each order. We use a small variance for 2nd order</span>
<span class="c1"># interactions in order to better visualise the difference between</span>
<span class="c1"># 1st + 2nd order and 2nd order only samples</span>
<span class="n">variances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e0</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">])</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="c1"># Initialise additive covariance object</span>
<span class="n">add_cov</span> <span class="o">=</span> <span class="n">AdditiveCovariance</span><span class="p">(</span><span class="n">covs</span><span class="o">=</span><span class="n">covs</span><span class="p">,</span>
                             <span class="n">variances</span><span class="o">=</span><span class="n">variances</span><span class="p">,</span>
                             <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
<span class="c1"># Number of points on each grid side, number of samples to draw</span>
<span class="n">num_points</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Grid locations to plot</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">num_points</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">x_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_flat</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;$1^</span><span class="si">{st}</span><span class="s1">$ order interactions only&#39;</span><span class="p">,</span>
          <span class="s1">&#39;$2^</span><span class="si">{nd}</span><span class="s1">$ order interactions only&#39;</span><span class="p">,</span>
          <span class="s1">&#39;$1^</span><span class="si">{st}</span><span class="s1">$ and $2^</span><span class="si">{nd}</span><span class="s1">$ order interactions&#39;</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">14</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
    
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">titles</span><span class="p">):</span>
    
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cov</span> <span class="o">=</span> <span class="n">add_cov</span><span class="p">(</span><span class="n">x_flat</span><span class="p">,</span> <span class="n">x_flat</span><span class="p">,</span> <span class="n">Kmin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">Kmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            
        <span class="k">elif</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">cov</span> <span class="o">=</span> <span class="n">add_cov</span><span class="p">(</span><span class="n">x_flat</span><span class="p">,</span> <span class="n">x_flat</span><span class="p">,</span> <span class="n">Kmin</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">Kmax</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cov</span> <span class="o">=</span> <span class="n">add_cov</span><span class="p">(</span><span class="n">x_flat</span><span class="p">,</span> <span class="n">x_flat</span><span class="p">)</span>
            
        <span class="n">cov</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span> <span class="p">(</span><span class="n">num_points</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_points</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">cov_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span> <span class="o">+</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>

        <span class="n">sample</span> <span class="o">=</span> <span class="n">cov_chol</span> <span class="o">@</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_points</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">num_points</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
            
        <span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">sample</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_zticks</span><span class="p">([])</span>
        
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/addgp_12_0.svg" src="../../../_images/addgp_12_0.svg" /></div>
</div>
<p>We observe that the function draws from the GP with first-order interactions only (left column) look like the sum of two univariate functions, one in <span class="math notranslate nohighlight">\(x_1\)</span> and another in <span class="math notranslate nohighlight">\(x_2\)</span>. Using second-order interactions only produces samples looking like bivariate functions (middle). The additive covariance involving both first and second order interactions produces samples which are the sum of three functions, one in <span class="math notranslate nohighlight">\(x_1\)</span> another in <span class="math notranslate nohighlight">\(x_2\)</span> and a last one in <span class="math notranslate nohighlight">\(x_1, x_2\)</span>. Additive covariances could be able to better describe data which exhibit low-order interactions and additive mixtures of interactions of different orders.</p>
</div>
<div class="section" id="training-and-inference">
<h3>Training and inference<a class="headerlink" href="#training-and-inference" title="Permalink to this headline">¶</a></h3>
<p>In practice we want to learn the variance parameters <span class="math notranslate nohighlight">\(\sigma_1^2, ..., \sigma_D^2\)</span> and the noise level <span class="math notranslate nohighlight">\(\sigma^2_n\)</span> from the data. One especially attractive reason to do this, beyond improving our model’s fit to the data, is that the inferred variance parameters provide an interpretable metric for the relative magnitude of the contribution of each degree of interaction to the function being modelled. So for example, if we fit the model and discover that one of the variance parameters dominates the others, we know that the data can be well explained by interactions of the corresponding order. This may be a very useful interpretative tool.</p>
<p>The method we use to learn the parameters will be the usual maximum marginal likelihood estimation. The helper <code class="docutils literal notranslate"><span class="pre">marginal_likelihood</span></code> computes this, while the <code class="docutils literal notranslate"><span class="pre">gp_pred_post</span></code> computes the posterior predictive of the distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow_probability</span> <span class="k">as</span> <span class="nn">tfp</span>
<span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span>

<span class="k">def</span> <span class="nf">marginal_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">covariance</span><span class="p">):</span>
    
    <span class="c1"># Covariance dtype</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">covariance</span><span class="o">.</span><span class="n">dtype</span>
    
    <span class="c1"># Use a zero mean prior</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    
    <span class="c1"># Compute the covariance between the datapoints and choleksy-factorise</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">cov_chol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    
    <span class="c1"># Compute the marginal likelihood</span>
    <span class="n">distribution</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalTriL</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">cov_chol</span><span class="p">)</span>
    <span class="n">marginal_likelihood</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">marginal_likelihood</span>


<span class="k">def</span> <span class="nf">gp_pred_post</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x_</span><span class="p">,</span> <span class="n">covariance</span><span class="p">):</span>
    
    <span class="c1"># Compute covariances between training inputs x and prediction inputs x_</span>
    <span class="n">Kxx</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">Kxx_</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">Kx_x</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">Kx_x_</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">x_</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Compute predictive mean and covariance</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">Kx_x</span> <span class="o">@</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Kxx</span><span class="p">,</span> <span class="n">y</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]))[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">Kx_x_</span> <span class="o">-</span> <span class="n">Kx_x</span> <span class="o">@</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Kxx</span><span class="p">,</span> <span class="n">Kxx_</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="two-dimensional-example">
<h2>Two-dimensional example<a class="headerlink" href="#two-dimensional-example" title="Permalink to this headline">¶</a></h2>
<p>We can now apply additive GPs to some synthetic data. We’ll generate data at random from the sum of two sinusoids</p>
<div class="math notranslate nohighlight">
\[\begin{align}
f(x_1, x_2) = \text{cos}(2\pi x_1) + \text{cos}(2\pi x_2),
\end{align}\]</div>
<p>then fit an additive GP to this data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Number of training points</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c1"># Lengthscale and variance of base kernels</span>
<span class="n">num_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">lengthscale</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">cov_variance</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">order_variance</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="n">num_data</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Initialise first-order covariances</span>
<span class="n">covs</span> <span class="o">=</span> <span class="p">[</span><span class="n">EQKernel</span><span class="p">(</span><span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">,</span>
                 <span class="n">variance</span><span class="o">=</span><span class="n">cov_variance</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_dim</span><span class="p">)]</span>

<span class="c1"># Set variance for each order</span>
<span class="n">variances</span> <span class="o">=</span> <span class="n">order_variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_dim</span><span class="p">,))</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="c1"># Initialise additive covariance object</span>
<span class="n">add_cov</span> <span class="o">=</span> <span class="n">AdditiveCovariance</span><span class="p">(</span><span class="n">covs</span><span class="o">=</span><span class="n">covs</span><span class="p">,</span>
                             <span class="n">variances</span><span class="o">=</span><span class="n">variances</span><span class="p">,</span>
                             <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Number of training steps and optimiser</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">optimiser</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">)</span>

<span class="c1"># Optimiser additive GP hyperparameters</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        
        <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">add_cov</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">marginal_likelihood</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">add_cov</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">num_data</span>
        
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">add_cov</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimiser</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">add_cov</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of points on each side of the grid</span>
<span class="n">num_points</span> <span class="o">=</span> <span class="mi">40</span>

<span class="c1"># Create grid to predict on</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">num_points</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">x_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_flat</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># True values of the latent function on the grid</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_flat</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_flat</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">num_points</span><span class="p">))</span>

<span class="c1"># Predictive posterior on the grid</span>
<span class="n">pred_mean</span><span class="p">,</span> <span class="n">pred_cov</span> <span class="o">=</span> <span class="n">gp_pred_post</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">x_flat</span><span class="p">,</span> <span class="n">add_cov</span><span class="p">)</span>

<span class="c1"># Reshape the predictive mean and marginal variance</span>
<span class="n">pred_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pred_mean</span><span class="p">,</span> <span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">num_points</span><span class="p">))</span>
<span class="n">pred_cov</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">diag_part</span><span class="p">(</span><span class="n">pred_cov</span><span class="p">),</span> <span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">num_points</span><span class="p">))</span>

<span class="c1"># Figure to plot on</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Plot data and predictive mean</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">pred_mean</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Predictive mean&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="c1"># Plot data and predictive variance</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">pred_cov</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Uncertainty $\sigma$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="c1"># Plot ground truth</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/addgp_18_0.svg" src="../../../_images/addgp_18_0.svg" /></div>
</div>
<p>We observe that with rather few datapoints, the model has been able to make sensible and calibrated predictions outside the region of the observed data. For example, the model has made both accurate and precise predictions at the <span class="math notranslate nohighlight">\((1, -1)\)</span> region, even though it has not observed any data there. The model has able to extrapolate outside the training range because it has uncovered the fact that the function is the sum of two univariate functions, which should evident from the variances that it has learnt.</p>
<p>To obtain the variance magnitude of each order, we can compute the covariance function of a zero-input with itself, and return the covariance contributions from each order separately. We observe that the model has discovered that the data is best descibed as the sum of two univariate functions.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="n">add_cov</span><span class="p">(</span><span class="n">zero</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">individual_orders</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Combined variances: 1st-order </span><span class="si">{</span><span class="n">sigma1</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> and 2nd-order </span><span class="si">{</span><span class="n">sigma2</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Combined variances: 1st-order 1.6385 and 2nd-order 0.1870.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="example-on-concrete-data">
<h2>Example on concrete data<a class="headerlink" href="#example-on-concrete-data" title="Permalink to this headline">¶</a></h2>
<p>We have tried additive GPs on two-dimensional data, but really the regime of interest is higher-dimensional data. In higher dimensions, we do not have a strong prior for the kinds of interactions between the input variables, nor can we easily visualise it. We’ll apply additive GPs to the <a class="reference external" href="http://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength">UCI concrete compressive strength dataset</a>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load pre-downloaded concrete dataset</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;data/concrete/x.npy&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;data/concrete/y.npy&#39;</span><span class="p">)</span>

<span class="c1"># Normalise inputs and outputs</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="c1"># Number of trainint datapoints</span>
<span class="n">num_train</span> <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># Training data</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">num_train</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">num_train</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Number of dimensions of the dataset</span>
<span class="n">num_dim</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Initialise lengthscale to 1, and set variance so that</span>
<span class="c1"># the total variance of the additive GP is 1</span>
<span class="n">lengthcale</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">cov_variance</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">order_variance</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="c1"># Initialise first-order covariances</span>
<span class="n">covs</span> <span class="o">=</span> <span class="p">[</span><span class="n">EQKernel</span><span class="p">(</span><span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">,</span>
                 <span class="n">variance</span><span class="o">=</span><span class="n">cov_variance</span><span class="p">,</span>
                 <span class="n">dim</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_dim</span><span class="p">)]</span>

<span class="c1"># Set variance for each order</span>
<span class="n">order_variances</span> <span class="o">=</span> <span class="n">order_variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_dim</span><span class="p">,))</span>

<span class="c1"># Initialise additive covariance object</span>
<span class="n">add_cov</span> <span class="o">=</span> <span class="n">AdditiveCovariance</span><span class="p">(</span><span class="n">covs</span><span class="o">=</span><span class="n">covs</span><span class="p">,</span>
                             <span class="n">variances</span><span class="o">=</span><span class="n">order_variances</span><span class="p">,</span>
                             <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Number of training steps and optimiser</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">optimiser</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">)</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Optimiser additive GP hyperparameters</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        
        <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">add_cov</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">marginal_likelihood</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">add_cov</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">num_train</span>
        
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">add_cov</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimiser</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">add_cov</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Marginal log-likelihood&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Step #&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;nats&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/addgp_23_0.svg" src="../../../_images/addgp_23_0.svg" /></div>
</div>
<p>We can now have a look at the variances learnt by the model as a means of interpreting it. Remembering that the <span class="math notranslate nohighlight">\(d^{th}\)</span> order interaction contains <span class="math notranslate nohighlight">\({D}\choose{d}\)</span> terms and multiplying the variances accordingly, we get an estimate for the relative importance of each order.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">combined_variances</span> <span class="o">=</span> <span class="n">add_cov</span><span class="p">(</span><span class="n">zero</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">individual_orders</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">combined_variances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">combined_variances</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Combined variances: </span><span class="si">{</span><span class="n">combined_variances</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Combined variances: [3.71 5.35 4.3  2.46 1.05 0.3  0.05 0.  ].
</pre></div>
</div>
</div>
</div>
<p>Thus we observe that the data is much better described by low-order interactions between the variables, than high-order interactions.</p>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>Additive GPs provide a way for placing priors over functions involving cross-variable interactions between the input variables. Additive GPs place priors involving any order of interaction that is desirable and automatically learn the importance of each order, directly from the data. Naively computing with the covariance requires exponentially many operations in the number of dimensions, however using the Newton-Girard formulae brings the cost of computing the covariance to <span class="math notranslate nohighlight">\(\mathcal{O}(D^2)\)</span>. Additive GPs can be effective in practical situations where we have little prior knowledge about the way the input variables interact.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-content/misc/addgp/addgp-0"><dl class="citation">
<dt class="bibtex label" id="duvenaud2011additive"><span class="brackets"><a class="fn-backref" href="#id2">DNR11</a></span></dt>
<dd><p>David K Duvenaud, Hannes Nickisch, and Carl Rasmussen. Additive gaussian processes. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger, editors, <em>Advances in Neural Information Processing Systems</em>, volume 24. Curran Associates, Inc., 2011. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2011/file/4c5bde74a8f110656874902f07378009-Paper.pdf">https://proceedings.neurips.cc/paper/2011/file/4c5bde74a8f110656874902f07378009-Paper.pdf</a>.</p>
</dd>
<dt class="bibtex label" id="rasmussen2003gaussian"><span class="brackets"><a class="fn-backref" href="#id1">Ras03</a></span></dt>
<dd><p>Carl Edward Rasmussen. Gaussian processes in machine learning. In <em>Summer school on machine learning</em>, 63–71. Springer, 2003.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "venv-random-walks"
        },
        kernelOptions: {
            kernelName: "venv-random-walks",
            path: "./content/misc/addgp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'venv-random-walks'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../svgd/svgd.html" title="previous page">Stein variational gradient descent</a>
    <a class='right-next' id="next-link" href="../../reading-and-links.html" title="next page">Interesting reading and websites</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratos Markou<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>