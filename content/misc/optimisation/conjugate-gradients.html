
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Conjugate gradients &#8212; Random walks</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://random-walks.org/content/misc/optimisation/conjugate-gradients.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="The Kalman filter and smoother" href="../kalman/kalman.html" />
    <link rel="prev" title="Neural ODEs" href="../node/node.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://random-walks.org/content/misc/optimisation/conjugate-gradients.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Conjugate gradients" />
<meta property="og:description" content="Conjugate gradients  Conjugate Gradients (CG) is a widespread optimisation method that is applicable to continuous optimisation problems where the first-order d" />
<meta property="og:image"       content="https://random-walks.org/_static/logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Random walks</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../home.html">
   Random walks
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../prob-intro/intro.html">
   Probability: An introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch01/content.html">
     Events and Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch02/content.html">
     Discrete random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch03/content.html">
     Multivariate discrete distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch04/content.html">
     Probability generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch05/content.html">
     Distribution and density functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch06/content.html">
     Multivariate distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch07/content.html">
     Moment generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch08/content.html">
     Main limit theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch09/content.html">
     Branching processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch10/content.html">
     Random walks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch11/content.html">
     Processes in continuous time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../prob-intro/ch12/content.html">
     Markov chains
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../cvx/intro.html">
   Convex optimisation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch01.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch02.html">
     Convex sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../cvx/ch03.html">
     Convex functions
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../gp/gp-intro.html">
   Gaussian Processes
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../gp/why-covariances.html">
     Why covariance functions?
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../gp/sparse/sparse-intro.html">
     Sparse Gaussian Processes
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/vfe.html">
       Variational Free Energy GPs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/gp-sampling.html">
       Sampling GP posteriors
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../gp/vfer/vfer.html">
     Variational Inference Revisited
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../misc.html">
   Miscellaneous
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../sde/num-sde.html">
     Numerical simulation of SDEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sde-as-gp/sde-as-gp.html">
     VI for diffusion processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../node/node.html">
     Neural ODEs
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Conjugate gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../kalman/kalman.html">
     The Kalman filter and smoother
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ncs/ncs.html">
     Natural cubic splines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ars/ars.html">
     Adaptive rejection sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../score-matching/score-matching.html">
     Estimation by score matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interacting/interacting.html">
     Interacting particle FPK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rff/rff.html">
     Random Fourier features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../svgd/svgd.html">
     Stein variational gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../addgp/addgp.html">
     Additive Gaussian Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gumbel/gumbel.html">
     Gumbel distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gip/gip.html">
     Global inducing points for BNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ais/ais.html">
     Annealed Importance Sampling
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reading-and-links.html">
   Interesting reading and websites
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/misc/optimisation/conjugate-gradients.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/stratisMarkou/random-walks/master?urlpath=tree/./content/misc/optimisation/conjugate-gradients.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivating-conjugate-gradients">
   Motivating Conjugate Gradients
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#steepest-descent">
     Steepest Descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conjugate-directions">
     Conjugate directions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Conjugate Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonlinear-conjugate-gradients">
   Nonlinear Conjugate Gradients
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#line-search">
     Line search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="conjugate-gradients">
<h1>Conjugate gradients<a class="headerlink" href="#conjugate-gradients" title="Permalink to this headline">¶</a></h1>
<p>Conjugate Gradients (CG) is a widespread optimisation method that is applicable to continuous optimisation problems where the first-order derivatives of the objective are available. These notes are roughly based off of Jonathan R. Shewchuk’s<a class="bibtex reference internal" href="#shewchuk" id="id1">[She94]</a> notes on CG.</p>
<div class="section" id="motivating-conjugate-gradients">
<h2>Motivating Conjugate Gradients<a class="headerlink" href="#motivating-conjugate-gradients" title="Permalink to this headline">¶</a></h2>
<p>We motivate CG by first discussing the Steepest Descent and Conjugate Direction methods, in the context of minimising quadratic functions of the form</p>
<div class="math notranslate nohighlight">
\[ \text{minimise}~~~F(x) = \frac{1}{2} x^\top A x - b^\top x,\]</div>
<p>where <span class="math notranslate nohighlight">\(x, b \in \mathbb{R}^D\)</span>, <span class="math notranslate nohighlight">\(A\)</span> is a symmmetric positive semi-definite matrix. With minor modifications, CG is also applicable to non-quadratic problems.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define objective function</span>
<span class="k">def</span> <span class="nf">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    
    <span class="n">F_</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...i, ij, ...j -&gt; ...&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">F_</span> <span class="o">=</span> <span class="n">F_</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">F_</span>


<span class="k">def</span> <span class="nf">plot_F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">show</span><span class="p">):</span>
    
    <span class="n">num_points</span> <span class="o">=</span> <span class="mi">20</span>
    
    <span class="n">x1_lin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    <span class="n">x2_lin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    
    <span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_lin</span><span class="p">,</span> <span class="n">x2_lin</span><span class="p">)</span>
    <span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">F_</span> <span class="o">=</span> <span class="n">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x_grid</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    
    <span class="n">levels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">,</span> <span class="n">F_</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">levels</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">show</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># A and b for defining the quadratic form</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>

<span class="n">plot_F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Objective $F$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/conjugate-gradients_2_0.svg" src="../../../_images/conjugate-gradients_2_0.svg" /></div>
</div>
<div class="section" id="steepest-descent">
<h3>Steepest Descent<a class="headerlink" href="#steepest-descent" title="Permalink to this headline">¶</a></h3>
<p>We can try solving the quadratic problem above iteratively, by starting from an initial guess <span class="math notranslate nohighlight">\(x = x_0\)</span> and repeatedly taking steps towards the direction of steepest decrease of <span class="math notranslate nohighlight">\(F\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{align}
x_{n + 1} = x_n + \alpha_n g_n, \text{ where } g_n = \nabla F(x_n).
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_n\)</span> is an appropriately chosen step size. For quadratic problems, we can choose the <span class="math notranslate nohighlight">\(\alpha_n\)</span> which minimises</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\DeclareMathOperator*{\argmin}{arg\,min}
\alpha_n = \argmin_\alpha F(x_n + \alpha g_n) = - \frac{g_n^\top g_n}{g_n^\top A g_n},
\end{align}\]</div>
<p>in closed form - see derivation below.</p>
<details class="proof">
<summary>Proof: Optimal single-step size for steepest descent</summary>
<p>The step size <span class="math notranslate nohighlight">\(\alpha_n\)</span> that minimises the objective in the direction of <span class="math notranslate nohighlight">\(g_n\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\DeclareMathOperator*{\argmin}{arg\,min}
\alpha_n = \argmin_\alpha F(x_n + \alpha g_n)
\end{align}\]</div>
<p>and satisfies</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\frac{d}{d\alpha_n} F(x_n + \alpha_n g_n) = 0 \implies A(x_n + \alpha_n g_n) - b = 0.
\end{align}\]</div>
<p>Taking the inner product with <span class="math notranslate nohighlight">\(g_n\)</span> and rearranging we arrive at</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\alpha_n = - \frac{g_n^\top g_n}{g_n^\top A g_n}.
\end{align}\]</div>
</details>
<br>
<p>The issue with this algorithm, as illustrated by the following example, is that when there is a large difference between the largest and smallest eigenvalues of <span class="math notranslate nohighlight">\(A\)</span>, the algorithm can jump around instead of moving towards the minimum.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quadratic_steepest_descent</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">):</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">x_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
    <span class="n">F_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">)]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        
        <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span>
        
        <span class="n">gT_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
        <span class="n">gT_A_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i, ij, j -&gt; &#39;</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
        
        <span class="n">a</span> <span class="o">=</span> <span class="o">-</span> <span class="n">gT_g</span> <span class="o">/</span> <span class="n">gT_A_g</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">a</span> <span class="o">*</span> <span class="n">g</span>
        
        <span class="n">x_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">F_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_hist</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">F_hist</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A and b for defining the quadratic form</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>

<span class="c1"># Initial guess</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>

<span class="c1"># Number of steps of steepest descent</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Execute steepest descent algorithm</span>
<span class="n">x_hist</span><span class="p">,</span> <span class="n">F_hist</span> <span class="o">=</span> <span class="n">quadratic_steepest_descent</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">)</span>

<span class="c1"># Plot the loss function</span>
<span class="n">plot_F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Plot path taken by steepest descent</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hist</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_hist</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Steepest descent&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/conjugate-gradients_5_0.svg" src="../../../_images/conjugate-gradients_5_0.svg" /></div>
</div>
</div>
<div class="section" id="conjugate-directions">
<h3>Conjugate directions<a class="headerlink" href="#conjugate-directions" title="Permalink to this headline">¶</a></h3>
<p>How can we fix this problem where the optimiser taking large steps in opposite directions? One idea would be to constrain the search directions to be orthogonal to each other. For example, we could pick the basis set of <span class="math notranslate nohighlight">\(x\)</span>, say <span class="math notranslate nohighlight">\(\{u_0, u_1, ..., u_D\}\)</span> as the search directions and take steps in each direction by finding the step size <span class="math notranslate nohighlight">\(\alpha_n\)</span> that minimises <span class="math notranslate nohighlight">\(F\)</span> along the search direction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quadratic_orthogonal_descent</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">):</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">x_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
    <span class="n">F_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">)]</span>
    
    <span class="n">num_dim</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        
        <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span>
        
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_dim</span><span class="p">)</span>
        <span class="n">u</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="n">num_dim</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        
        <span class="n">gT_u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>
        <span class="n">uT_A_u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i, ij, j -&gt; &#39;</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>
        
        <span class="n">a</span> <span class="o">=</span> <span class="o">-</span> <span class="n">gT_u</span> <span class="o">/</span> <span class="n">uT_A_u</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">a</span> <span class="o">*</span> <span class="n">u</span>
        
        <span class="n">x_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">F_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_hist</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">F_hist</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Perhaps this sounds like a good idea, but it isn’t. As illustrated below, the optimisation process can still jump around a great deal, and converge to the minimum very slowly. This occurs because although the search directions are orthogonal to each other, the fact that the eigenvectors of <span class="math notranslate nohighlight">\(A\)</span> are at an angle to the search directions prevent the algorithm from making large steps.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A and b for defining the quadratic form</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>

<span class="c1"># Initial guess</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>

<span class="c1"># Number of steps of descent</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Execute descent with orthogonal search directions</span>
<span class="n">x_hist</span><span class="p">,</span> <span class="n">F_hist</span> <span class="o">=</span> <span class="n">quadratic_orthogonal_descent</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">)</span>

<span class="c1"># Plot the loss function</span>
<span class="n">plot_F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Plot path taken by orthogonal search directions</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hist</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_hist</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Descent with orthogonal search directions&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/conjugate-gradients_9_0.svg" src="../../../_images/conjugate-gradients_9_0.svg" /></div>
</div>
<p>If the search directions were orthogonal and <span class="math notranslate nohighlight">\(A\)</span> was a diagonal matrix, then the search would reach the optimum within <span class="math notranslate nohighlight">\(D\)</span> steps - see the example below.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A and b for defining the quadratic form</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>

<span class="c1"># Initial guess</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>

<span class="c1"># Number of steps of steepest descent</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Execute descent with orthogonal search directions</span>
<span class="n">x_hist_ortho</span><span class="p">,</span> <span class="n">F_hist_ortho</span> <span class="o">=</span> <span class="n">quadratic_orthogonal_descent</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">)</span>

<span class="c1"># Plot the loss function</span>
<span class="n">plot_F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Plot path taken by orthogonal search directions</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hist_ortho</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist_ortho</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_hist_ortho</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist_ortho</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Descent with orthogonal search directions&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/conjugate-gradients_11_0.svg" src="../../../_images/conjugate-gradients_11_0.svg" /></div>
</div>
<p>This points at the idea of choosing search directions <span class="math notranslate nohighlight">\(\{d_0, d_1, ..., d_{D - 1}\}\)</span> which are orthogonal in the space spanned by the eigenvectors of <span class="math notranslate nohighlight">\(A\)</span>. Any such set of vectors must be <span class="math notranslate nohighlight">\(A\)</span>-orthogonal in the sense</p>
<div class="math notranslate nohighlight">
\[\begin{align}
d_i A d_j = 0, \text{ if } i \neq j.
\end{align}\]</div>
<p>How can achieve <span class="math notranslate nohighlight">\(A\)</span>-orthogonality? Given any basis set, say <span class="math notranslate nohighlight">\(\{u_0, u_1, ..., u_{D - 1}\}\)</span> we can prodce another basis that is <span class="math notranslate nohighlight">\(A\)</span>-orthogonal, by removing from each basis vector all components that are not <span class="math notranslate nohighlight">\(A\)</span>-orthogonal to any preceeding vectors</p>
<div class="math notranslate nohighlight">
\[\begin{align}
v_i = u_i + \sum_{j = 1}^{i - 1} \beta_{ij} v_j.
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(v_0 = u_0\)</span> and <span class="math notranslate nohighlight">\(\beta_{ij}\)</span> are appropriately chosen constants such that <span class="math notranslate nohighlight">\(d_i A d_j = 0\)</span> whenever <span class="math notranslate nohighlight">\(i \neq j\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{align}
\beta_{ij} = - \frac{u_i^\top A v_j}{v_j^\top A v_j}.
\end{align}\]</div>
<details class="proof">
<summary>Proof: Deriving the \(\beta_{ij}\) coefficients</summary>
<p>Starting from the expression</p>
<div class="math notranslate nohighlight">
\[\begin{align}
v_i = u_i + \sum_{j = 1}^{i - 1} \beta_{ij} v_j,
\end{align}\]</div>
<p>take the product with <span class="math notranslate nohighlight">\(v_j^\top A\)</span> to obtain</p>
<div class="math notranslate nohighlight">
\[\begin{align}
v_j^\top A v_i = v_j^\top A u_i + \sum_{k = 1}^{i - 1} \beta_{ik} v_j^\top A v_k.
\end{align}\]</div>
<p>Using the requirement that <span class="math notranslate nohighlight">\(v_j^\top A v_k = 0\)</span> if <span class="math notranslate nohighlight">\(j \neq k\)</span> and rearranging we arrive at</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\beta_{ij} = -\frac{u_i^\top A v_j}{v_j^\top A v_j}.
\end{align}\]</div>
</details>
<br>
<p>This is called a <em>conjugate Gram-Schmidt process</em>. Its disadvantage is that all previous search vectors must be kept in memory to generate a new search vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quadratic_gram_schmidt</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">x_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
    <span class="n">F_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">)]</span>
    
    <span class="n">num_dim</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Create orthogonal u basis</span>
    <span class="n">U</span> <span class="o">=</span> <span class="p">[</span><span class="n">u</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_dim</span><span class="p">)]</span>
    
    <span class="n">D</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Execute descent</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_dim</span><span class="p">):</span>
        
        <span class="c1"># Gradient wrt x and search direction</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span>
        
        <span class="n">d_i</span> <span class="o">=</span> <span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
            
            <span class="n">b_ij</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">D</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
            <span class="n">b_ij</span> <span class="o">=</span> <span class="n">b_ij</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">D</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">D</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
            
            <span class="n">d_i</span> <span class="o">=</span> <span class="n">d_i</span> <span class="o">+</span> <span class="n">b_ij</span> <span class="o">*</span> <span class="n">D</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            
        <span class="n">D</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d_i</span><span class="p">)</span>
        
        <span class="n">gT_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">d_i</span><span class="p">)</span>
        <span class="n">dT_A_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i, ij, j -&gt; &#39;</span><span class="p">,</span> <span class="n">d_i</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">d_i</span><span class="p">)</span>
        
        <span class="n">a</span> <span class="o">=</span> <span class="o">-</span> <span class="n">gT_d</span> <span class="o">/</span> <span class="n">dT_A_d</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">a</span> <span class="o">*</span> <span class="n">d_i</span>
        
        <span class="n">x_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">F_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_hist</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">F_hist</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A and b for defining the quadratic form</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>

<span class="c1"># Initial guess</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>

<span class="c1"># Number of steps of steepest descent</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Execute descent with Gram-Schmidt orthogonalised directions</span>
<span class="n">x_hist_gram_schmidt</span><span class="p">,</span> <span class="n">F_hist_gram_schmidt</span> <span class="o">=</span> <span class="n">quadratic_gram_schmidt</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">)</span>

<span class="c1"># Plot the loss function</span>
<span class="n">plot_F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Plot path taken using Gram-Schmidt orthogonalised directions</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hist_gram_schmidt</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist_gram_schmidt</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_hist_gram_schmidt</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist_gram_schmidt</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Descent with Gram-Schmidt&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/conjugate-gradients_14_0.svg" src="../../../_images/conjugate-gradients_14_0.svg" /></div>
</div>
<p>For quadratic <span class="math notranslate nohighlight">\(F\)</span>, conjugate directions (with Gram-Schmidt) converges in <span class="math notranslate nohighlight">\(D\)</span> steps, where <span class="math notranslate nohighlight">\(D\)</span> is the dimensionality of the problem. However, Gram-Schmidt must keep track of all previous search directions and adjust the next search direction to be <span class="math notranslate nohighlight">\(A\)</span>-orthogonal to them, which incurs a memory as well as a computational cost. This brings us to the method Conjugate Gradients (CG), which fixes this issue by picking sensible search directions <span class="math notranslate nohighlight">\(d_0, d_1, ...\)</span>, leveraging the fact that <span class="math notranslate nohighlight">\(f\)</span> is a quadratic form.</p>
</div>
</div>
<div class="section" id="id2">
<h2>Conjugate Gradients<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>Our choice of setting <span class="math notranslate nohighlight">\(u_0\)</span> equal to the basis was an arbitrary decision to get the algorithm started, but is not at all sensible. For one, if we use this choice then the initial search direction <span class="math notranslate nohighlight">\(d_0\)</span> does not at all depend on <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(b\)</span> at all - surely there must be a better initial condition. Consider instead setting</p>
<div class="math notranslate nohighlight">
\[ u_n = -g_n = \nabla F(x_n) = -(Ax_n - b). \]</div>
<p>This seems more sensible, because it at least takes into account local information about <span class="math notranslate nohighlight">\(F\)</span> by means of the gradient. However, it turns out that this decision brings more benefits than is immediately obvious. Under the choice <span class="math notranslate nohighlight">\(u_n = -g_n\)</span>, the Gram-Schmidt conjugation step becomes</p>
<div class="math notranslate nohighlight">
\[d_{n + 1} = - g_{n + 1} + \sum_{k = 1}^n \beta_{nk} d_k,\]</div>
<p>while the transition rule still has the form</p>
<div class="math notranslate nohighlight">
\[\begin{align}
x_{n + 1} &amp;= x_n + \alpha_n d_n.
\end{align}\]</div>
<p>Because <span class="math notranslate nohighlight">\(\nabla F(x_n) = -(Ax_n - b)\)</span>, the gradient also evolves according to</p>
<div class="math notranslate nohighlight">
\[\begin{align}
g_{n + 1} = g_n + \alpha_n A d_n.
\end{align}\]</div>
<p>It can be shown (see below) that all Gram-Schmidt coeffiecients appart from <span class="math notranslate nohighlight">\(\beta_{n, n - 1}\)</span> vanish, giving the iterative step</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
x_{n + 1} &amp;= x_n + \alpha_n d_n, &amp;&amp; \text{ where } \alpha_n = - \frac{d_n^\top g_n}{d_n^\top Ad_n},\\
g_{n + 1} &amp;= g_n + \alpha_n A d_n, \\
d_{n + 1} &amp;= - g_{n + 1} + \beta_n d_n, &amp;&amp; \text{ where } \beta_n = \frac{g_{n + 1}^\top A d_n}{d_n^\top Ad_n}.
\end{align}\end{split}\]</div>
<details class="proof">
<summary>Proof: The Gram-Schmidt coefficients for Conjugate Gradients</summary>
<p>Suppose we set <span class="math notranslate nohighlight">\(u_n = -g_n\)</span>, start from <span class="math notranslate nohighlight">\(x = x_0\)</span> and evolve according to the transition rules</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
x_{n + 1} &amp;= x_n + \alpha_n d_n, \\
g_{n + 1} &amp;= g_n + \alpha_n A d_n, &amp;&amp; \text{ where } \alpha_n = - \frac{d_n^\top g_n}{d_n^\top Ad_n}, \\
d_{n + 1} &amp;= - g_{n + 1} + \sum_{k = 1}^n \beta_{nk} d_k,
\end{align}\end{split}\]</div>
<p>where the constants <span class="math notranslate nohighlight">\(\beta_{nk}\)</span> are chosen so that the <span class="math notranslate nohighlight">\(d_n\)</span> vectors are all <span class="math notranslate nohighlight">\(A\)</span>-orthogonal. Defining the spanning set</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathcal{D}_{n + 1} &amp;= \text{span}~(d_0, d_1, d_2, ..., d_n),
\end{align}\]</div>
<p>we can see that minimising the objective by minimising the objective along each of the search directionsin turn, is equivalent to minimising the objective jointly over <span class="math notranslate nohighlight">\(x_0 + \mathcal{D}_{n + 1}\)</span>, because the vectors <span class="math notranslate nohighlight">\(\{d_0, d_1, d_2, ..., d_n\}\)</span> are <span class="math notranslate nohighlight">\(A\)</span>-orthogonal. Therefore, the gradient <span class="math notranslate nohighlight">\(g_{n + 1}\)</span> at <span class="math notranslate nohighlight">\(x = x_{n + 1}\)</span> is othogonal (not <span class="math notranslate nohighlight">\(A\)</span>-orthogonal but <span class="math notranslate nohighlight">\(I\)</span>-orthogonal) to the spanning set <span class="math notranslate nohighlight">\(\mathcal{D}_{n + 1}\)</span>, because otherwise the position <span class="math notranslate nohighlight">\(x_{n + 1}\)</span> would not be a minimiser of the objective within <span class="math notranslate nohighlight">\(x_0 + \mathcal{D}_{n + 1}\)</span>, which we can write as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
g_n \perp d_m \text{ for all } m &lt; n.
\end{align}\]</div>
<p>In addition we observe that the span <span class="math notranslate nohighlight">\(\mathcal{D}_{n + 1}\)</span> can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathcal{D}_{n + 1} &amp;= \text{span}~(d_0, d_1, d_2, ..., d_n) \\
              &amp;= \text{span}~(d_0, Ad_0, A^2 d_0, ..., A^{n - 1} d_n),
\end{align}\end{split}\]</div>
<p>where at any given step of the algorithm, the set is augmented from <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> to <span class="math notranslate nohighlight">\(\mathcal{D}_n + A\mathcal{D}_{n + 1}\)</span>. Since <span class="math notranslate nohighlight">\(A\mathcal{D}_n \subseteq \mathcal{D}_{n + 1}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\begin{align}
g_{n + 1} \perp \mathcal{D}_n \implies g_{n + 1} \perp A\mathcal{D}_{n - 1},
\end{align}\]</div>
<p>so <span class="math notranslate nohighlight">\(d_m^\top A g_{n + 1} = 0\)</span> for all <span class="math notranslate nohighlight">\(m &lt; n\)</span>. Using this we see that all Gram-Schmidt coefficients must vanish except for <span class="math notranslate nohighlight">\(\beta_{nn}\)</span>. We therefore call this coeffient <span class="math notranslate nohighlight">\(\beta_n\)</span> and compute its value as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\beta_n = \frac{g_{n + 1}^\top A d_n}{d_n^\top Ad_n}.
\end{align}\]</div>
<p><strong>Intuition:</strong> The vanishing of the Gramm-Schmidt coefficients relies on the fact that <span class="math notranslate nohighlight">\(g_{n + 1} \perp A\mathcal{D}_{n - 1}\)</span>, which follows from the facts that <span class="math notranslate nohighlight">\(g_{n + 1} \perp \mathcal{D}_n\)</span> and that <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> is a Krylov space</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathcal{D}_n = \text{span}~(d_0, Ad_0, A^2 d_0, ..., A^{n - 1} d_{n-1}).
\end{align}\]</div>
<p>The fact that <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> is a Krylov space is a consequence of choosing <span class="math notranslate nohighlight">\(u_n = -g_n\)</span>. If instead we had picked <span class="math notranslate nohighlight">\(u_n\)</span> to be one-hot vectors with a <span class="math notranslate nohighlight">\(1\)</span> in the <span class="math notranslate nohighlight">\(n^{th}\)</span> entry, then <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> would not be a Krylov space and the Gram-Schmidt coefficients would not in cancel in general.</p>
</details>
<br>
<p>Note how at each step we only have to compute a single Gram-Schmidt term <span class="math notranslate nohighlight">\(\beta_n A d_n\)</span>, and that we don’t need to keep all previous search directions <span class="math notranslate nohighlight">\(d_0\)</span> through to <span class="math notranslate nohighlight">\(d_{n - 1}\)</span> in memory anymore.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">conjugate_gradients</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">x_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
    <span class="n">F_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">)]</span>
    
    <span class="n">num_dim</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            
    <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span>
    <span class="n">d</span> <span class="o">=</span> <span class="o">-</span> <span class="n">g</span>
    
    <span class="c1"># Execute descent</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_dim</span><span class="p">):</span>
        
        <span class="n">alpha</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">d</span>
        <span class="n">x_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">F_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
        
        <span class="n">g</span> <span class="o">=</span> <span class="n">g</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

        <span class="n">beta</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
        <span class="n">d</span> <span class="o">=</span> <span class="o">-</span> <span class="n">g</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">d</span>
        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_hist</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">F_hist</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_center-output tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A and b for defining the quadratic form</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>

<span class="c1"># Initial guess</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>

<span class="c1"># Number of steps of steepest descent</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Execute descent with Conjugate Gradients</span>
<span class="n">x_hist_cg</span><span class="p">,</span> <span class="n">F_hist_cg</span> <span class="o">=</span> <span class="n">conjugate_gradients</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">)</span>

<span class="c1"># Plot the loss function</span>
<span class="n">plot_F</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Plot path taken by Conjugate Gradients</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hist_cg</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist_cg</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_hist_cg</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hist_cg</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Conjugate Gradients&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/conjugate-gradients_18_0.svg" src="../../../_images/conjugate-gradients_18_0.svg" /></div>
</div>
<p>We also apply each of these algorithms to a larger problem. We pick a random matrix <span class="math notranslate nohighlight">\(A\)</span> (which will almost surely be rank <span class="math notranslate nohighlight">\(d\)</span>) and make it positive-definite by setting <span class="math notranslate nohighlight">\(A \leftarrow A A^\top\)</span>. We also pich the vector <span class="math notranslate nohighlight">\(b\)</span> randomly. We optimise the quadratic objective using each method and compare their performance.</p>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># A and b for defining the quadratic form</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,))</span>

<span class="c1"># Initial guess</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,))</span>

<span class="c1"># Number of steps of steepest descent</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># Execute descent with orthogonal search directions</span>
<span class="n">_</span><span class="p">,</span> <span class="n">F_hist_steep</span> <span class="o">=</span> <span class="n">quadratic_steepest_descent</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">F_hist_ortho</span> <span class="o">=</span> <span class="n">quadratic_orthogonal_descent</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">F_hist_gs</span> <span class="o">=</span> <span class="n">quadratic_gram_schmidt</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">F_hist_cg</span> <span class="o">=</span> <span class="n">conjugate_gradients</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">)</span>


<span class="c1"># Figure for objectives</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot objective</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">F_hist_steep</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Steepest descent (SD)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">F_hist_ortho</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Orthogonal descent (OD)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">F_hist_gs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Conjugate directions with GS (CDGS)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">F_hist_cg</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Conjugate gradients (CG)&#39;</span><span class="p">)</span>

<span class="c1"># Plotting options</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Comparison of descent methods on random $A$, $b$ ($d = 20$)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Optimisation step&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Objective $F$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/conjugate-gradients_20_0.svg" src="../../../_images/conjugate-gradients_20_0.svg" /></div>
</div>
<p>Note that CDGS and CG produce the same solution. GS initially enjoys larger improvements because it moves closer to the direction of steepest descent. For the first steps, SD and CG have the same perfromance because they move along the same direction, but CG subsequently outperforms SD.</p>
</div>
<div class="section" id="nonlinear-conjugate-gradients">
<h2>Nonlinear Conjugate Gradients<a class="headerlink" href="#nonlinear-conjugate-gradients" title="Permalink to this headline">¶</a></h2>
<p>We derived and applied CG to a quadratic objective, but ultimately we are interested in non-quadratic problems. Although quadratic objectives are of limited interest, one notable application of CG is for solving linear equations of the form <span class="math notranslate nohighlight">\(Ax = b\)</span>, where <span class="math notranslate nohighlight">\(A\)</span> is a sparse matrix. In such cases, the computations involved in computing the conjugate directions can be sped up, improving the overall cost of the algorithm - compared to the <span class="math notranslate nohighlight">\(\mathcal{O}(D^3)\)</span> cost of Gaussian elimination.</p>
<p>To apply CG to a non-quadratic <span class="math notranslate nohighlight">\(F\)</span>, we must modify it in three places. First, and most importantly, what does it mean for the search directions to be conjugate, when the problem is non-quadratic and thus has a varying Hessian? One interpretation is that if we are sufficiently close to a local minimum, the objective will be roughly quadratic, so we can approximate <span class="math notranslate nohighlight">\(F\)</span> as a quadratic and apply CG. To achieve this, we could apply Gram-Schmidt to make each <span class="math notranslate nohighlight">\(d_n\)</span> <span class="math notranslate nohighlight">\(A\)</span>-orthogonal to all previous search directions. But we don’t want to give up the computational efficiency of the CG update rule either, so we will apply a rule that looks a lot like the CG update rule:</p>
<div class="math notranslate nohighlight">
\[\begin{align}
d_{n + 1} &amp;= - g_{n + 1} + \beta_n d_n, &amp;&amp; \text{ where } \beta_n = \frac{g_{n + 1}^\top (g_{n + 1} - g_n)}{g_n^\top g_n}.
\end{align}\]</div>
<p>This is called the Polak-Ribiere (PR) update rule and reduces to the exact update rule we derived earlier whenever <span class="math notranslate nohighlight">\(F\)</span> is quadratic - there exist variety of alternative such rules.<a class="bibtex reference internal" href="#conjugatemethods" id="id3">[HZ06]</a> Alternative update rules may be equivalent when when the objective is quadratic, they are not equivalent when the objective is non-quadratic. The choice of update rule makes a difference in this case, however we will not focus on this here and work with PR from now on.</p>
<p>Second, we have to solve for <span class="math notranslate nohighlight">\(\alpha_n\)</span> approximately. When <span class="math notranslate nohighlight">\(F\)</span> was known and quadratic we obtained the optimal step size in closed form</p>
<div class="math notranslate nohighlight">
\[ \alpha_n = - \frac{d_n^\top g_n}{d_n^\top Ad_n}, \]</div>
<p>however in most cases of interest this will not be possible, either because the function <span class="math notranslate nohighlight">\(F\)</span> is not available to us, or because the solution simply does not exist in closed form even if the analytic form of <span class="math notranslate nohighlight">\(F\)</span> was available. We therefore will have to approximately solve the optimisation problem</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\alpha_n = \argmin_{\alpha \geq 0} F(x_n + \alpha d_n).
\end{align}\]</div>
<p>This optimisation is called a line search problem because it amounts to searching for a minimiser along the line <span class="math notranslate nohighlight">\(x_n + \alpha d_n\)</span>. Lastly, in quadratic problems we updated the gradient according to</p>
<div class="math notranslate nohighlight">
\[\begin{align}
g_{n + 1} = \nabla F(x_{n + 1}) = \nabla F(x_n) + \alpha_n A d_n = g_n + \alpha_n A d_n.
\end{align}\]</div>
<p>The change in the gradient was a linear transformation of the search direction. That’s because <span class="math notranslate nohighlight">\(F\)</span> was quadratic, so its gradient was affine in <span class="math notranslate nohighlight">\(x\)</span>. This is no longer true, so instead we re-compute the gradient at each step</p>
<div class="math notranslate nohighlight">
\[\begin{align}
g_{n + 1} = \nabla F(x_{n + 1}).
\end{align}\]</div>
<p>In particular, we assume that the objective function <span class="math notranslate nohighlight">\(F\)</span> returns both its value as well as the partial derivatives w.r.t. <span class="math notranslate nohighlight">\(x\)</span> at the point being queried. Next we discuss solving the line search problem.</p>
<div class="section" id="line-search">
<h3>Line search<a class="headerlink" href="#line-search" title="Permalink to this headline">¶</a></h3>
<p>For a quadratic objective <span class="math notranslate nohighlight">\(F\)</span>, we can obtain the minimiser <span class="math notranslate nohighlight">\(\alpha_n\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{align}
\alpha_n = \argmin_{\alpha \geq 0} F(x_n + \alpha d_n)
\end{align}\]</div>
<p>in closed form. However, for more complicated objectives <span class="math notranslate nohighlight">\(\alpha_n\)</span> will not be closed-form and must find an approximate solution to it. Line search algorithms attempt to solve this optimisation problem by searching along the line <span class="math notranslate nohighlight">\(x_n + \alpha d_n\)</span> until a satisfactory solution is found. For convenience, we define the univariate function</p>
<div class="math notranslate nohighlight">
\[\begin{align}
f(a) = F(x_n + \alpha d_n).
\end{align}\]</div>
<p>A candidate solution is deemed acceptable if it satisfies certain conditions. A widespread choice for these conditions are the strong Wolfe-Powell (WP) conditions:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
f(\alpha) \leq f(0) + c_1 \alpha f'(0) \label{wp1}, \\
|f'(\alpha)| \leq c_2 |f'(0)|          \label{wp2}.
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(c_1\)</span> and <span class="math notranslate nohighlight">\(c_2\)</span> are positive constants. The first condition says that the candidate minimiser should be better than the initial point <span class="math notranslate nohighlight">\(x_n\)</span> by an amount at least as large as the fraction of the improvement expected by a linear approxmation to <span class="math notranslate nohighlight">\(F\)</span>. If this condition is not met, we can likely find a better point in the neighbourhood of <span class="math notranslate nohighlight">\(\alpha\)</span>. The second condition, says that the magnitude of the gradient of the objective <span class="math notranslate nohighlight">\(f'(\cdot)\)</span> should not be greater than a fraction of the gradient at the initial point. If this condition is not met at <span class="math notranslate nohighlight">\(\alpha\)</span>, it is likely we can find a significantly better point in the neighbourhood of <span class="math notranslate nohighlight">\(\alpha\)</span>. If both conditions are satisfied, we deemm the solution good enough and accept the point as an approximate solution to the line search. Below is an implementation for a helper function checking for the WPCs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">wolfe_powell</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes points *p0* and *p* and constants *c1* and *c2* and determines</span>
<span class="sd">    whether *p1* satisfies the Wolfe-Powell conditions w.r.t. *p0*:</span>
<span class="sd">    </span>
<span class="sd">        f(p) &lt;= f(p0) + c1 * (x1 - x0) *  f&#39;(p0),</span>
<span class="sd">        |f&#39;(p1)| &lt;= c2 * |f&#39;(p0)|.</span>
<span class="sd">        </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    </span>
<span class="sd">    p0 : dict</span>
<span class="sd">        Must contain keys &#39;x&#39;, &#39;f&#39; and &#39;dfdx&#39;.</span>
<span class="sd">        </span>
<span class="sd">    p1 : dict</span>
<span class="sd">        Must contain keys &#39;x&#39;, &#39;f&#39; and &#39;dfdx&#39;.</span>
<span class="sd">        </span>
<span class="sd">    c1 : float</span>
<span class="sd">        Must satisfy 0 &lt; c1 &lt;= c2 &lt; 1.</span>
<span class="sd">        </span>
<span class="sd">    c2 : float</span>
<span class="sd">        Must satisfy 0 &lt; c1 &lt;= c2 &lt; 1.</span>
<span class="sd">        </span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    </span>
<span class="sd">    wp : bool</span>
<span class="sd">        True if both WPCs are satisfied, else False</span>
<span class="sd">        </span>
<span class="sd">    wp1 : bool</span>
<span class="sd">        True if first WPC is satisfied, else False</span>
<span class="sd">        </span>
<span class="sd">    wp2 : tup</span>
<span class="sd">        Tuple of bools of length 3, each of which corresponding to</span>
<span class="sd">        </span>
<span class="sd">             f&#39;(p1)  &lt; - c2 * |f&#39;(p0)|,</span>
<span class="sd">             f&#39;(p1)  &gt;   c2 * |f&#39;(p0)|,</span>
<span class="sd">            |f&#39;(p1)| &lt;   c2 * |f&#39;(p0)|.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># First WPC</span>
    <span class="n">wp1</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">p0</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">c1</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">p0</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">p0</span><span class="p">[</span><span class="s1">&#39;dfdx&#39;</span><span class="p">]</span>
    
    <span class="c1"># Second WPC, with each possibility in the inequality separate</span>
    <span class="n">wp2</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">]</span>
    
    <span class="c1"># Set each entry of the second WPC separately</span>
    <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="s1">&#39;dfdx&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="o">-</span> <span class="n">c2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p0</span><span class="p">[</span><span class="s1">&#39;dfdx&#39;</span><span class="p">]):</span>
        <span class="n">wp2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        
    <span class="k">elif</span> <span class="n">p</span><span class="p">[</span><span class="s1">&#39;dfdx&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">c2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p0</span><span class="p">[</span><span class="s1">&#39;dfdx&#39;</span><span class="p">]):</span>
        <span class="n">wp2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wp2</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    
    <span class="c1"># If first and second WPCs hold, this is True, otherwise False</span>
    <span class="n">wp</span> <span class="o">=</span> <span class="n">wp1</span> <span class="ow">and</span> <span class="n">wp2</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">wp</span><span class="p">,</span> <span class="n">wp1</span><span class="p">,</span> <span class="n">wp2</span>
</pre></div>
</div>
</div>
</div>
<p>If our fist guess passes the WPCs, then it is acceptable and the line search stops. If it is not accepted however, we must find a next candidate point. We can achieve this using the following algorithm.</p>
<div class="lemma">
<p><strong>Algorithm (Bisection line search)</strong> Given a univariate <span class="math notranslate nohighlight">\(f(\cdot)\)</span> with <span class="math notranslate nohighlight">\(f'(0) &lt; 0\)</span>, constants <span class="math notranslate nohighlight">\(c_1, c_2\)</span> and an initial guess <span class="math notranslate nohighlight">\(\alpha_0\)</span>, the following algorithm is guaranteed to terminate with a WP-acceptable point:</p>
<ol>
<li><p>If <span class="math notranslate nohighlight">\(f'(\alpha_n) &lt; - c_2 |f'(0)|\)</span> holds, set <span class="math notranslate nohighlight">\(a_{n + 1} = 2 a_n\)</span> and repeat this step. Otherwise set <span class="math notranslate nohighlight">\(\beta_0 = 0, \gamma_0 = \alpha_n\)</span> and proceed to the next step.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\gamma_n\)</span> is accepted by the WPCs, return it as the solution. Otherwise</p>
<p>2.1 If the first WPC is satisfied and <span class="math notranslate nohighlight">\(f'(\gamma_n) &gt; 0\)</span>, set <span class="math notranslate nohighlight">\(\beta_{n + 1} = \beta_n\)</span> and <span class="math notranslate nohighlight">\(\gamma_{n + 1} = \frac{\beta_n + \gamma_n}{2}\)</span> and repeat this step.</p>
<p>2.2 Else set <span class="math notranslate nohighlight">\(\beta_{n + 1} = \frac{\beta_n + \gamma_n}{2}\)</span> and <span class="math notranslate nohighlight">\(\gamma_{n + 1} = \gamma_n\)</span> and repeat this step.</p>
</li>
</ol>
</div>
<br>
<p>This algorithm makes an initial guess and exponentially expands the search space. If the first step terminates, then a WP-acceptable point is guaranteed to lie in <span class="math notranslate nohighlight">\([\beta_n, \alpha_n]\)</span>. If however the first step does not terminate, the objective will decrease at every iteration, without bound. We can assume that this will not be the case for a well-behaved optimisation problem, and if we encounter this case in practice, we can simply proceed until the computational budget is expended and return the result as the solution. Similarly, if the second step terminates then we have found a WP-acceptable point. Otherwise the size of the interval <span class="math notranslate nohighlight">\([\beta_n, \gamma_n]\)</span> decreases exponentially and is guaranteed to terminate if <span class="math notranslate nohighlight">\(c_1 \leq c_2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">exponential_extrapolation</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    </span>
<span class="sd">    p0 : dict</span>
<span class="sd">        Must contain keys &#39;x&#39;, &#39;f&#39; and &#39;dfdx&#39;</span>
<span class="sd">        </span>
<span class="sd">    p1 : dict</span>
<span class="sd">        Must contain keys &#39;x&#39;, &#39;f&#39; and &#39;dfdx&#39;</span>
<span class="sd">        </span>
<span class="sd">    c1 : float</span>
<span class="sd">        Must satisfy 0 &lt; c1 &lt;= c2 &lt; 1.</span>
<span class="sd">        </span>
<span class="sd">    c2 : float</span>
<span class="sd">        Must satisfy 0 &lt; c1 &lt;= c2 &lt; 1.</span>
<span class="sd">        </span>
<span class="sd">    tolearnce : float</span>
<span class="sd">        </span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    p2 : dict</span>
<span class="sd">        A point *p2* such that the interval [p0, p2] is guaranteed to</span>
<span class="sd">        contain a point which satisfies the WPCs.</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
        
    <span class="n">p2</span> <span class="o">=</span> <span class="n">p1</span>
    
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        
        <span class="c1"># Evaluate WPCs to decide wether to stop extrapolating</span>
        <span class="n">wp</span><span class="p">,</span> <span class="n">wp1</span><span class="p">,</span> <span class="n">wp2</span> <span class="o">=</span> <span class="n">wolfe_powell</span><span class="p">(</span><span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p2</span><span class="p">,</span> <span class="n">c1</span><span class="o">=</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="o">=</span><span class="n">c2</span><span class="p">)</span>
        
        <span class="c1"># Continue only if f&#39;(p1) &lt; - c2 * |f&#39;(p0)|, otherwise stop</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">wp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">wp2</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            
            <span class="c1"># Set p2 twice as far from p0 as p1 is</span>
            <span class="n">p2</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span> <span class="p">:</span> <span class="n">p0</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">p1</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">p0</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="mi">2</span><span class="p">}</span>
        
            <span class="c1"># Call univariate objective at p2</span>
            <span class="n">p2</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">],</span> <span class="n">p2</span><span class="p">[</span><span class="s1">&#39;dfdx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">p2</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
            
            <span class="n">p1</span> <span class="o">=</span> <span class="n">p2</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">p2</span>
</pre></div>
</div>
</div>
</div>
<p>Below is an implementation of the bisection step of the linesearch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bisection_interpolation</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    </span>
<span class="sd">    p0 : dict</span>
<span class="sd">        Must contain keys &#39;x&#39;, &#39;f&#39; and &#39;dfdx&#39;</span>
<span class="sd">        </span>
<span class="sd">    p1 : dict</span>
<span class="sd">        Must contain keys &#39;x&#39;, &#39;f&#39; and &#39;dfdx&#39;</span>
<span class="sd">        </span>
<span class="sd">    c1 : float</span>
<span class="sd">        Must satisfy 0 &lt; c1 &lt;= c2 &lt; 1.</span>
<span class="sd">        </span>
<span class="sd">    c2 : float</span>
<span class="sd">        Must satisfy 0 &lt; c1 &lt;= c2 &lt; 1.</span>
<span class="sd">        </span>
<span class="sd">    tolearnce : float</span>
<span class="sd">        </span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    p2 : dict</span>
<span class="sd">        A point *p2* which satisfies the WPCs.</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">p_</span> <span class="o">=</span> <span class="n">p0</span>
    
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        
        <span class="c1"># Set p2 at the midpoint of p0 and p1</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span> <span class="p">:</span> <span class="p">(</span><span class="n">p1</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">p0</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span><span class="p">}</span>
        
        <span class="c1"># Call univariate objective at p2</span>
        <span class="n">p2</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">],</span> <span class="n">p2</span><span class="p">[</span><span class="s1">&#39;dfdx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">p2</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
        
        <span class="c1"># Evaluate WPCs to decide wether to stop</span>
        <span class="n">wp</span><span class="p">,</span> <span class="n">wp1</span><span class="p">,</span> <span class="n">wp2</span> <span class="o">=</span> <span class="n">wolfe_powell</span><span class="p">(</span><span class="n">p0</span><span class="o">=</span><span class="n">p_</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p2</span><span class="p">,</span> <span class="n">c1</span><span class="o">=</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="o">=</span><span class="n">c2</span><span class="p">)</span>
        
        <span class="c1"># If WPCs satisfied, return point</span>
        <span class="k">if</span> <span class="n">wp</span> <span class="ow">or</span> <span class="p">(</span><span class="n">p1</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">p0</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">p2</span>
        
        <span class="k">elif</span> <span class="p">(</span><span class="ow">not</span> <span class="n">wp1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">wp1</span> <span class="ow">and</span> <span class="n">p2</span><span class="p">[</span><span class="s1">&#39;dfdx&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">p1</span> <span class="o">=</span> <span class="n">p2</span>
            
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p0</span> <span class="o">=</span> <span class="n">p2</span>
</pre></div>
</div>
</div>
</div>
<p>Putting the extrapolation and interpolation parts of the search together we arrive at the complete bisection linesearch algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bisection_linesearch</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a univariate function *f* and initial guess *a0* and returns a</span>
<span class="sd">    point *a* which satisfies the WPCs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Set p2 at the midpoint of p0 and p1</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span> <span class="p">:</span> <span class="n">a0</span><span class="p">}</span>

    <span class="c1"># Call univariate objective at p2</span>
    <span class="n">p1</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">],</span> <span class="n">p1</span><span class="p">[</span><span class="s1">&#39;dfdx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">a0</span><span class="p">)</span>
    
    <span class="c1"># First extrapolate, then interpolate</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">exponential_extrapolation</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="o">=</span><span class="n">p1</span><span class="p">,</span> <span class="n">c1</span><span class="o">=</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="o">=</span><span class="n">c2</span><span class="p">)</span>
    <span class="n">p3</span> <span class="o">=</span> <span class="n">bisection_interpolation</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="o">=</span><span class="n">p2</span><span class="p">,</span> <span class="n">c1</span><span class="o">=</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="o">=</span><span class="n">c2</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="n">tolerance</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">p3</span>
</pre></div>
</div>
</div>
</div>
<p>Lastly, the bisection linesearch together with the CG update rule we have an algorithm for approximately solving nonlinear optimisation problems. Below is an implementation putting it all together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">nonlinear_conjugate_gradients</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">):</span>
        
    <span class="n">x_opt</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">f_opt</span><span class="p">,</span> <span class="n">dFdx_opt</span> <span class="o">=</span> <span class="n">F</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)</span>
    
    <span class="n">x_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_opt</span><span class="p">]</span>
    <span class="n">f_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">f_opt</span><span class="p">]</span>
    
    <span class="n">d</span> <span class="o">=</span> <span class="o">-</span> <span class="n">dFdx_opt</span>
    <span class="n">reset_d_every</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        
        <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">alpha</span><span class="p">):</span>
            
            <span class="n">f_alpha</span><span class="p">,</span> <span class="n">dFdx_alpha</span> <span class="o">=</span> <span class="n">F</span><span class="p">(</span><span class="n">x_opt</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>
            <span class="n">dfdx_alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dFdx_alpha</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="n">f_alpha</span><span class="p">,</span> <span class="n">dfdx_alpha</span>
        
        <span class="c1"># Create univariate p0</span>
        <span class="n">p0</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span>    <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
              <span class="s1">&#39;f&#39;</span>    <span class="p">:</span> <span class="n">f_opt</span><span class="p">,</span>
              <span class="s1">&#39;dfdx&#39;</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dFdx_opt</span><span class="p">,</span> <span class="n">d</span><span class="p">)}</span>
        
        <span class="c1"># Find step size satisfying the WPCs</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">bisection_linesearch</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span>
                                     <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span>
                                     <span class="n">a0</span><span class="o">=</span><span class="n">a0</span><span class="p">,</span>
                                     <span class="n">c1</span><span class="o">=</span><span class="n">c1</span><span class="p">,</span>
                                     <span class="n">c2</span><span class="o">=</span><span class="n">c2</span><span class="p">,</span>
                                     <span class="n">tolerance</span><span class="o">=</span><span class="n">tolerance</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
        
        <span class="c1"># Update x</span>
        <span class="n">x_opt</span> <span class="o">=</span> <span class="n">x_opt</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">d</span>
        <span class="n">f_opt_</span><span class="p">,</span> <span class="n">dFdx_opt_</span> <span class="o">=</span> <span class="n">F</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)</span>
        
        <span class="c1"># Update beta and d</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dFdx_opt_</span><span class="p">,</span> <span class="p">(</span><span class="n">dFdx_opt_</span> <span class="o">-</span> <span class="n">dFdx_opt</span><span class="p">))</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dFdx_opt</span><span class="p">,</span> <span class="n">dFdx_opt</span><span class="p">)</span>

        <span class="n">d</span> <span class="o">=</span> <span class="o">-</span> <span class="n">dFdx_opt_</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">d</span>
        
        <span class="c1"># Update set f and derivative to new values</span>
        <span class="n">f_opt</span> <span class="o">=</span> <span class="n">f_opt_</span>
        <span class="n">dFdx_opt</span> <span class="o">=</span> <span class="n">dFdx_opt_</span>
        
        <span class="n">x_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)</span>
        <span class="n">f_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f_opt</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_hist</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">f_hist</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can now apply our nonlinear CG implementation to a quadratic problem, in order to sanity-check it. To recap, the nonlinear CG algorithm implemented here is different from the exact CG algorithm in that it uses the Polak-Ribiere update and also solves for the step-size <span class="math notranslate nohighlight">\(\alpha_n\)</span> approximately. As expected, nonlinear CGs give a nearly identical answer to the exact CG algorithm.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># A and b for defining the quadratic form</span>
<span class="n">num_dim</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_dim</span><span class="p">,</span> <span class="n">num_dim</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">5e-1</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_dim</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_dim</span><span class="p">,))</span>

<span class="n">F</span> <span class="o">=</span> <span class="n">objective</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># Initial guess</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_dim</span><span class="p">,))</span>

<span class="n">a0</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">c1</span> <span class="o">=</span> <span class="mf">1e-7</span>
<span class="n">c2</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="n">num_dim</span>

<span class="n">x_hist_ncg</span><span class="p">,</span> <span class="n">F_hist_ncg</span> <span class="o">=</span> <span class="n">nonlinear_conjugate_gradients</span><span class="p">(</span><span class="n">F</span><span class="p">,</span>
                                                       <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span>
                                                       <span class="n">a0</span><span class="o">=</span><span class="n">a0</span><span class="p">,</span>
                                                       <span class="n">c1</span><span class="o">=</span><span class="n">c1</span><span class="p">,</span>
                                                       <span class="n">c2</span><span class="o">=</span><span class="n">c2</span><span class="p">,</span>
                                                       <span class="n">num_steps</span><span class="o">=</span><span class="n">num_dim</span><span class="p">,</span>
                                                       <span class="n">tolerance</span><span class="o">=</span><span class="n">tolerance</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial guess
[-1.70627019  1.9507754  -0.50965218 -0.4380743  -1.25279536  0.77749036]
Objective: 15.961

Final solution
[ 0.32754125  0.97890273 -0.38035804 -0.01772335 -0.5640225  -0.44451708]
Objective: -1.478

Optimal solution
[ 0.32827142  0.97844478 -0.38013915 -0.01791253 -0.56457165 -0.44429853]
Objective: -1.478
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-content/misc/optimisation/conjugate-gradients-0"><dl class="citation">
<dt class="bibtex label" id="conjugatemethods"><span class="brackets"><a class="fn-backref" href="#id3">HZ06</a></span></dt>
<dd><p>William W Hager and Hongchao Zhang. A survey of nonlinear conjugate gradient methods. <em>Pacific journal of Optimization</em>, 2(1):35–58, 2006.</p>
</dd>
<dt class="bibtex label" id="shewchuk"><span class="brackets"><a class="fn-backref" href="#id1">She94</a></span></dt>
<dd><p>Jonathan R Shewchuk. An introduction to the conjugate gradient method without the agonizing pain. Technical Report, Carnegie Mellon University, 1994.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "venv-random-walks"
        },
        kernelOptions: {
            kernelName: "venv-random-walks",
            path: "./content/misc/optimisation"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'venv-random-walks'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../node/node.html" title="previous page">Neural ODEs</a>
    <a class='right-next' id="next-link" href="../kalman/kalman.html" title="next page">The Kalman filter and smoother</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratos Markou<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>