{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Gaussian Processes\n",
    "\n",
    "## How can sparsity help GPs scale?\n",
    "\n",
    "To speed up hyperparameter learning and inference Gaussian Processes, it's clear that we'll have to give up something: if we want to work with the *exact* marginal likelihood and the *exact* predictive posterior\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\mathbf{y} | \\mathbf{X}) &= \\mathcal{N}\\left(\\mathbf{y}; \\mathbf{0}, \\mathbf{K}_{\\mathbf{X}\\mathbf{X}} + \\sigma^2 \\mathbf{I}\\right),\\\\\n",
    "p(\\mathbf{y}^* | \\mathbf{x}^*, \\mathbf{y}, \\mathbf{X}) &= \\mathcal{N}\\left(\\mathbf{y}^*; \\mathbf{K}_{\\mathbf{x}^*\\mathbf{X}} \\left(\\mathbf{K}_{\\mathbf{X}\\mathbf{X}} + \\sigma^2 \\mathbf{I} \\right)^{-1} \\mathbf{y}, \\mathbf{K}_{\\mathbf{x}^*\\mathbf{x}^*} - \\mathbf{K}_{\\mathbf{x}^*\\mathbf{X}} \\left(\\mathbf{K}_{\\mathbf{X}\\mathbf{X}} + \\sigma^2 \\mathbf{I} \\right)^{-1}\\mathbf{K}_{\\mathbf{X}\\mathbf{x}^*}\\right),\n",
    "\\end{align}$$\n",
    "\n",
    "we cannot do any better than suffering $\\mathcal{O}(N^3)$ cost when calculating either quantity, because they both involve the inverse of an $N \\times N$ covariance matrix. If we want to scale GPs to larger datasets *we cannot consider correlations between all datapoints* because of the cubic cost of inverting the associated covariance. One idea for scaling GPs to larger data is to make some approximations about the structure of the covariance matrix, such as an independence assumption between certain variables, which will make the matrix inversion cheaper.\n",
    "\n",
    "## Literature on sparse Gaussian Processes\n",
    "\n",
    "\n",
    "### Sparse Approximation methods\n",
    "- Fully Independent Training Conditionals (FITC) approximation, originally called SPGP.\n",
    "- Deterministic Training Conditionals (DTC) approximation.\n",
    "- Variational Free Energy (VFE) approximation.\n",
    "- Partial Independent Training Conditionals (PITC) approximation.\n",
    "\n",
    "### Review papers\n",
    "- *Turner, Yan and Bui*, A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation.\n",
    "- *Quinonero-Candela and Rasmussen*, A Unifying View of Sparse Approximate Gaussian Process Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-random-walks",
   "language": "python",
   "name": "venv-random-walks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
