
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Why covariance functions? &#8212; Random walks</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://random-walks.org/content/gaussian-processes/why-covariances.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://random-walks.org/content/gaussian-processes/why-covariances.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Why covariance functions?" />
<meta property="og:description" content="Why covariance functions?  One question about GPs that I couldnâ€™t wrap my head around was why do we work with covariance instead of precision functions. After a" />
<meta property="og:image"       content="https://random-walks.org/_static/logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Random walks</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../home.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../prob-intro/intro.html">
   Probability: An introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch01/content.html">
     Events and Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch02/content.html">
     Discrete random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch03/content.html">
     Multivariate discrete distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch04/content.html">
     Probability generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch05/content.html">
     Distribution and density functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch06/content.html">
     Multivariate distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch07/content.html">
     Moment generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch08/content.html">
     Main limit theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch09/content.html">
     Branching processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch10/content.html">
     Random walks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch11/content.html">
     Processes in continuous time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prob-intro/ch12/content.html">
     Markov chains
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../misc/misc.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/sde/num-sde.html">
     Numerical simulation of SDEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/sde-as-gp/sde-as-gp.html">
     VI for diffusion processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/node/node.html">
     Neural ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/optimisation/conjugate-gradients.html">
     Conjugate gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/kalman/kalman.html">
     The Kalman filter and smoother
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/ncs/ncs.html">
     Natural cubic splines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/ars/ars.html">
     Adaptive rejection sampling
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reading-and-links.html">
   Interesting reading and websites
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/gaussian-processes/why-covariances.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="why-covariance-functions">
<h1>Why covariance functions?<a class="headerlink" href="#why-covariance-functions" title="Permalink to this headline">Â¶</a></h1>
<p>One question about GPs that I couldnâ€™t wrap my head around was why do we work with covariance instead of precision functions. After all, one central limitation of GPs is the cost of inverting matrices, which we could perhaps cut down on if we worked with precision instead of covariance functions.</p>
<p>The reason we donâ€™t work with precision functions is because itâ€™s not possible to define a function which maps pairs of inputs to entries in a precision matrix, such that the precision matrices produced by it are consistent with one another. In particular, given a set of inputs <span class="math notranslate nohighlight">\(\mathbf{x}_{1:N} = \{\mathbf{x}_n \in \mathbb{R}^d\}_{n = 1}^N\)</span>, we can define a covariance function <span class="math notranslate nohighlight">\(k : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\)</span>, mapping <strong>pairs of inputs</strong> <span class="math notranslate nohighlight">\((\mathbf{x}_i, \mathbf{x}_j)\)</span> to the respective entries of a covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{y_{1:N} | \mathbf{x}_{1:N}}\)</span>, and that the covariance matrices produced by this <span class="math notranslate nohighlight">\(k\)</span> are consistent with each other. However, it is not possible in general to define a precision function <span class="math notranslate nohighlight">\(r : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\)</span>, mapping <strong>pairs of inputs</strong> <span class="math notranslate nohighlight">\((\mathbf{x}_i, \mathbf{x}_j)\)</span> to the repsective entries of a covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_{y_{1:N} | \mathbf{x}_{1:N}}\)</span>, such that the corresponding precision matrices are consistent with one another.</p>
<p>Consider set of input-output variables <span class="math notranslate nohighlight">\(\mathbf{x}_{1:N}, y_{1:N}\)</span>. We will show two ways of computing the marginal distribution of a subset of these variables, <span class="math notranslate nohighlight">\(y_{1:M}\)</span> where <span class="math notranslate nohighlight">\(M &lt; N\)</span> and show that the covariance function produces consistent covariance matrices, while the precision function is not guaranteed to produce consistent precision matrices. One way to obtain the marginal distribution is to compute the integral</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(y_{1:M} | \mathbf{x}_{1:M}) = \int p(y_{1:N} | \mathbf{x}_{1:N}) dy_{M+1:N} = \int \mathcal{N}\left(\mathbf{y}_{1:N}; \boldsymbol{\mu}_{y_{1:N} | \mathbf{x}_{1:N}}, \boldsymbol{\Sigma}_{y_{1:N} | \mathbf{x}_{1:N}} \right) dy_{M+1:N}.
\end{align}\]</div>
<p>Another way of obtaining the marginal <span class="math notranslate nohighlight">\(p(y_{1:M} | \mathbf{x}_{1:M})\)</span> is to simply apply <span class="math notranslate nohighlight">\(k : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\)</span> or <span class="math notranslate nohighlight">\(r : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\)</span> to the subset <span class="math notranslate nohighlight">\(\mathbf{x}_{1 : M}\)</span> of input variables, to obtain the covariance or precision matrix respectively. Regardless of whether we use <span class="math notranslate nohighlight">\(k\)</span> or <span class="math notranslate nohighlight">\(r\)</span>, the distributions obtained through (a) marginalisation or (b) direct computation, better be the same. From the marginalisation property of multivariate Gaussians we have</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(y_{1:M} | \mathbf{x}_{1:M}) = \mathcal{N}\left(\mathbf{y}_{1:M}; \mathbf{a}, \mathbf{A} \right),
\end{align}\]</div>
<p>where we have partitioned the mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{y_{1:N} | \mathbf{x}_{1:N}}\)</span> and covariance <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{y_{1:N} | \mathbf{x}_{1:N}}\)</span> of the joint distribution as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
p(y_{1:N} | \mathbf{x}_{1:N}) = \mathcal{N}\left(\mathbf{y}_{1:N}; \boldsymbol{\mu}_{y_{1:N} | \mathbf{x}_{1:N}}, \boldsymbol{\Sigma}_{y_{1:N} | \mathbf{x}_{1:N}} \right)= \mathcal{N}\left(\begin{bmatrix} \mathbf{y}_{1:M} \\ \mathbf{y}_{M+1:N}
\end{bmatrix}; \begin{bmatrix} \mathbf{a} \\ \mathbf{b}
\end{bmatrix},
\begin{bmatrix}
\mathbf{A}\phantom{^\top} &amp; \mathbf{B}\\
\mathbf{B}^\top &amp; \mathbf{C}
\end{bmatrix}\right).
\end{align}\end{split}\]</div>
<p>A covariance function <span class="math notranslate nohighlight">\(k : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\)</span> taking <strong>pairs of inputs</strong> <span class="math notranslate nohighlight">\((\mathbf{x}_i, \mathbf{x}_j)\)</span> as arguments, is consistent in the sense that (a) applying <span class="math notranslate nohighlight">\(k\)</span> to all pairs in <span class="math notranslate nohighlight">\(\mathbf{x}_{1 : N}\)</span> and then marginalising over <span class="math notranslate nohighlight">\(\mathbf{x}_{M+1:N}\)</span>, and (b) applying <span class="math notranslate nohighlight">\(k\)</span> to the pairs in <span class="math notranslate nohighlight">\(\mathbf{x}_{M+1:N}\)</span>; will produce the same covariance matrix. This model is self-consistent in this respect.</p>
<p>On the other hand, if the precision function <span class="math notranslate nohighlight">\(r\)</span> takes pairs of inputs <span class="math notranslate nohighlight">\((\mathbf{x}_i, \mathbf{x}_j)\)</span> as its argument, then it cannot be self-consistent. We can see this by considering</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\boldsymbol{\Lambda}_{y_{1:N} | \mathbf{x}_{1:N}} &amp;= \begin{pmatrix}
\mathbf{A}\phantom{^\top} &amp; \mathbf{B}\\
\mathbf{B}^\top &amp; \mathbf{C}
\end{pmatrix}^{-1} = \begin{pmatrix}
\mathbf{M} &amp; -\mathbf{M}\mathbf{B}\mathbf{C}^{-1}\\
\mathbf{C}^{-1}\mathbf{B}^\top\mathbf{M} &amp; \mathbf{C}^{-1} + \mathbf{C}^{-1} \mathbf{B}^\top \mathbf{M}\mathbf{B}\mathbf{C}^{-1}
\end{pmatrix}, \text{ where } \mathbf{M} = (\mathbf{A} - \mathbf{B} \mathbf{C}^{-1}\mathbf{B}^\top)^{-1},
\end{align}\end{split}\]</div>
<p>and oberving that the entry <span class="math notranslate nohighlight">\(M_{ij}\)</span> is a function of all the <span class="math notranslate nohighlight">\(\mathbf{x}_{1:N}\)</span> inputs and not just <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_j\)</span>. Thus, the precision function cannot be self-consistent if it takes pairs of inputs as its argument. In other words, the precision matrix depends on the whole set of inputs we are conditioning on, and any precision function that takes <strong>pairs of inputs</strong> as its arguments will be inconsistent in general - perhaps with some trivial exceptions. We therefore have the following informal observation.</p>
<div class='observation'>
<p><strong>Observation (Precision functions and consistency)</strong> It is not possible in general to define a function <span class="math notranslate nohighlight">\(r : \mathbb{R}^{d} \times \mathbb{R}^{d} \to \mathbb{R}\)</span>, mapping pairs of <span class="math notranslate nohighlight">\(d\)</span>-dimensional vectors to real numbers, such that it produces precision matrices which are consistent under marginalisation.</p>
</div>
<br>
<p>Since GP models are defined by covariance functions with input-pair arguments, this rules out the possibility of defining a GP using a precision function. We also observe that whereas the covariance of two random variables is a pair-wise quantity (by definition), the precision of two random variables is a set-wise or global quantity.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/gaussian-processes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratos Markou<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>