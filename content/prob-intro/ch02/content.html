
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Discrete random variables &#8212; Random walks</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://random-walks.org/content/prob-intro/ch02/content.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Multivariate discrete distributions" href="../ch03/content.html" />
    <link rel="prev" title="Events and Probabilities" href="../ch01/content.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://random-walks.org/content/prob-intro/ch02/content.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Discrete random variables" />
<meta property="og:description" content="Discrete random variables  Discrete random variables are random variables whose images are countable sets. We define discrete random variables and probability m" />
<meta property="og:image"       content="https://random-walks.org/_static/logo.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Random walks</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../home.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../intro.html">
   Probability: An introduction
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/content.html">
     Events and Probabilities
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Discrete random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/content.html">
     Multivariate discrete distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch04/content.html">
     Probability generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch05/content.html">
     Distribution and density functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch06/content.html">
     Multivariate distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch07/content.html">
     Moment generating functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch08/content.html">
     Main limit theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch09/content.html">
     Branching processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch10/content.html">
     Random walks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch11/content.html">
     Processes in continuous time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/content.html">
     Markov chains
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../gp/gp-intro.html">
   Gaussian Processes
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../gp/why-covariances.html">
     Why covariance functions?
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../gp/sparse/sparse-intro.html">
     Sparse Gaussian Processes
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../gp/sparse/vfe.html">
       Variational Free Energy GPs
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../misc/misc.html">
   Miscellaneous
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/sde/num-sde.html">
     Numerical simulation of SDEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/sde-as-gp/sde-as-gp.html">
     VI for diffusion processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/node/node.html">
     Neural ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/optimisation/conjugate-gradients.html">
     Conjugate gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/kalman/kalman.html">
     The Kalman filter and smoother
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/ncs/ncs.html">
     Natural cubic splines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/ars/ars.html">
     Adaptive rejection sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/score-matching/score-matching.html">
     Estimation by score matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/interacting/interacting.html">
     Interacting particle FPK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/rff/rff.html">
     Random Fourier features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/svgd/svgd.html">
     Stein variational gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../misc/addgp/addgp.html">
     Additive Gaussian Processes
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reading-and-links.html">
   Interesting reading and websites
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/prob-intro/ch02/content.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Discrete random variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-mass-functions">
   Probability mass functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fundamental-discrete-distributions">
   Fundamental discrete distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bernoulli">
     Bernoulli
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binomial">
     Binomial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#poisson">
     Poisson
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#geometric">
     Geometric
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#negative-binomial">
     Negative binomial
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectations">
   Expectations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditional-expectations">
   Conditional expectations
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="discrete-random-variables">
<h1>Discrete random variables<a class="headerlink" href="#discrete-random-variables" title="Permalink to this headline">¶</a></h1>
<p>Discrete random variables are random variables whose images are countable sets. We define discrete random variables and probability mass functions and present some examples. The (conditional) expectation and variance are important summary statistics of random variables.</p>
<div class="section" id="id1">
<h2>Discrete random variables<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>We are often interested in the value of a function of elementary events. For example we might be interested in the profit of a gambling game as a function of the elementary outcomes of the game.</p>
<div class='definition'>
<p><strong>Definition (Discrete random variable)</strong> A discrete random variable <span class="math notranslate nohighlight">\(X\)</span> is a function <span class="math notranslate nohighlight">\(X : \Omega \to \mathbb{R}\)</span> such that:</p>
<ol class="simple">
<li><p>The image <span class="math notranslate nohighlight">\(X(\Omega)\)</span> is a countable subset of <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>.</p></li>
<li><p>For every <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> we have <span class="math notranslate nohighlight">\(\{\omega \in \Omega : X(\omega) = x\} \in \mathcal{F}\)</span>.</p></li>
</ol>
</div>
<br>
<p>By defining discrete random variables as functions of elementary events, we separate the ideas of experimental outcomes and of functions of these outcomes.</p>
<p>Condition (1) specifies that a discrete random variable can only take countably many values. Condition (2) specifies that every set that is mapped to a certain value <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> is contained in the event space <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> and is therefore assigned a probability by any probability measure defined on <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.</p>
</div>
<div class="section" id="probability-mass-functions">
<h2>Probability mass functions<a class="headerlink" href="#probability-mass-functions" title="Permalink to this headline">¶</a></h2>
<p>Once we have defined the continuous random variable, we can make statemets about the probability that it will take a certain value, using its <strong>probability mass function</strong>.</p>
<div class='definition'>
<p><strong>Definition (Probability mass function)</strong> Let <span class="math notranslate nohighlight">\(X : \Omega \to \mathbb{R}\)</span> be a discrete random variable. The <strong>probability mass function</strong> (or pmf) is the function <span class="math notranslate nohighlight">\(p_X : \mathbb{R} \to [0, 1]\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[p_X(x) = \mathbb{P}(X = x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{P}(X = x)\)</span> is shorthand for <span class="math notranslate nohighlight">\(\mathbb{P}(\{\omega \in \Omega : X(\omega) = x\})\)</span>.</p>
</div>
<br>
<p>Reviewing the definitions leading up to the pmf, we defined: elementary events, event spaces and probability measures, followed by random variables and the probability mass function. This was a rigorous build-up of what a discrete random variable is, however in many cases we need not consider this construction at all, instead declaring that “<span class="math notranslate nohighlight">\(X\)</span> is a random variable with pmf <span class="math notranslate nohighlight">\(p_X(\cdot)\)</span>” and proceed to give an expression for <span class="math notranslate nohighlight">\(p_X(\cdot)\)</span> directly.</p>
<div class='theorem'>
<p><strong>Theorem (pmf <span class="math notranslate nohighlight">\(\implies\)</span> probability space and random variable)</strong> Let <span class="math notranslate nohighlight">\(S = \{s_i : i \in I\}\)</span> be a countable set of real numbers and let <span class="math notranslate nohighlight">\(\{\pi_i : i \in I\}\)</span> be a collectioin of real numbers satisfying</p>
<div class="math notranslate nohighlight">
\[\pi_i \geq 0 \text{ for } i \in I, \text{ and } \sum_{i \in I} \pi_i = 1.\]</div>
<p>There exists a probability space <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, \mathbb{P})\)</span> and a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> on <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, \mathbb{P})\)</span> such that the pmf <span class="math notranslate nohighlight">\(p_X(\cdot)\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
p_X(s_i) &amp;= \pi_i &amp;&amp; \text{ for } i \in I\\
p_X(s) &amp;= 0 &amp;&amp; \text{ if } s \not \in S
\end{align}\end{split}\]</div>
</div>
<br>
<p>The proof for this theorem is that we can take <span class="math notranslate nohighlight">\(\Omega = S\)</span>, <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> to be the powerset of <span class="math notranslate nohighlight">\(\Omega\)</span> and <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{P}(A) = \sum_{i : s_i \in A} \pi_i &amp; \text{ for } A \in \mathcal{F}.
\end{align}\]</div>
<p>Lastly defining <span class="math notranslate nohighlight">\(X : \Omega \to \mathbb{R}\)</span> to be any one-to-one function, we arrive at the result because we can verify that:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is a valid event space.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}\)</span> is a valid probability measure.</p></li>
<li><p><span class="math notranslate nohighlight">\(X\)</span> is a valid discrete random variable and has the pmf from the theorem statement.</p></li>
</ol>
</div>
<div class="section" id="fundamental-discrete-distributions">
<h2>Fundamental discrete distributions<a class="headerlink" href="#fundamental-discrete-distributions" title="Permalink to this headline">¶</a></h2>
<p>Here are some examples of fundamental discrete distributions. Appealing to the theorem above, we can forget about probability spaces and consider only the values taken by the pmf of the random variable in question.</p>
<div class="section" id="bernoulli">
<h3>Bernoulli<a class="headerlink" href="#bernoulli" title="Permalink to this headline">¶</a></h3>
<p>Also called the coin toss, the Bernoulli distribution is the simplest discrete distribution. A random variable <span class="math notranslate nohighlight">\(X\)</span> is Bernoulli-distributed with paramter <span class="math notranslate nohighlight">\(p \in [0, 1]\)</span> if <span class="math notranslate nohighlight">\(X\)</span> can take the values 0 or 1:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align}
\mathbb{P}(X = 0) &amp;= 1 - p, \\
\mathbb{P}(X = 1) &amp;= p.
\end{align}\end{split}\]</div>
</div>
<div class="section" id="binomial">
<h3>Binomial<a class="headerlink" href="#binomial" title="Permalink to this headline">¶</a></h3>
<p>A random variable <span class="math notranslate nohighlight">\(X\)</span> is binomially distributed if</p>
<div class="math notranslate nohighlight">
\[ \mathbb{P}(X = k) = {n \choose k} p^k (1 - p)^{n - k} ~~\text{for}~~k = 0, 1, 2, ..., n.\]</div>
<p>The sum of <span class="math notranslate nohighlight">\(n\)</span> independent and identically distributed coin tosses is Bernoulli distributed.</p>
</div>
<div class="section" id="poisson">
<h3>Poisson<a class="headerlink" href="#poisson" title="Permalink to this headline">¶</a></h3>
<p>A random variable <span class="math notranslate nohighlight">\(X\)</span> is said to be Poisson-distributed with parameter <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> if</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{P}(X = k) = \frac{1}{k!} \lambda^k e^{-\lambda} &amp; \text{ for } k = 0, 1, 2 ...
\end{align}\]</div>
<p>The Poisson distribution is an appropriate model for <em>point data</em>. For example, if buses arrive at a local stop such that</p>
<ol class="simple">
<li><p>Each bus arrival occurs at a single point in time.</p></li>
<li><p>Arrivals are independent of each other.</p></li>
<li><p>The number <span class="math notranslate nohighlight">\(N_{t, t + dt}\)</span> of buses arriving within an infinitesimal time interval <span class="math notranslate nohighlight">\([t, t + dt]\)</span> follows</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mathbb{P}(N_{t, t+ dt} = 0) &amp;= 1 - \lambda dt + o(dt)\\
\mathbb{P}(N_{t, t+ dt} = 1) &amp;= \lambda dt + o(dt),
\end{align}\end{split}\]</div>
<p>then the number of events occuring within any time interval are Poisson-distributed.</p>
</div>
<div class="section" id="geometric">
<h3>Geometric<a class="headerlink" href="#geometric" title="Permalink to this headline">¶</a></h3>
<p>A random variable <span class="math notranslate nohighlight">\(X\)</span> has the geometric distribution with parameter <span class="math notranslate nohighlight">\(p\)</span> if</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{P}(X = k) = pq^k &amp; \text{ for } k = 0, 1, 2 ...
\end{align}\]</div>
<p>The geometric distribution naturally models random variables such as “the number of i.i.d. trials up to and including the first occurence of A”. For example, the number of i.i.d. coin tosses required until heads is obtained the first time, is geometrically distributed.</p>
</div>
<div class="section" id="negative-binomial">
<h3>Negative binomial<a class="headerlink" href="#negative-binomial" title="Permalink to this headline">¶</a></h3>
<p>A random variable <span class="math notranslate nohighlight">\(X\)</span> has the negative binomial distribution with parameters <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(p \in [0, 1]\)</span> if</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{P}(X = k) = {k - 1 \choose n - 1} p^n q^{k - n} &amp; \text{ for } k = n, n + 1, n + 2, ...
\end{align}\]</div>
<p>The number of i.i.d. coin tosses up to and including the <span class="math notranslate nohighlight">\(n^{th}\)</span> success is distributed according to the negative binomial distribution. To see this, consider that a sequence of trials up to the <span class="math notranslate nohighlight">\(n^{th}\)</span> success contains <span class="math notranslate nohighlight">\(n\)</span> successes and <span class="math notranslate nohighlight">\(k - n\)</span> failures, so the probability of obtaining that particular sequence is <span class="math notranslate nohighlight">\(p^nq^{k - n}\)</span>. To count the number of all possible sequences with <span class="math notranslate nohighlight">\(k - n\)</span> failures and <span class="math notranslate nohighlight">\(n\)</span> successes, consider that the last trial must be a success. That leaves us with <span class="math notranslate nohighlight">\(k - 1\)</span> choose <span class="math notranslate nohighlight">\(n - 1\)</span> ways to rearrange the remaining successes and failures, arriving at the final expression.</p>
<p>Note also that from the definition of this distribution, if <span class="math notranslate nohighlight">\(X_1, X_2, ..., X_n\)</span> are geometrically distributed with parameter <span class="math notranslate nohighlight">\(p\)</span>, then <span class="math notranslate nohighlight">\(X = X_1 + X_2 + ... + X_n\)</span> is negative-binomially distributed with parameters <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(p\)</span>.</p>
</div>
</div>
<div class="section" id="expectations">
<h2>Expectations<a class="headerlink" href="#expectations" title="Permalink to this headline">¶</a></h2>
<p>Although random variables are not perfectly determined, we can reason about the values they could attain. The expectation of a random variable captures the value that we expect the variable will have on average - as weighted by the probability measure.</p>
<div class='definition'>
<p><strong>Definition (Expectation)</strong> The <strong>expectation</strong> of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> is denoted by <span class="math notranslate nohighlight">\(\mathbb{E}(X)\)</span> and defined by</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}(X) = \sum_{x : x \in X(\Omega)} x \mathbb{P}(X = x).
\end{align}\]</div>
<p>whenever this sum converges absolutely, i.e. <span class="math notranslate nohighlight">\(\sum |x\mathbb{P}(X = x)| &lt; \infty.\)</span></p>
</div>
<br>
<p>The need for the last statement in this definition is that the expectation of a discrete random variable may not always exist. For example, if <span class="math notranslate nohighlight">\(X\)</span> has pmf</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(X = x) = \frac{A}{x^{3/2}} \text{ for } x = 1, 2, ...\]</div>
<p>where <span class="math notranslate nohighlight">\(A\)</span> is a normalising constant, then the sum in the definition of <span class="math notranslate nohighlight">\(\mathbb{E}(X)\)</span> does not converge. When taking expectations of functions of discrete random variables, the following result holds, that we often take for granted.</p>
<div class='theorem'>
<p><strong>Theorem (Law of the subconscious statistician)</strong> If <span class="math notranslate nohighlight">\(X\)</span> is a discrete random variable and <span class="math notranslate nohighlight">\(g : \mathbb{R} \to \mathbb{R}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}\left(g(x)\right) = \sum_{x \in \text{Im}X} x \mathbb{P}(X = x)
\end{align}\]</div>
<p>whenever this sum converges absolutely.</p>
</div>
<br>
<p>The above can be shown by defining <span class="math notranslate nohighlight">\(Y = g(X)\)</span> and considering the expectation
of <span class="math notranslate nohighlight">\(Y\)</span>, showing this is equal to <span class="math notranslate nohighlight">\(\mathbb{E}\left(g(X)\right)\)</span>. Two useful
results about discrete random variables are:</p>
<div class='theorem'>
<p><strong>Theorem (Two results for discrete random variables)</strong> Let <span class="math notranslate nohighlight">\(X\)</span> be a discrete
random variable and <span class="math notranslate nohighlight">\(a, b \in \mathbb{R}\)</span>, then</p>
<ol class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\mathbb{P}(X \geq 0) = 1\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}(X)\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{P}(X = 0
) = 1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(a X + b) = a\mathbb{E}(X) + b\)</span>.</p></li>
</ol>
</div>
<br>
<p>Appart from the expectation, the variance is another central quantity that
captures information about a random variable, particularly about its spread.</p>
<div class='definition'>
<p><strong>Definition (Variance)</strong> The <strong>variance</strong> of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> is denoted by <span class="math notranslate nohighlight">\(\text{Var}(X)\)</span> and defined by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\text{Var}(X) &amp;= \mathbb{E}\left([X - \mathbb{E}(X)]^2\right) \\
&amp;= \mathbb{E}(X^2) - \mathbb{E}(X)^2
\end{align}\end{split}\]</div>
</div>
<br>
</div>
<div class="section" id="conditional-expectations">
<h2>Conditional expectations<a class="headerlink" href="#conditional-expectations" title="Permalink to this headline">¶</a></h2>
<p>We are often interested in the expectation of a random variable <strong>conditioned on some event</strong>. For example, the event we are conditioning on could be some observed data, based on which we would like to update our beliefs about the random variable, and compute its expectation.</p>
<div class='definition'>
<p><strong>Definition (Conditional expectation)</strong> If <span class="math notranslate nohighlight">\(X\)</span> is a random variable and <span class="math notranslate nohighlight">\(\mathbb{P}(B) &gt; 0\)</span>, then the expectation of <span class="math notranslate nohighlight">\(X\)</span> conditioned on <span class="math notranslate nohighlight">\(B\)</span> is written <span class="math notranslate nohighlight">\(\mathbb{E}[X | B]\)</span> and defined by</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}[X | B] &amp;= \sum_{x \in \text{Im} X} x\mathbb{P}(X = x | B)
&amp;= \mathbb{E}(X^2) - \mathbb{E}(X)^2
\end{align}\]</div>
<p>whenever this sum converges absolutely.</p>
</div>
<br>
<p>As with conditional probabilities, there is a partition theorem associated with conditional expectations, also known as the law of total expectation.</p>
<div class='theorem'>
<p><strong>Theorem (Partition theorem for conditional expectations)</strong> If <span class="math notranslate nohighlight">\(X\)</span> is a discrete random variable and <span class="math notranslate nohighlight">\(\{B_1, B_2, ...\}\)</span> is a partition of <span class="math notranslate nohighlight">\(\Omega\)</span> with <span class="math notranslate nohighlight">\(\mathbb{P}(B_k) &gt; 0\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\mathbb{E}(X) = \sum_k \mathbb{E}(X | B_k)\mathbb{P}(B_k)
\end{align}\]</div>
<p>whenever this sum converges absolutely.</p>
</div>
<br></div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/prob-intro/ch02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../ch01/content.html" title="previous page">Events and Probabilities</a>
    <a class='right-next' id="next-link" href="../ch03/content.html" title="next page">Multivariate discrete distributions</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratos Markou<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>