# Multivariate discrete distributions

## Definition

The definition of the pmf of a discrete random variable can be extended into a
distribution over several random variables in the following way.

<div class='definition'>

**Definition (Joint probability mass function)** Given random variables $X$
 and $Y$ on $(\Omega, \mathcal{F}, \mathbb{P})$, the joint probability mass
  function over $X$ and $Y$ is the function $p_{X, Y} : \mathbb{R}^2 \to [0
  , 1]$ defined by
  
$$\begin{align}
p_{X, Y}(x, y) = \mathbb{P}\left({\omega \in \Omega : X(\omega) = x, Y
(\omega) = y}\right).
\end{align}$$

This is usually abbreviated to $p_{X, Y}(x, y) = \mathbb{P}\left(X = x, Y = y\right)$.

</div>

<br>

Using the additivity of $\mathbb{P}$, we can verify that $p_{X, Y}$ also
 satisfies the marginalisation property

$$\begin{align} p_X(x) = \sum_{y \in \text{Im}Y} \mathbb{P}\left(X = x, Y = y
\right),
\end{align}$$

and also since $\mathbb{P}(\Omega) = 1$ we have

$$\begin{align}
\sum_{x \in \text{Im}X} \sum_{y \in \text{Im}Y} p_{X, Y}(x, y) = 1.
\end{align}$$

This definition can be extended to multivariate distributions of more than
 two variables by adding more variables to the set being measured.
 
 ## Multivariate expectation and independence
 
We are often interested in taking the expectation of functions of multiple
 random variables, given by the following formula as intuition would suggest:
 
<div class='theorem'>

**Theorem (Law of the subconscious statistician - multivariate)** Let $X$ and
 $Y$ be discrete random variables on $(\Omega, \mathcal{F}, \mathbb{P})$ and
  $g : \mathbb{R}^2 \to \mathbb{R}$. Then
  
  $$\begin{align}
  \mathbb{E}(g(X, Y)) = \sum_{x \in \text{Im} X}\sum_{y \in \text{Im} Y} g(x
  , y) \mathbb{P}(X = x, Y = y)
  \end{align}$$
  
  whenever this sum converges absolutely.
  
</div>

<br>

Often, downstream calculations, including the expectation written above, can
 be simplified if the random variables are independent. Previously we defined
  independence in terms of events and we can extend this concept to variables
   in the following intuitive way.

<div class='definition'>

**Definition (Independence)** Two discrete random variables $X$ and $Y$ are
 independent if ${X = x}$ and ${Y = y}$ are independent for all $x, y \in
  \mathbb{R}$, and we typically abbreviate this condition as
  
  $$\begin{align}
  \mathbb{P}(X = x, Y = y) = \mathbb{P}(X = x)\mathbb{P}(Y = y) \text{ for
   } x, y \in \mathbb{R}.
  \end{align}$$
  
  Random variables which are not independent are called **dependent**.
  
</div>

<br>

Two discrete random variables are independent if their pmf can be expressed
 as the product of its marginals, or more generally as the product of
  functions of different arguments, as shown below.
  
 
<div class='theorem'>

**Theorem (Independence $\iff$ pmf factorises)** Two discrete random
 variables $X$ and $Y$ are independent if and only if there exist $f, g
  : \mathbb{R} \to \mathbb{R}$ such that
  
  $$\begin{align}
  p_{X, Y}(x, y) = f(x)g(y) \text{ for } x, y \mathbb{R}.
  \end{align}$$
  
</div>

<br>

This can be shown simply by showing that the product $f(x)g(y)$ is equal to
 $p_X(x)p_Y(y)$.
 
<div class='theorem'>

**Theorem (Expectation of product of independent variables)** If $X$ and $Y$ are
 independent discrete random variables, the expectation of their product is
  equal to the product of their expectations, as in
  
  $$\begin{align}
  \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y].
  \end{align}$$

</div>

<br>



