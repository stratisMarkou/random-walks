---
interact_link: content/gaussian-processes/sparse/fitc.ipynb
kernel_name: venv-random-walks
kernel_path: content/gaussian-processes/sparse
has_widgets: false
title: |-
  FITC
pagenum: 3
prev_page:
  url: /gaussian-processes/sparse/intro.html
next_page:
  url: /guide/01_overview.html
suffix: .ipynb
search: mathbf x bar f k p n y boldsymbol left right align fitc gp model theta fn mathcal mean data begin end lambda prior d sigma covariance generative different inducing noise input points not m full level conditional sparse process original given its assumptions well obtain posterior likelihood marginal q training spgp pseudo particular variables predictions modelling top sim observations function predictive rule gaussian where br independent method matrix also case lets define set inputs random sampled further sample conditioned independently text vanilla under independence dependent point large apply bayes form mu b priors trainable learn fully approximation works new

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">FITC</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-the-Fully-Independent-Training-Conditional-(FITC-aka-SPGP)?">What is the Fully Independent Training Conditional (FITC aka SPGP)?<a class="anchor-link" href="#What-is-the-Fully-Independent-Training-Conditional-(FITC-aka-SPGP)?"> </a></h2><p>The Sparse Pseudo-input Guassian Process (SPGP) approximation is a particular method moodelling with a sparse, and therefore cheaply invertible, covariance matrix. This method discards the original GP model and works with a <em>new model</em>, which assumes the observed data are independent given certain <em>latent variables</em> called <em>pseudo-points</em>, leading to the sparse covariance structure. SPGP has also been called the Fully Independent Training Conditional (FITC) approximation and we adopt the same nomenclature because it's more appropriate and specific - there are in fact several sparse pseudo-input GP approximations, so SPGP is not very specific a name.</p>
<p>In any case, it's worth repeating that FITC works by throwing the original GP away and working with a computationally cheaper model which will hopefully approximate the predictions of the original. Let's start by laying out the modelling assumptions and the generative process and then moving on to inference and predictions.</p>
<h2 id="The-FITC-generative-model-and-assumptions">The FITC generative model and assumptions<a class="anchor-link" href="#The-FITC-generative-model-and-assumptions"> </a></h2><p>Consider the following model. We define a set of $M$ inputs $\mathbf{\bar{X}} = \{\mathbf{\bar{x}}_m\}_{m=1}^M$, which we call the <em>inducing-</em> or <em>pseudo-points</em>. We also define a set of $M$ unobserved random variables $\mathbf{\bar{f}} = (\bar{f_1}, \bar{f_2}, ..., \bar{f_M})^\top$, and place a GP prior over them:</p>
$$p\left(\mathbf{\bar{f}} | \mathbf{\bar{X}}\right) \sim \mathcal{N}\left(\mathbf{0}, \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}\right).$$<p>Given the sampled $\mathbf{\bar{f}}$ as well as $N$ further inputs $\mathbf{X} = \{\mathbf{\bar{x}}_n\}_{n=1}^N$, we sample $N$ further random variables from the GP prior conditioned on $\mathbf{\bar{f}}$ <em>independently from each other</em>:</p>
$$p \left(f_n | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{x}_n \right) \sim \mathcal{N}\left(f_n;~\mathbf{K}_{\mathbf{x}_n\mathbf{\bar{X}}} \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{\bar{f}},~\mathbf{K}_{\mathbf{x}_n\mathbf{x}_n} - \mathbf{K}_{\mathbf{x}_n\mathbf{X}}\mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{K}_{\mathbf{X}\mathbf{x}_n} \right) \text{ independently for each } n.$$<p>Finally, some idependent and identically distributed onse is added to the $\mathbf{f}$ to obtain the noisy observations $\mathbf{y}$:</p>
\begin{align}
p(\mathbf{y} | \mathbf{f}) = \mathcal{N}\left(\mathbf{y}; \mathbf{f}, \sigma^2 \mathbf{I}\right),
\end{align}<p>and that's how we obtain the observed data $\mathbf{y}$. Let sample some data accoding to this generative process and see what it looks like. We'll work with a zero-mean GP with an Exponentiated Quadratic covariance function - imports and definitions are hidden below.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">eq_covariance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span>
                  <span class="n">x2</span><span class="p">,</span>
                  <span class="n">coeff</span><span class="p">,</span>
                  <span class="n">scale</span><span class="p">,</span>
                  <span class="n">diag_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                  <span class="n">epsilon</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="c1"># If not calculating diagonal only, expand to broadcast</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">diag_only</span><span class="p">:</span>

        <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

    <span class="c1"># Compute differences</span>
    <span class="n">diffs</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span>

    <span class="c1"># Compute quadratic form</span>
    <span class="n">quad</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">diffs</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">scale</span>
    <span class="n">quad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">quad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Exponentiate and multiply by covariance coeff</span>
    <span class="n">exp_quad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">quad</span><span class="p">)</span>
    <span class="n">eq_cov</span> <span class="o">=</span> <span class="n">coeff</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">exp_quad</span>

    <span class="c1"># Add epsilon for invertibility</span>
    <span class="k">if</span> <span class="n">epsilon</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

        <span class="n">eq_cov</span> <span class="o">=</span> <span class="n">eq_cov</span> <span class="o">+</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">eq_cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">eq_cov</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Set random seed - change to see different samples</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Num. inducing points (M), num. observations (N)</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># EQ hyperparameters</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="mf">1e0</span>
<span class="n">scale</span> <span class="o">=</span> <span class="mf">1e0</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">1e-1</span>

<span class="c1"># Pick inducing and observed inputs at random</span>
<span class="n">x_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mf">3.</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mf">4.</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Compute covariance matrix terms</span>
<span class="n">K_ind_ind</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">x_ind</span><span class="p">,</span> <span class="n">x_ind</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
<span class="n">K_ind_obs</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">x_ind</span><span class="p">,</span> <span class="n">x_obs</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">K_obs_ind</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">x_obs</span><span class="p">,</span> <span class="n">x_ind</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">K_obs_diag</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">x_obs</span><span class="p">,</span> <span class="n">x_obs</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">diag_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Sample f_ind | x_ind</span>
<span class="n">f_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">),</span>
               <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Mean and vaariance of normal distribution of f_obs | f_ind</span>
<span class="n">f_obs_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_obs_ind</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">,</span> <span class="n">f_ind</span><span class="p">))</span>
<span class="n">f_obs_var</span> <span class="o">=</span> <span class="n">K_obs_diag</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_obs_ind</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">,</span> <span class="n">K_ind_obs</span><span class="p">)))</span>
<span class="n">f_obs_var</span> <span class="o">=</span> <span class="n">f_obs_var</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># Sample f_obs | f_ind from normal i.i.d.</span>
<span class="n">f_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">f_obs_mean</span><span class="p">,</span> <span class="n">f_obs_var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Sample y_obs | f_obs (noisy observations)</span>
<span class="n">y_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">f_obs</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>

<span class="c1"># Locations to plot mean and variance of generative model, y_plot | f_ind, x_plot</span>
<span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># Covariances between inducing points and input locations</span>
<span class="n">K_ind_plot</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">x_ind</span><span class="p">,</span> <span class="n">x_plot</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">K_plot_ind</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">x_ind</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">K_plot_diag</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">x_plot</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">diag_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Mean and standard deviation of y_plot | f_ind, x_plot</span>
<span class="n">y_plot_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_plot_ind</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">,</span> <span class="n">f_ind</span><span class="p">))[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">f_plot_var</span> <span class="o">=</span> <span class="n">K_plot_diag</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_plot_ind</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">,</span> <span class="n">K_ind_plot</span><span class="p">)))</span>
<span class="n">y_plot_var</span> <span class="o">=</span> <span class="n">f_plot_var</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">y_plot_std</span> <span class="o">=</span> <span class="n">y_plot_var</span> <span class="o">**</span> <span class="mf">0.5</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Plot inducing points and observed data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot inducing points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ind</span><span class="p">,</span>
            <span class="n">f_ind</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span>
            <span class="n">zorder</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathbf{\bar</span><span class="si">{f}</span><span class="s1">}$&#39;</span><span class="p">)</span>

<span class="c1"># Plot observed data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_obs</span><span class="p">,</span>
            <span class="n">y_obs</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span>
            <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{y}</span><span class="s1">$&#39;</span><span class="p">)</span>

<span class="c1"># Plot mean of generative model</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span>
         <span class="n">y_plot_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> 
         <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot noise of generative model</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_plot</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                 <span class="n">y_plot_mean</span> <span class="o">-</span> <span class="n">y_plot_std</span><span class="p">,</span>
                 <span class="n">y_plot_mean</span> <span class="o">+</span> <span class="n">y_plot_std</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
                 <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\pm 1$ std.&#39;</span><span class="p">)</span>

<span class="c1"># Plot formatting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Samples from FITC generative model&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_3_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Interpretation-and-comparison-to-the-full-GP">Interpretation and comparison to the full GP<a class="anchor-link" href="#Interpretation-and-comparison-to-the-full-GP"> </a></h2><h3 id="Comparison-to-the-full-GP">Comparison to the full GP<a class="anchor-link" href="#Comparison-to-the-full-GP"> </a></h3><p>So how is the FITC generative model different from a zero-mean GP with an EQ covariance? In the vanilla GP setting, all observations are correlated and the joint distribution does not factor. Suppose we drew $\mathbf{\bar{f}}$ from a GP prior:</p>
$$\mathbf{\bar{f}} | \mathbf{\bar{X}} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}\right).$$<p>The way to draw $\mathbf{f}$ further samples <em>conditioned on</em> $\mathbf{\bar{f}}$ <em>under the vanilla GP model</em> is to use the conditional prior:</p>
\begin{align}
p(\mathbf{f} | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X}) = \mathcal{N}\left(\mathbf{f} ;~\mathbf{K}_{\mathbf{X}\mathbf{\bar{X}}} \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{\bar{f}},~\mathbf{K}_{\mathbf{X}\mathbf{X}} - \mathbf{K}_{\mathbf{X}\mathbf{X}}\mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{K}_{\mathbf{X}\mathbf{X}} \right).
\end{align}<p>Since the matrix $\mathbf{K}_{\mathbf{X}\mathbf{X}} - \mathbf{K}_{\mathbf{X}\mathbf{X}}\mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{K}_{\mathbf{X}\mathbf{X}}$ is in general full-rank, it includes correlations between all $f$'s and the joint over $\mathbf{f}$ cannot be simplified:</p>
\begin{align}
p(\mathbf{f} | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X}) &amp;= p(f_{n-1}, ..., f_{1}| f_n, \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})p(f_n | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})\\
&amp;=p(f_{n-2}, ..., f_1| f_{n-1}, f_n, \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})p(f_{n-1} | f_n \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})p(f_n | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})\\
&amp;...\\
&amp;=p(f_1| f_2, ..., f_n, \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})~p(f_2| f_3, ..., f_n, \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})~...~p(f_n | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X}).
\end{align}<p>By contrast, the generative process of FITC says: given $\mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X}$, my $f_n$ samples will all be drawn from the conditional prior, but they will be drawn independently from each other. Under this generative process we have $p(f_i | f_j, \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})$ = $p(f_i |\mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})$ for $i \neq j$ and we can simplify the conditional prior as:</p>
\begin{align}
p(\mathbf{f} | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X}) &amp;= p(f_1, ..., f_{n-1}| f_n, \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})p(f_n | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})\\
&amp;=p(f_1, ..., f_{n-2}| f_n, f_{n-1}, \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})p(f_{n-1} | f_n \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})p(f_n | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})\\
&amp;...\\
&amp;=p(f_1| \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X})~...~p(f_n | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X}).
\end{align}<p>It's worthwhile stressing that FITC is just a different model. It can be considered as an attempt to approximate the predictions made by a full GP, but its modelling assumption about independence makes is a fundamentally different model from the full GP.</p>
<h3 id="Interpretation-as-an-input-dependent-noise-regressor">Interpretation as an input dependent noise regressor<a class="anchor-link" href="#Interpretation-as-an-input-dependent-noise-regressor"> </a></h3><p>Snelson and Ghahramani point out that FITC can be regarded as a regression model with an input-dependent noise level, since in the predictive posterior:</p>
\begin{align}
p \left(y_n | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{x}_n \right) \sim \mathcal{N}\left(f_n;~\mathbf{K}_{\mathbf{x}_n\mathbf{\bar{X}}} \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{\bar{f}},~\mathbf{K}_{\mathbf{x}_n\mathbf{x}_n} - \mathbf{K}_{\mathbf{x}_n\mathbf{X}}\mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{K}_{\mathbf{X}\mathbf{x}_n} + \sigma^2 \mathbf{I} \right),
\end{align}<p>both the predictive variance is a function of $\mathbf{x}$. However, the FITC model couples the predictive mean and variance in a potentially undesirable way. In particular, under FITC it is not possible to have a posterior predictive with a mean that is far from the prior mean whilst maintaining a large noise level. The noise level in FITC corresponds to the uncertainty of a full GP conditioned on the inducing points. This uncertainty can only increase by moving away from the inducing points.</p>
<h2 id="Inference-and-prediction">Inference and prediction<a class="anchor-link" href="#Inference-and-prediction"> </a></h2><p>We have looked at the generative assumptions of FITC and seen how to sample $\mathbf{\bar{f}}$, then $\mathbf{f} | \mathbf{\bar{f}}$, then $\mathbf{y} | \mathbf{f}$ (forward probability). How about inferring $p(\mathbf{\bar{f}} | \mathbf{y})$? We can apply Bayes' rule for Gaussian variables to the prior and likelihood</p>
\begin{align}
p\left(\mathbf{\bar{f}} | \mathbf{\bar{X}}\right) &amp;= \mathcal{N}\left(\mathbf{0}, \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}\right)\\
p\left(\mathbf{y} | \mathbf{\bar{f}}, \mathbf{\bar{X}}, \mathbf{X} \right) &amp;= \mathcal{N}\left(f_n;~\mathbf{K}_{\mathbf{X}\mathbf{\bar{X}}} \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{\bar{f}},~\mathbf{D} + \sigma^2 \mathbf{I} \right),
\end{align}<p>where $\mathbf{D}$ is a diagonal matrix with entries $D_{nn} = \mathbf{K}_{\mathbf{x}_n\mathbf{x}_n} - \mathbf{K}_{\mathbf{x}_n\mathbf{X}}\mathbf{K}_{\mathbf{X}\mathbf{X}}^{-1}\mathbf{K}_{\mathbf{X}\mathbf{x}_n}$, we can apply Bayes' rule for Gaussian distributions to obtain the marginal of $\mathbf{y}$ and the posterior $\mathbf{\bar{f}}| \mathbf{y}$</p>
\begin{align}
p(\mathbf{y}|\mathbf{\bar{X}}, \boldsymbol{\theta}) &amp;= \mathcal{N}\left(\mathbf{y}; \mathbf{0}, \mathbf{K}_{\mathbf{X}\mathbf{\bar{X}}} \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{K}_{\mathbf{\bar{X}}\mathbf{X}} + \mathbf{D} + \sigma^2 \mathbf{I}\right)\\
p(\mathbf{\bar{f}} | \mathbf{y}, \mathbf{\bar{X}}, \mathbf{X}) &amp;= \mathcal{N}\left(\mathbf{\bar{f}}; \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}} \mathbf{Q}^{-1} \mathbf{K}_{\mathbf{\bar{X}} \mathbf{X}} \left( \mathbf{D} + \sigma^2 \mathbf{I} \right)^{-1} \mathbf{y}, \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}} \mathbf{Q}^{-1} \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}} \right)
\end{align}<p>where $\mathbf{Q} = \left(\mathbf{K}_{\mathbf{\bar{X}} \mathbf{\bar{X}}} + \mathbf{K}_{\mathbf{\bar{X}}\mathbf{X}} \left( \mathbf{D} + \sigma^2 \mathbf{I} \right) \mathbf{K}_{\mathbf{X}\mathbf{\bar{X}}} \right)$. To make a prediction at a new input $\mathbf{\bar{x}_*}$, we integrate out $f_*$ according to the posterior:</p>
\begin{align}
p(y_*| \mathbf{x}_*, \mathbf{y}, \mathbf{X}, \mathbf{\bar{X}}) &amp;= \int p(y_*| f_*) p(f_* | \mathbf{x}_*, \mathbf{y}, \mathbf{X}, \mathbf{\bar{X}}) d\mathbf{\bar{f}} \\
&amp;= \mathcal{N}\left(y_*; \mathbf{K}_{\mathbf{x}_* \mathbf{\bar{X}}} \mathbf{Q}^{-1} \mathbf{K}_{\mathbf{\bar{X}} \mathbf{X}} \left( \mathbf{D} + \sigma^2 \mathbf{I} \right)^{-1} \mathbf{y}, \mathbf{K}_{\mathbf{x_*} \mathbf{x_*}} - \mathbf{K}_{\mathbf{x_*}\mathbf{\bar{X}}} \left( \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1} - \mathbf{Q}^{-1} \right) \mathbf{K}_{\mathbf{\bar{X}}\mathbf{x_*}} + \sigma^2 \right)
\end{align}<p><details></p>
<p><summary>Bayes' rule for Gaussian distributions</summary>
<br>
Assuming a Gaussian prior and a likelihood of the form
\begin{align}
p(\mathbf{x}) &amp;= \mathcal{N}\left(\mathbf{x}; \boldsymbol{\mu}_{\mathbf{x}}, \boldsymbol{\Lambda}^{-1}_{\mathbf{x}} \right)\\
p(\mathbf{y} | \mathbf{x}) &amp;= \mathcal{N}\left(\mathbf{y}; \mathbf{A}\mathbf{x} + \mathbf{b}, \boldsymbol{\Lambda}^{-1}_{\mathbf{y} | \mathbf{x}} \right)
\end{align}
<br>
respectively, we can apply Bayes' rule to obtain the marginal of $\mathbf{y}$ and posterior $\mathbf{x}| \mathbf{y}$:
<br></p>
\begin{align}
p(\mathbf{y}) &amp;= \mathcal{N}\left(\mathbf{y}; \mathbf{A}\boldsymbol{\mu}_{\mathbf{x}} + \mathbf{b}, \boldsymbol{\Lambda}^{-1}_{\mathbf{y} | \mathbf{x}} + \mathbf{A} \boldsymbol{\Lambda}^{-1}_{\mathbf{x}}\mathbf{A}^\top \right)\\
p(\mathbf{x} | \mathbf{y}) &amp;= \mathcal{N}\left(\mathbf{y}; \boldsymbol{\Lambda}_{\mathbf{x}|\mathbf{y}}^{-1} \left( \mathbf{A}^\top\boldsymbol{\Lambda}_{\mathbf{y} | \mathbf{x}} (\mathbf{y} - \mathbf{b}) +  \boldsymbol{\Lambda}_{\mathbf{x}} \boldsymbol{\mu}_{\mathbf{x}}\right), \boldsymbol{\Lambda}_{\mathbf{x}|\mathbf{y}}^{-1} \right)
\end{align}<p><br>
where $\boldsymbol{\Lambda}_{\mathbf{x}|\mathbf{y}} = \left(\boldsymbol{\Lambda}_{\mathbf{x}} + \mathbf{A}^\top\boldsymbol{\Lambda}_{\mathbf{y} | \mathbf{x}}\mathbf{A} \right)$.
&lt;/details&gt;
*/</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-$\mathbf{\bar{X}}$-and-$\boldsymbol{\theta}$">Learning $\mathbf{\bar{X}}$ and $\boldsymbol{\theta}$<a class="anchor-link" href="#Learning-$\mathbf{\bar{X}}$-and-$\boldsymbol{\theta}$"> </a></h2><p>Up to now, we have pretended to know the inducing inputs $\mathbf{\bar{X}}$ and kernel hyperparameters $\boldsymbol{\theta}$, but in general this will not be the case. In practice, we will be interested iin setting $\mathbf{\bar{X}}$ and $\boldsymbol{\theta}$ from the data. One approach would be to set hyper-priors $p(\mathbf{\bar{X}})$ and $p(\boldsymbol{\theta})$ and integrate out $\mathbf{\bar{X}}$ and $\boldsymbol{\theta}$, but this would be challenging because the associated expressions would not have a closed form. Instead, we can still use the priors $p(\mathbf{\bar{X}})$ and $p(\boldsymbol{\theta})$ to obtain maximum-a-posteriori point estimates:</p>
\begin{align}
\mathbf{\bar{X}}_{MAP}, \boldsymbol{\theta}_{MAP} = \text{argmax}_{\mathbf{\bar{X}}, \boldsymbol{\theta}}~ \left[p(\mathbf{y}|\mathbf{\bar{X}}, \boldsymbol{\theta}) p(\mathbf{\bar{X}}, \boldsymbol{\theta})\right]
\end{align}<p>In the case of flat priors, which are uniform in the allowed range of $\mathbf{\bar{X}}$ and $\boldsymbol{\theta}$, this reduces to maximising the marginal likelihood:</p>
\begin{align}
\mathbf{\bar{X}}_{ML}, \boldsymbol{\theta}_{ML} = \text{argmax}_{\mathbf{\bar{X}}, \boldsymbol{\theta}}~ p(\mathbf{y}|\mathbf{\bar{X}}, \boldsymbol{\theta}).
\end{align}<p>The marginal likelihood $p(\mathbf{y}|\mathbf{\bar{X}}, \boldsymbol{\theta})$ can be expressed in closed form as:</p>
\begin{align}
p(\mathbf{y}|\mathbf{\bar{X}}, \boldsymbol{\theta}) &amp;= \int p(\mathbf{y}| \mathbf{\bar{f}}, \mathbf{\bar{X}}, \boldsymbol{\theta}) p(\mathbf{\bar{f}} | \mathbf{\bar{X}}) d\mathbf{\bar{f}}\\
&amp;= \mathcal{N}\left(\mathbf{y}; \mathbf{0}, \mathbf{K}_{\mathbf{X}\mathbf{\bar{X}}} \mathbf{K}_{\mathbf{\bar{X}}\mathbf{\bar{X}}}^{-1}\mathbf{K}_{\mathbf{\bar{X}}\mathbf{X}} + \mathbf{D} + \sigma^2 \mathbf{I}\right)
\end{align}
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implementation">Implementation<a class="anchor-link" href="#Implementation"> </a></h2><p>Let's see what an implementation of FITC might look like. We'll define classes for the mean and covariance function and have them inherit from <code>tf.keras.Model</code>, to make them trainable.</p>
<h3 id="Mean-and-Covariance">Mean and Covariance<a class="anchor-link" href="#Mean-and-Covariance"> </a></h3><p>We'll use a constant mean function, with a trainable mean and an exponentiated quadratic (EQ) covariance.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">class</span> <span class="nc">constant_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;eq_covariance&#39;</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">constant</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
        
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">constant</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    
    
<span class="k">class</span> <span class="nc">eq_covariance</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">log_coeff</span><span class="p">,</span>
                 <span class="n">log_scales</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;eq_covariance&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
        <span class="c1"># Convert parameters to tensors</span>
        <span class="n">log_coeff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_coeff</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">log_scales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_scales</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Reshape parameter tensors</span>
        <span class="n">log_coeff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">log_coeff</span><span class="p">)</span>
        <span class="n">log_scales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">log_scales</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">assert</span> <span class="n">log_scales</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">dim</span><span class="p">,</span>                \
            <span class="sa">f</span><span class="s1">&#39;Expected the size of scales at axis 2 &#39;</span>    <span class="o">+</span> \
            <span class="sa">f</span><span class="s1">&#39;to be dim, found shapes </span><span class="si">{</span><span class="n">scales</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> &#39;</span>   <span class="o">+</span> \
            <span class="sa">f</span><span class="s1">&#39;and </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s1">.&#39;</span>

        <span class="k">assert</span> <span class="n">log_coeff</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(),</span>                     \
            <span class="sa">f</span><span class="s1">&#39;Expected coeff to be a single scalar, &#39;</span>   <span class="o">+</span> \
            <span class="sa">f</span><span class="s1">&#39;found coeff.shape == </span><span class="si">{</span><span class="n">coeff</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">.&#39;</span>
        
        <span class="c1"># Set input dimensionality</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        
        <span class="c1"># Set EQ parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_scales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">log_scales</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_coeff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">log_coeff</span><span class="p">)</span>
        
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">scales</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_scales</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.</span>
    
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">coeff</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_coeff</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.</span>
        
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">x1</span><span class="p">,</span>
                 <span class="n">x2</span><span class="p">,</span>
                 <span class="n">diag_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="c1"># Reshape input tensors</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Check dimensions are correct</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">and</span>       \
               <span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">x2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">),</span>   \
            <span class="sa">f</span><span class="s1">&#39;Expected x1 and x2 to have 2 dimensions &#39;</span>  <span class="o">+</span> \
            <span class="sa">f</span><span class="s1">&#39;and to both match self.dim at second &#39;</span>     <span class="o">+</span> \
            <span class="sa">f</span><span class="s1">&#39;dimension, instead found shapes &#39;</span>          <span class="o">+</span> \
            <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> and </span><span class="si">{</span><span class="n">x2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">.&#39;</span>

        <span class="n">scales</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales</span>
        
        <span class="c1"># If not calculating diagonal only, expand to broadcast</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">diag_only</span><span class="p">:</span>

            <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

            <span class="n">scales</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

        <span class="c1"># Compute differences</span>
        <span class="n">diffs</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span>

        <span class="c1"># Compute quadratic form</span>
        <span class="n">quad</span> <span class="o">=</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">diffs</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">scales</span>
        <span class="n">quad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">quad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Exponentiate and multiply by covariance coeff</span>
        <span class="n">exp_quad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">quad</span><span class="p">)</span>
        <span class="n">eq_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">exp_quad</span>
        
        <span class="c1"># Add epsilon for invertibility</span>
        <span class="k">if</span> <span class="n">epsilon</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            
            <span class="n">eq_cov</span> <span class="o">=</span> <span class="n">eq_cov</span> <span class="o">+</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">eq_cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">eq_cov</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-FITC-GP-model">The FITC GP model<a class="anchor-link" href="#The-FITC-GP-model"> </a></h3><p>We'll also make the GP itself a trainable model, which takes the training data and inducing point initialisation, a mean, a covariance and an initial log-noise level.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">FITCGP</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">x_train</span><span class="p">,</span>
                 <span class="n">y_train</span><span class="p">,</span>
                 <span class="n">x_ind_init</span><span class="p">,</span>
                 <span class="n">mean</span><span class="p">,</span>
                 <span class="n">cov</span><span class="p">,</span>
                 <span class="n">log_noise</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;sparse-gp&#39;</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># Set training data and inducing point initialisation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
                                            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span>
                                            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">,</span>
                                          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">)</span>
        
        <span class="c1"># Set mean and covariance functions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cov</span> <span class="o">=</span> <span class="n">cov</span>
    
        <span class="c1"># Set log of noise parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">log_noise</span><span class="p">,</span>
                                              <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span><span class="p">)</span>
        
        
        
    <span class="k">def</span> <span class="nf">post_pred</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">):</span>
        
        <span class="c1"># Compute D matrix (diagonal) plus noise</span>
        <span class="n">D_diag_plus_noise</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D_diag</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span>
        
        <span class="c1"># Compute Q matrix</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span>
        
        <span class="c1"># Covariance between prediction and inducing points</span>
        <span class="n">K_pred_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span>
                              <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">)</span>
        
        <span class="n">K_ind_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">K_pred_ind</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        
        <span class="c1"># Covariance between inducing and training points</span>
        <span class="n">K_ind_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">)</span>
        
        <span class="c1"># Covariance between inducing and training points</span>
        <span class="n">K_ind_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span>
                             <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        
        <span class="c1"># Compute diagonal of covariance between prediction points</span>
        <span class="n">K_pred_pred_diag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span>
                                    <span class="n">x_pred</span><span class="p">,</span>
                                    <span class="n">diag_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Compute inversions to use one einsum at the end</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">)</span>
        <span class="n">Q_inv_K_ind_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K_ind_train</span><span class="p">)</span>
        <span class="n">D_diag_plus_noise_inv_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">D_diag_plus_noise</span> <span class="o">**</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span>
        
        <span class="c1"># Compute mean of posterior predictive</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij, jk, k -&gt; i&#39;</span><span class="p">,</span>
                         <span class="n">K_pred_ind</span><span class="p">,</span>
                         <span class="n">Q_inv_K_ind_train</span><span class="p">,</span>
                         <span class="n">D_diag_plus_noise_inv_y</span><span class="p">)</span>
        
        <span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>
        
        <span class="c1"># Compute inversions</span>
        <span class="n">K_ind_ind_inv_K_ind_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">,</span> <span class="n">K_ind_pred</span><span class="p">)</span>
        <span class="n">Q_inv_K_ind_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K_ind_pred</span><span class="p">)</span>
        
        <span class="n">diff_term</span> <span class="o">=</span> <span class="n">K_ind_ind_inv_K_ind_pred</span> <span class="o">-</span> <span class="n">Q_inv_K_ind_pred</span>
        <span class="n">diff_term</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij, ji -&gt; i&#39;</span><span class="p">,</span>
                              <span class="n">K_pred_ind</span><span class="p">,</span>
                              <span class="n">diff_term</span><span class="p">)</span>
        
        <span class="n">var</span> <span class="o">=</span> <span class="n">K_pred_pred_diag</span> <span class="o">-</span> <span class="n">diff_term</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span>
        
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span>
    
    
    <span class="k">def</span> <span class="nf">log_lik</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the log marginal likelihood.</span>
<span class="sd">        </span>
<span class="sd">        Diagonal A : A = D + sigma^2 I</span>
<span class="sd">        Covariance : B = A + K_nm K_mm^-1 K_mn</span>
<span class="sd">        Cholesky V : VVT = C = K_mm + K_mn A^-1 K_nm</span>
<span class="sd">        Precision  : L = A^-1 - (A^-1 K_nm VT^-1) (V^-1 K_mn A^-1) = A^-1 - UT U</span>
<span class="sd">        </span>
<span class="sd">        LogNorm    : -0.5 * (N * log(2 * pi) + log|A + K_nm K_mm^-1 K_mn|)</span>
<span class="sd">        LogNorm    : log|A + K_nm K_mm^-1 K_mn| =</span>
<span class="sd">                        = log|K_mm + K_mn A^-1 K_nm| - log|K_mm| + log|A|</span>
<span class="sd">                    </span>
<span class="sd">        Quad: -0.5 * yT L y = </span>
<span class="sd">                    = -0.5 * yT A^-1 y + 0.5 * yT (V^-1 K_mn C^-1)T (V^-1 K_mn C^-1) y</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="n">A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D_diag</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span>
        
        <span class="n">K_ind_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span>
                             <span class="n">epsilon</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        
        <span class="n">K_ind_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">)</span>
        
        <span class="c1"># Compute V</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">K_ind_ind</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">C</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij, j, kj -&gt; ik&#39;</span><span class="p">,</span>
                          <span class="n">K_ind_train</span><span class="p">,</span>
                          <span class="n">A</span> <span class="o">**</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">K_ind_train</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
        
        <span class="c1"># Compute U</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">K_ind_train</span> <span class="o">/</span> <span class="n">A</span><span class="p">)</span>
        
        <span class="c1"># Difference between mean and y_train</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
        
        <span class="c1"># Compute quadratic form</span>
        <span class="n">U_diff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij, j -&gt; i&#39;</span><span class="p">,</span>
                        <span class="n">U</span><span class="p">,</span>
                        <span class="n">diff</span><span class="p">)</span>
        
        <span class="n">quad</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">diff</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">A</span><span class="p">)</span>
        <span class="n">quad</span> <span class="o">=</span> <span class="n">quad</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">U_diff</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">logdet</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">slogdet</span><span class="p">(</span><span class="n">C</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">logdet</span> <span class="o">=</span> <span class="n">logdet</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">slogdet</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">logdet</span> <span class="o">=</span> <span class="n">logdet</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
        
        <span class="n">log_lik</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">logdet</span> <span class="o">+</span> <span class="n">quad</span>
        
        <span class="k">return</span> <span class="n">log_lik</span>
    
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">D_diag</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="c1"># Covariance between training points (diagnal components)</span>
        <span class="n">K_train_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
                                 <span class="n">diag_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Covariance between training and inducing points</span>
        <span class="n">K_train_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">)</span>
        
        <span class="n">K_ind_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">K_train_ind</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        
        <span class="c1"># Covariance between inducing points</span>
        <span class="n">K_ind_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span>
                             <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
        
        <span class="n">K_ind_ind_inv_K_ind_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">K_ind_ind</span><span class="p">),</span>
                                                             <span class="n">K_ind_train</span><span class="p">)</span>
        
        <span class="c1"># Compute diagonal D matrix</span>
        <span class="n">K_inv_term</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;nm, mn -&gt; n&#39;</span><span class="p">,</span>
                               <span class="n">K_train_ind</span><span class="p">,</span>
                               <span class="n">K_ind_ind_inv_K_ind_train</span><span class="p">)</span>
        
        <span class="n">D</span> <span class="o">=</span> <span class="n">K_train_train</span> <span class="o">-</span> <span class="n">K_inv_term</span>
        
        <span class="k">return</span> <span class="n">D</span>
    
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">noise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_noise</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">Q</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="n">K_ind_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">)</span>
        
        <span class="n">K_train_ind</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">x_ind</span><span class="p">)</span>
        
        <span class="n">D_diag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D_diag</span>
        <span class="n">D_diag_plus_noise_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">D_diag</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mi">1</span>
        
        <span class="n">Q</span> <span class="o">=</span> <span class="n">K_ind_ind</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;nm, n, nk -&gt; mk&#39;</span><span class="p">,</span>
                          <span class="n">K_train_ind</span><span class="p">,</span>
                          <span class="n">D_diag_plus_noise_inv</span><span class="p">,</span>
                          <span class="n">K_train_ind</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">Q</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="A-sanity-check:-train-on-data-generated-by-FITC">A sanity check: train on data generated by FITC<a class="anchor-link" href="#A-sanity-check:-train-on-data-generated-by-FITC"> </a></h2><p>Before training on random data, we better make sure the model can learn to fit data that are sampled from it. In particular, we are interested to see that the model can learn the $\boldsymbol{\theta}$ and $\mathbf{\bar{X}}$ which were used to generate the data, starting from a different initial $\boldsymbol{\theta}$ and $\mathbf{\bar{X}}$. If it doesn't, then we know something's wrong. So, let's try to learn the dataset we sampled previously, from a different initialisation:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input tag_hide_output">

<div class="cell border-box-sizing code_cell rendered tag_hide_input tag_hide_output">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
         <span class="n">x_pred</span><span class="p">,</span>
         <span class="n">x_train</span><span class="p">,</span>
         <span class="n">y_train</span><span class="p">,</span>
         <span class="n">x_ind_prev</span><span class="p">,</span>
         <span class="n">x_ind_init</span><span class="p">,</span>
         <span class="n">step</span><span class="p">):</span>

    <span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">post_pred</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>

    <span class="n">x_pred</span> <span class="o">=</span> <span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">x_ind_curr</span> <span class="o">=</span> <span class="n">fitc_gp</span><span class="o">.</span><span class="n">x_ind</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span>
                     <span class="n">mean</span> <span class="o">-</span> <span class="n">var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">mean</span> <span class="o">+</span> <span class="n">var</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ind_curr</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">5.</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x_ind_curr</span><span class="p">),</span>
                <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Current $\bar{\mathbf</span><span class="si">{X}</span><span class="s1">}$&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ind_prev</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">5.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x_ind_prev</span><span class="p">),</span>
                <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Previous $\bar{\mathbf</span><span class="si">{X}</span><span class="s1">}$&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">,</span>
                <span class="mf">5.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">),</span>
                <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Init. $\bar{\mathbf</span><span class="si">{X}</span><span class="s1">}$&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;FITC after </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s1"> optimisation steps&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    
<span class="k">def</span> <span class="nf">print_numbers</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
    
    <span class="n">log_lik</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">log_lik</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Step: </span><span class="si">{</span><span class="n">step</span><span class="si">:</span><span class="s1">5&gt;</span><span class="si">}</span><span class="s1"> &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;Log evidence: </span><span class="si">{</span><span class="n">log_lik</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s1">8.3f</span><span class="si">}</span><span class="s1"> &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;Log coeff: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">cov</span><span class="o">.</span><span class="n">log_coeff</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s1">5.2f</span><span class="si">}</span><span class="s1"> &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;Log scales: </span><span class="si">{</span><span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">num</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cov</span><span class="o">.</span><span class="n">log_scales</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span><span class="si">}</span><span class="s1"> &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;Log noise: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">log_noise</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s1">5.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow_probability</span> <span class="k">as</span> <span class="nn">tfp</span>

<span class="c1"># Set random seed and tensor dtype</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>

<span class="c1"># Number of inducing points</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">inducing_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">)</span>
<span class="n">log_noise</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">log_coeff</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">log_scales</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">]</span>
<span class="n">learn_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)</span>

<span class="c1"># Training data x_obs, y_obs are the data sampled from FITC from before</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">x_obs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_obs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Initial locations of inducing points</span>
<span class="n">x_ind_dist</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">inducing_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">high</span><span class="o">=</span><span class="n">inducing_range</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">x_ind_init</span> <span class="o">=</span> <span class="n">x_ind_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x_ind_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Locations to visualise the</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">mean</span> <span class="o">=</span> <span class="n">constant_mean</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">cov</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">log_coeff</span><span class="o">=</span><span class="n">log_coeff</span><span class="p">,</span>
                    <span class="n">log_scales</span><span class="o">=</span><span class="n">log_scales</span><span class="p">,</span>
                    <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">fitc_gp</span> <span class="o">=</span> <span class="n">FITCGP</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span>
                 <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span>
                 <span class="n">log_noise</span><span class="o">=</span><span class="n">log_noise</span><span class="p">,</span>
                 <span class="n">x_train</span><span class="o">=</span><span class="n">x_train</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span>
                 <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
                 <span class="n">x_ind_init</span><span class="o">=</span><span class="n">x_ind_init</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learn_rate</span><span class="p">)</span>

<span class="n">x_ind_prev</span> <span class="o">=</span> <span class="n">x_ind_init</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>


<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">num_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        
        <span class="n">print_numbers</span><span class="p">(</span><span class="n">fitc_gp</span><span class="p">,</span>
                      <span class="n">step</span><span class="p">)</span>
        
        <span class="n">plot</span><span class="p">(</span><span class="n">fitc_gp</span><span class="p">,</span>
             <span class="n">x_pred</span><span class="p">,</span>
             <span class="n">x_train</span><span class="p">,</span>
             <span class="n">y_train</span><span class="p">,</span>
             <span class="n">x_ind_prev</span><span class="p">,</span>
             <span class="n">x_ind_init</span><span class="p">,</span>
             <span class="n">step</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

        <span class="n">log_evidence</span> <span class="o">=</span> <span class="n">fitc_gp</span><span class="o">.</span><span class="n">log_lik</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">log_evidence</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">fitc_gp</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">fitc_gp</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Step: 0 Log evidence: -213.4827 Log coeff:  1.00 Log scales: [1.0] Log noise:  0.00
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_12_1.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Step: 10000 Log evidence: -79.6820 Log coeff: -0.02 Log scales: [-0.019] Log noise: -2.56
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_12_3.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2><h3 id="Summary">Summary<a class="anchor-link" href="#Summary"> </a></h3><p>FITC is a sparse model which can be scaled to larger datasets. It achieves scalability by introducing $M$ <em>inducing points</em> and assuming an independence between the observations given the outputs of the inducing points. Due to this independence, FITC enjoys $\mathcal(O)(NM^2)$ complexity when evaluating the evidence or making predictions, allowing it to scale to larger datasets.</p>
<h3 id="Issues-and-observations">Issues and observations<a class="anchor-link" href="#Issues-and-observations"> </a></h3><ul>
<li><p>FITC is a different model from the simple GP we started with. The modelling assumptions are simply different from the original GP. So, if we are looking for a tractable method that is still faithful to the original GP, FITC may not be a great choice. That being said, maybe FITC is a model we should have considered in the first place, as a candidate for modelling the data. The suitability of different models can be quantified by evaluating the marginal likelihood - if FITC is more likely than a vanilla GP, given the data, then we should use that!</p>
</li>
<li><p>Although FITC can be used as a cheap and easy way to model data with an input-dependent noise level, it's generative assumptions tie the mean and error bars in an unusual way. In particular, the model cannot get more uncertain without the mean decaying back to the prior mean. In regions where the data has a large mean and large noise level simulateneously, FITC would have trouble.</p>
</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input tag_hide_output">

<div class="cell border-box-sizing code_cell rendered tag_hide_input tag_hide_output">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    
    <span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_y</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>

    <span class="n">x_pred_</span> <span class="o">=</span> <span class="n">x_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">mean_</span> <span class="o">=</span> <span class="n">mean</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">var_</span> <span class="o">=</span> <span class="n">var</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">x_ind_curr</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">inducing_variable</span><span class="o">.</span><span class="n">Z</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred_</span><span class="p">,</span> <span class="n">mean_</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pred_</span><span class="p">,</span>
                     <span class="n">mean_</span> <span class="o">-</span> <span class="n">var_</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">mean_</span> <span class="o">+</span> <span class="n">var_</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ind_curr</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x_ind_curr</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input tag_hide_output">

<div class="cell border-box-sizing code_cell rendered tag_hide_input tag_hide_output">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span>

<span class="n">num_inducing</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">inducing_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">)</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">x_obs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_obs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># get_gp_train_data(num_datapoints=10)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_ind_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">inducing_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inducing_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_inducing</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">x_ind_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_ind_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">kernel</span> <span class="o">=</span> <span class="n">gpf</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">SquaredExponential</span><span class="p">(</span><span class="n">variance</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">lengthscales</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>

<span class="n">fitc</span> <span class="o">=</span> <span class="n">gpf</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">GPRFITC</span><span class="p">([</span><span class="n">x_train</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span><span class="n">y_train</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> 
                          <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> 
                          <span class="n">inducing_variable</span><span class="o">=</span><span class="n">x_ind_init</span><span class="p">,</span>
                          <span class="n">noise_variance</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before optimisation&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">fitc</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">kernel</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">optimization_step</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

        <span class="n">log_evidence</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">maximum_log_likelihood_objective</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">log_evidence</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">fitc</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">fitc</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">[:</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">return</span> <span class="n">log_evidence</span>
    
    
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10001</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">log_evidence</span> <span class="o">=</span> <span class="n">fitc</span><span class="o">.</span><span class="n">fitc_log_marginal_likelihood</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">log_evidence</span><span class="p">)</span>
        <span class="n">plot</span><span class="p">(</span><span class="n">fitc</span><span class="p">)</span>
        
    <span class="n">log_evidence</span> <span class="o">=</span> <span class="n">optimization_step</span><span class="p">(</span><span class="n">fitc</span><span class="p">)</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After optimisation&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">fitc</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">kernel</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Before optimisation
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=(10, 1) dtype=float64, numpy=
array([[-3.        ],
       [-2.33333325],
       [-1.66666663],
       [-1.        ],
       [-0.33333325],
       [ 0.33333349],
       [ 1.        ],
       [ 1.66666698],
       [ 2.33333349],
       [ 3.        ]])&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=0.5413248546129181&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=0.5413248546129181&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=-4.600266525158521&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=0.5413248546129181&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=0.5413248546129181&gt;
0 tf.Tensor(-613.4539470918894, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_1.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>500 tf.Tensor(-59.921398748788825, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_3.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>1000 tf.Tensor(-59.813392799390805, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_5.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>1500 tf.Tensor(-59.80665555230898, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_7.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>2000 tf.Tensor(-59.80065949319633, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_9.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>2500 tf.Tensor(-59.78588034691213, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_11.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>3000 tf.Tensor(-59.68516264797949, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_13.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>3500 tf.Tensor(-78.13200919642566, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_15.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>4000 tf.Tensor(-66.63021115391462, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_17.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>4500 tf.Tensor(-66.63709985751825, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_19.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>5000 tf.Tensor(-66.63710620253367, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_21.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>5500 tf.Tensor(-66.63710621845378, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_23.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>6000 tf.Tensor(-66.63710621849663, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_25.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>6500 tf.Tensor(-66.6371062185412, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_27.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>7000 tf.Tensor(-66.63710621849529, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_29.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>7500 tf.Tensor(-66.63710621855148, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_31.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>8000 tf.Tensor(-66.63710621850706, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_33.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>8500 tf.Tensor(-66.6371062184906, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_35.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>9000 tf.Tensor(-66.6371062184997, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_37.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>9500 tf.Tensor(-66.63710621847164, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_39.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>10000 tf.Tensor(-66.63710621850845, shape=(), dtype=float64)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/gaussian-processes/sparse/fitc_15_41.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>After optimisation
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=(10, 1) dtype=float64, numpy=
array([[-3.42186804],
       [ 1.07563989],
       [-1.87487721],
       [ 1.0568379 ],
       [ 1.03793191],
       [ 1.0568379 ],
       [ 1.0568379 ],
       [ 1.0568379 ],
       [ 1.0568379 ],
       [ 1.0568379 ]])&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=0.5413248546129181&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=0.5413248546129181&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=-4.600266525158521&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=0.5413248546129181&gt;
&lt;tf.Variable &#39;Pow/mul/Variable:0&#39; shape=() dtype=float64, numpy=0.5413248546129181&gt;
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input tag_hide_output">

<div class="cell border-box-sizing code_cell rendered tag_hide_input tag_hide_output">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_sine_train_data</span><span class="p">():</span>

    <span class="n">num_datapoints</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="mf">1e-1</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_datapoints</span><span class="p">,))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_datapoints</span><span class="p">,))</span>
    
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
    

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_sine_train_data</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-1-28739c33c191&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">     10</span> 
<span class="ansi-green-intense-fg ansi-bold">     11</span> 
<span class="ansi-green-fg">---&gt; 12</span><span class="ansi-red-fg"> </span>x<span class="ansi-blue-fg">,</span> y <span class="ansi-blue-fg">=</span> get_sine_train_data<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     13</span> 
<span class="ansi-green-intense-fg ansi-bold">     14</span> plt<span class="ansi-blue-fg">.</span>scatter<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">,</span> y<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-1-28739c33c191&gt;</span> in <span class="ansi-cyan-fg">get_sine_train_data</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>     noise <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">1e-1</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> 
<span class="ansi-green-fg">----&gt; 6</span><span class="ansi-red-fg">     </span>x <span class="ansi-blue-fg">=</span> np<span class="ansi-blue-fg">.</span>random<span class="ansi-blue-fg">.</span>uniform<span class="ansi-blue-fg">(</span>low<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span>np<span class="ansi-blue-fg">.</span>pi<span class="ansi-blue-fg">,</span> high<span class="ansi-blue-fg">=</span>np<span class="ansi-blue-fg">.</span>pi<span class="ansi-blue-fg">,</span> size<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">(</span>num_datapoints<span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      7</span>     y <span class="ansi-blue-fg">=</span> np<span class="ansi-blue-fg">.</span>sin<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">+</span> np<span class="ansi-blue-fg">.</span>random<span class="ansi-blue-fg">.</span>normal<span class="ansi-blue-fg">(</span>loc<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.</span><span class="ansi-blue-fg">,</span> scale<span class="ansi-blue-fg">=</span>noise<span class="ansi-blue-fg">,</span> size<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">(</span>num_datapoints<span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      8</span> 

<span class="ansi-red-fg">NameError</span>: name &#39;np&#39; is not defined</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input tag_hide_output">

<div class="cell border-box-sizing code_cell rendered tag_hide_input tag_hide_output">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_gp_train_data</span><span class="p">(</span><span class="n">num_datapoints</span><span class="p">):</span>
    
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">log_coeff</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">log_noise</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>
    <span class="n">log_scales</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_datapoints</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>

    <span class="n">mean</span> <span class="o">=</span> <span class="n">zero_mean</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">cov</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">log_coeff</span><span class="o">=</span><span class="n">log_coeff</span><span class="p">,</span>
                        <span class="n">log_scales</span><span class="o">=</span><span class="n">log_scales</span><span class="p">,</span>
                        <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">cov</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">((</span><span class="mi">10</span> <span class="o">**</span> <span class="n">log_noise</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="n">normal</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">MultivariateNormalTriL</span>
    <span class="n">normal</span> <span class="o">=</span> <span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span><span class="p">))</span>
    
    <span class="n">y</span> <span class="o">=</span> <span class="n">normal</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span>
    

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_gp_train_data</span><span class="p">(</span><span class="n">num_datapoints</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input tag_hide_output">

<div class="cell border-box-sizing code_cell rendered tag_hide_input tag_hide_output">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">get_gp_train_data</span><span class="p">(</span><span class="n">num_datapoints</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">cov</span> <span class="o">=</span> <span class="n">eq_covariance</span><span class="p">(</span><span class="n">log_coeff</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                    <span class="n">log_scales</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                    <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">K_xX</span> <span class="o">=</span> <span class="n">cov</span><span class="p">(</span><span class="n">x_pred</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">x_train</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
<span class="n">K_Xx</span> <span class="o">=</span> <span class="n">cov</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">x_pred</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
<span class="n">K_XX</span> <span class="o">=</span> <span class="n">cov</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">x_train</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="n">X_XX_plus_noise</span> <span class="o">=</span> <span class="n">K_XX</span> <span class="o">+</span> <span class="mf">1e-2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K_XX</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij, jk -&gt; ik&#39;</span><span class="p">,</span>
                   <span class="n">K_xX</span><span class="p">,</span>
                   <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X_XX_plus_noise</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

 


    </main>
    