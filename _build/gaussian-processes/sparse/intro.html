---
interact_link: content/gaussian-processes/sparse/intro.ipynb
kernel_name: venv-random-walks
kernel_path: content/gaussian-processes/sparse
has_widgets: false
title: |-
  Sparse Gaussian Processes
pagenum: 2
prev_page:
  url: /gaussian-processes/gp-intro.html
next_page:
  url: /gaussian-processes/sparse/fitc.html
suffix: .ipynb
search: mathbf x k y n approximation sparse gaussian left right processes gps mathcal sigma covariance matrix training conditionals scale want exact align p cannot cost because larger between independent sparsity help speed hyperparameter learning inference its clear well give something work marginal likelihood predictive posterior begin end any better suffering o calculating either quantity both involve inverse times datasets consider correlations datapoints cubic inverting associated idea scaling data approximations structure such independence assumption certain variables inversion cheaper literature methods fully fitc originally called spgp deterministic dtc variational free energy vfe partial pitc great review papers unifying view approximate process regression

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Sparse Gaussian Processes</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-can-sparsity-help-GPs-scale?">How can sparsity help GPs scale?<a class="anchor-link" href="#How-can-sparsity-help-GPs-scale?"> </a></h2><p>To speed up hyperparameter learning and inference Gaussian Processes, it's clear that we'll have to give up something: if we want to work with the <em>exact</em> marginal likelihood and the <em>exact</em> predictive posterior</p>
\begin{align}
p(\mathbf{y} | \mathbf{X}) &amp;= \mathcal{N}\left(\mathbf{y}; \mathbf{0}, \mathbf{K}_{\mathbf{X}\mathbf{X}} + \sigma^2 \mathbf{I}\right),\\
p(\mathbf{y}^* | \mathbf{x}^*, \mathbf{y}, \mathbf{X}) &amp;= \mathcal{N}\left(\mathbf{y}^*; \mathbf{K}_{\mathbf{x}^*\mathbf{X}} \left(\mathbf{K}_{\mathbf{X}\mathbf{X}} + \sigma^2 \mathbf{I} \right)^{-1} \mathbf{y}, \mathbf{K}_{\mathbf{x}^*\mathbf{x}^*} - \mathbf{K}_{\mathbf{x}^*\mathbf{X}} \left(\mathbf{K}_{\mathbf{X}\mathbf{X}} + \sigma^2 \mathbf{I} \right)^{-1}\mathbf{K}_{\mathbf{X}\mathbf{x}^*}\right),
\end{align}<p>we cannot do any better than suffering $\mathcal{O}(N^3)$ cost when calculating either quantity, because they both involve the inverse of an $N \times N$ covariance matrix. If we want to scale GPs to larger datasets <em>we cannot consider correlations between all datapoints</em> because of the cubic cost of inverting the associated covariance. One idea for scaling GPs to larger data is to make some approximations about the structure of the covariance matrix, such as an independence assumption between certain variables, which will make the matrix inversion cheaper.</p>
<h2 id="Literature-on-sparse-Gaussian-Processes">Literature on sparse Gaussian Processes<a class="anchor-link" href="#Literature-on-sparse-Gaussian-Processes"> </a></h2><h3 id="Sparse-Approximation-methods">Sparse Approximation methods<a class="anchor-link" href="#Sparse-Approximation-methods"> </a></h3><ul>
<li>Fully Independent Training Conditionals (FITC) approximation, originally called SPGP.</li>
<li>Deterministic Training Conditionals (DTC) approximation.</li>
<li>Variational Free Energy (VFE) approximation.</li>
<li>Partial Independent Training Conditionals (PITC) approximation.</li>
</ul>
<h3 id="Great-review-papers">Great review papers<a class="anchor-link" href="#Great-review-papers"> </a></h3><ul>
<li>A Unifying View of Sparse Approximate Gaussian Process Regression.</li>
</ul>

</div>
</div>
</div>
</div>

 


    </main>
    