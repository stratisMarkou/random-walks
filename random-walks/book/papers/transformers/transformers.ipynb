{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to transformers\n",
    "\n",
    "The transformer {cite}`vaswani2017attention` is a deep learning architecture which has powered many of the recent advances across a range of machine learning applications, including text modelling, image modelling {cite}`dosovitskiy2021image`, and many others.\n",
    "This is an overview of the transformer architecture, including a self-contained mathematical description of the architectural details, and a concise implementation.\n",
    "All of this exposition is based off an excellent introduction paper on transformers by Rich Turner {cite}`turner2023introduction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with tokens\n",
    "__One architecture, many applications.__\n",
    "The purpose of the transformer architecture was, originally, to model sequence data such as text.\n",
    "The approach for achieving this was to first convert individual words, or characters, into one-dimensional arrays called _tokens_, and then operate on these tokens with a neural network.\n",
    "This approach however extends beyond word modelling.\n",
    "For example, the transformer can be applied to tasks as diverse as modelling of images and video, proteins, or weather.\n",
    "In all these applications, the data are first converted into sets of tokens.\n",
    "After this step, the transformer can be applied in roughly the same way, irrespective of the original representation of the data.\n",
    "This versatility, together with their empirical performance, are some of the main appealing features of the transformer.\n",
    "\n",
    "__Inputs as tokens.__\n",
    "In particular, for the moment, we will assume that the input data have already been converted into tokens and defer the details of this tokenisation for later.\n",
    "More concretely, let us assume that each data example, e.g. a sentence, image, or protein,  has been conerted into a set of tokens $\\{x_n\\}_{n=1}^N,$ where each $x_n$ is a $D$ dimensional array $x_n \\in \\mathbb{R}^D.$\n",
    "We can collect these tokens into a single $D \\times N$ array $X^{(0)} \\in \\mathbb{R}^{D \\times N},$ forming a single data input for the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "Much like in other deep architectures, the transformer maintains a representation of the input data, and progressively refines it using a sequence of so-called _transformer blocks_.\n",
    "In particular, given an initial representation $X^{(0)}$ the archtecture comprises of $M$ transformer blocks, i.e. for each $m = 1, \\dots, M,$ it computes\n",
    "\n",
    "$$X^{(m)} = \\texttt{TransformerBlock}(X^{(m-1)}).$$\n",
    "\n",
    "Each of these blocks consists of two main operations, namely a self-attention operation and a token-wise multi-layer perceptron (MLP) operation.\n",
    "The self-attention operation has the role of combining the representations of different tokens in a sequence, in order to model dependencies between the tokens.\n",
    "It is applied collectively to all tokens within the transformer block.\n",
    "The MLP operation has the role of refining the representation of each token.\n",
    "It is applied separately to each token and is shared across all tokens within a transformer block.\n",
    "Let's look at these two operations in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention\n",
    "\n",
    "__Attention.__\n",
    "The role of the first operation in a transformer block is to combine the representations of different tokens in order to model dependencies between them.\n",
    "Given a $D \\times N$ input array $X^{(m)} = (x_1, \\dots, x_N^{(m)})$ the output of the self-attention layer is another $D \\times N$ array $Y^{(m)} = (y_1, \\dots, y_N^{(m)}),$ where each column is simply a weighted average of the input features, that is\n",
    "\n",
    "$$y^{(m)}_n = \\sum_{n' = 1}^N x^{(m - 1)}_{n'} A_{n', n}^{(m)}.$$\n",
    "\n",
    "The weighting array $A_{n', n}^{(m)}$ is of size $N \\times N$ and has the property that its columns normalise to one, that is $\\sum_{n'=1}^N A_{n', n}^{(m)} = 1.$\n",
    "It is referred to the attention matrix because it weighs the extent to which the feature $y^{(m)}_n$ should depend on each $x^{(m)}_{n'},$ i.e. it determines the extent to which each $y^{(m)}_n$ should attend to each $x^{(m)}_{n'}.$\n",
    "For compactness, we can collect these equations to a single linear operation, that is\n",
    "\n",
    "$$Y^{(m)} = X^{(m - 1)} A^{(m)}.$$\n",
    "\n",
    "But what about the attention weights themselves?\n",
    "We have not specified how these are computed and, their precise definition is going to be one important factor that differentiates transformers from other architectures.\n",
    "In fact, many other operations forming the core of other archictectures, such as convolution layers in convolutional neural networks (CNNs), can be written as similar weighted sums.\n",
    "Let's next look at the specifics of the transformer attention weights.\n",
    "\n",
    "__Self-attention.__\n",
    "One of the innovations within the transformer architecture is that the attention weights are adaptive, meaning that they are computed based on the input itself.\n",
    "This is in contrast with other deep learning architectures such CNNs, where weighted sums are also used, but these weights are fixed and shared across all inputs.\n",
    "One straightforward way to compute attention weights would be to compare them by a simple similarity metric, such as an inner product.\n",
    "For example, given two tokens $x_i$ and $x_j,$ we can compute a dot-product between them, which acts as a similarity metric, exponetiate the result to make it positive and then normalise the result to ensure that each column sums to one, that is\n",
    "\n",
    "$$A^{(m)}_{n, n'} = \\frac{\\exp(x_n^\\top x_{n'})}{\\sum_{n'' = 1}^N \\exp(x_{n''}^\\top x_{n'})}.$$\n",
    "\n",
    "An alternative, slightly more flexible approach is to transform each token in the sequence by a linear map, say by applying a matrix $U \\in \\mathbb{R}^{K \\times D}$ to each token first, that is\n",
    "\n",
    "$$A^{(m)}_{n, n'} = \\frac{\\exp(x_n^\\top U^\\top U x_{n'})}{\\sum_{n'' = 1}^N \\exp(x_{n''}^\\top U^\\top U x_{n'})}.$$\n",
    "\n",
    "This allows the tokens to be compared in a different space.\n",
    "For example, if $K < D$ this approach automatically projects out some of the components of the tokens, comparing them in a lower-dimensional space.\n",
    "However, this approach still has an important limitation, namely symmetry.\n",
    "Specifically, the attention matrix above would be symmetric, which means that any two tokens would attend to each other with equal strengths.\n",
    "This might be undesirable because, for example, we could imagine that one token might be important for informing the representation of another token, but not the other way around.\n",
    "To address this, we can apply different linear operations, say $U_k$ and $U_q$ to each of the tokens being compared, and instead compute\n",
    "\n",
    "$$A^{(m)}_{n, n'} = \\frac{\\exp(x_n^\\top U_k^\\top U_q x_{n'})}{\\sum_{n'' = 1}^N \\exp(x_{n''}^\\top U_k^\\top U_q x_{n'})}.$$\n",
    "\n",
    "In this way, the resulting attention matrix that is not necessarily symmetric and an overall more expressive architecture.\n",
    "Tokens no longer have to attend to each other with the same strength.\n",
    "This weighting is known as self-attention, since each token in the sequence attends to every other token of the same sequence.\n",
    "It is also possible to generalise this to attention between different sequences, which might be useful for some applications such as, for example joint modelling of text and images.\n",
    "This generalisation is called cross-attention, and we defer its discussion for later.\n",
    "\n",
    "__Multi-head self-attention.__\n",
    "In order to increase the capacity of the self-attention layer, the transformer block includes $H$ separate self-attention operations with different parameters, in parallel.\n",
    "The results of these operations are then projected down to a single $D \\times N$ array again, which is required for further processing.\n",
    "In particular, we have\n",
    "\n",
    "```{margin}\n",
    "As a recap to the notation in these equations: the $m$ superscript runs from $1$ to $M$ and is the index of the transformer block, the $n, n'$ and $n''$ superscripts run from $1$ to $N$ and index the tokens in the sequence within the current block, the $h$ subscript runs from $1$ to $H$ and denotes a particular self-attention head in the block.\n",
    "Finally the $k$ and $q$ subscripts are not indices, but symbols distinguishing the two different kinds of matrices $U_k$ and $U_q.$\n",
    "```\n",
    "\n",
    "$$\\begin{align}\n",
    "Y^{(m)} = \\texttt{MHSA}(X^{(m - 1)}) &= \\sum^H_{h = 1} V_h^{(m)} X^{(m - 1)} A_h^{(m)}, \\text{ where } \\\\\n",
    "\\left[A^{(m)}_h\\right]_{n, n'} &= \\frac{\\exp\\left(k_{h, n}^{(m)\\top} q_{h, n'}^{(m)}\\right)}{\\sum_{n'' = 1}^N \\exp\\left(k_{h, n''}^{(m)\\top} q_{h, n'}^{(m)}\\right)} \n",
    "\\end{align}$$\n",
    "\n",
    "where $q_{h, n}^{(m)} = U^{(m)}_{q, h} x_n^{(m-1)}$ and $k_{h, n}^{(m)} = U^{(m)}_{k, h} x_n^{(m-1)}.$\n",
    "At this point we should note that, due to the nonlinearity of $A^{(m)},$ together with the multiplication by $V^{(m)}_h$ and summation across $h,$ multi-head cross attention performs not just inter-feature but also intra-feature processing, i.e. each token interacts with and changes its own representation.\n",
    "However, the capacity of this intra-feature processing is limited, and it is the job of the second stage, the MLP, to address this.\n",
    "Let's next look at the MLP stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "The self-attention layer has the role of aggregating information across tokens in a sequence to model joint dependencies.\n",
    "In order to refine the representations themselves, a simple MLP is applied to each token in isolation, in a relatively simple step\n",
    "\n",
    "$$x^{(m)_n} = \\texttt{MLP}(y^{(m)}_n).$$\n",
    "\n",
    "Note that this MLP is shared across all input locations, i.e. all tokens, within a given layer.\n",
    "\n",
    "### Residuals and normalisation\n",
    "Before putting together the $\\texttt{MHSA}$ and $\\texttt{MLP}$ operations, we will add two ubiquitous deep learning operations to improve the stability and ease of training of the model, namely residual connections and normalisation.\n",
    "\n",
    "__Residual connections.__\n",
    "Residual connections {cite}`he2015deep` are widely used across deep learning architectures, because they simplify model initialisation, stabilise learning and provide a useful inductive bias toward simpler functions.{cite}`szegedy2017inception`\n",
    "Instead of specifying a mapping of the form $x^{(m)} = f(x^{(m)}),$ a residual connection amounts to specifying a function involving an identity function plus a residual term\n",
    "\n",
    "$$x^{(m)} = x^{(m-1)} + g(x^{(m)}).$$\n",
    "\n",
    "This can be equivalently thought of as learning to model differences between the representations at different blocks, that is $x^{(m)} - x^{(m-1)} = g(x^{(m)}).$\n",
    "If we do not use residual connections and compose multiple blocks together, the activations in each can become more extreme as we go deeper in the network, resulting in either zero or extremely large gradients, which can be problematic during training.\n",
    "One motivation for using residual connections is that, if we initialise the parameters of $g$ such that its outputs are close to zero, then $x^{(m)}$ will be approximately constant across $m = 1, \\dots, M.$\n",
    "This can improve training ease and stability because all blocks in the network, even the deeper ones, receive an input close to $x^{(0)},$ and the gradients will tend to receive less extreme gradients.\n",
    "Residual connections are used both in the $\\texttt{MHSA}$ and $\\texttt{MLP}$ layers of the transformer.\n",
    "\n",
    "__Token normalisation.__\n",
    "Another ubiquitous and extremely useful operation in deep learning is normalisation.\n",
    "There are various different kinds of normalisation, including LayerNorm {cite}`ba2016layer`, BatchNorm {cite}`ioffe2015batch`, GroupNorm {cite}`wu2018group` and InstanceNorm {cite}`ulyanov2016instance`.\n",
    "Normalisation has been widely found to improve learning stability and overall model performance.\n",
    "One reason for this is that normalisation typically prevents the inputs to a layer from becoming extremely large, which can result into extreme or staturated outputs, which in turn mean that the gradients with respect to the network parameters can be close to zero or extremely large.\n",
    "The transformer architecture uses LayerNorm which, when applied to the tokens, amounts to per-token normalisation.\n",
    "Specifically, when applied to an array $X$ of input tokens, LayerNorm amounts to\n",
    "\n",
    "$$\\texttt{LayerNorm}(X)_{d, n} = \\bar{x}_{d, n} = \\frac{x_{d, n} - \\mu(x_n)}{\\sigma(x_n)} \\gamma_d + \\beta_d,$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ denote operations that compute the mean and the standard deviation respectively, and $\\gamma_d$ and $\\beta_d$ are a learnt scale and a learnt shift.\n",
    "In other words, within a transformer, LayerNorm separately normalises each token within each sequence within each batch.\n",
    "\n",
    "\n",
    "### Putting it together\n",
    "In summary, we can collect these operations into the following equations\n",
    "\n",
    "$$\\begin{align}\n",
    "\\bar{X}^{(m-1)} &= \\texttt{LayerNorm}\\left(X^{(m-1)}\\right) \\\\\n",
    "Y^{(m)} &= \\bar{X}^{(m-1)} + \\texttt{MHSA}\\left(\\bar{X}^{(m-1)}\\right) \\\\\n",
    "\\bar{Y}^{(m)} &= \\texttt{LayerNorm}\\left(Y^{(m)}\\right) \\\\\n",
    "X^{(m)} &= Y^{(m)} + \\texttt{MLP}(\\bar{Y}^{(m)})\n",
    "\\end{align}$$\n",
    "\n",
    "These make up the entirety of the transformer block, which is repeated $M$ times to compute the output of the transformer.\n",
    "An important detail we have not discussed thus far is how to build the tokens themselves.\n",
    "\n",
    "\n",
    "### Tokens and embeddings\n",
    "\n",
    "__Tokenisation.__\n",
    "Tokenisation is an application-specific detail but, generally, there are two main approaches, depending on whether the inputs are continuous or discrete.\n",
    "As a reminder, in both cases, we want convert each input element in our sequence, say $s_n,$ to a $D$-dimensional array $x^{(0)}_n.$\n",
    "We will specify a map $\\texttt{tokenise}$ that performs the operation $s_n = \\texttt{tokenise}(x^{(0)}_n)$ separately for the case where the inputs $s_n$ are discrete or continuous.\n",
    "\n",
    "__Discrete or continuous inputs.__\n",
    "In text modelling the raw inputs are integers representing unique words or characters.\n",
    "In such applications, i.e. whenever we have discrete inputs, we can use a look-up table containing learnable vectors.\n",
    "That is, if $s_n \\in \\{1, \\dots, K\\},$ we can define $K$ arrays, each of length $D$, say $z_0, \\dots, z_K \\in \\mathbb{R}^D,$ and let\n",
    "\n",
    "$$x^{(0)}_n = \\texttt{tokenise}(s_n) = z_{s_n}.$$\n",
    "\n",
    "This allows us to map each word into a continuous space and operate on the resulting arrays with the transformer architecture.\n",
    "In other applications, such as vision, the inputs are typically treated as continuous, that is $s_n \\in \\mathbb{R}^{D_s}.$\n",
    "In such cases, we can simply apply a simple operation such as a linear transformation, to map each $s_n$ into a $D$-dimensional array.\n",
    "For example, letting $W \\in \\mathbb{R}^{D\\times D_s},$ we can define\n",
    "\n",
    "$$x^{(0)}_n = W s_n,$$\n",
    "\n",
    "giving a $D$-dimensional token which is ready for use in the transformer.\n",
    "We have now covered almost all parts of the transformer, except one final, but very important point concerning the embeddings.\n",
    "Thus far, we have glossed over the fact that the transformer block has no notion of position, which is a very important issue that we look into next.\n",
    "\n",
    "__Positional embeddings.__\n",
    "Specifically, the $\\texttt{MHSA}$ operation, the token-wise $\\texttt{MLP}$ operation, as well as $\\texttt{LayerNorm}$ and residual additions are all examples of permutation equivariant: permuting the tokens and applying any one of these operations gives the same result as first applying the operation and then permuting the resulting tokens.\n",
    "Composing these operations retains permutation equivariance, meaning that permuting the elements of the original sequence and applying the transformer will yield exactly the same result as first applying the transformer and then permuting the resulting features.\n",
    "This is undesirable because, for example in text modelling, the phrases \"Arsenal bets Chelsea\" and \"Chelsea beats Arsenal\" are composed of identical words but have opposite meanings, and we would like the resulting features produced by the transformer to reflect this.\n",
    "One way to get around this issue is augmenting the tokens with information about the position of an input feature within the sequence.\n",
    "For example, we could set up an additional embedding which directly maps each position to a learnable array and concatentate the result with the tokenised feature, that is\n",
    "\n",
    "$$x^{(0)}_n = \\texttt{tokenise}_1(s_n) \\odot \\texttt{tokenise}_2(n),$$\n",
    "\n",
    "where $\\odot$ denotes concatenation, and we have used different tokenisation functions for the sequence features and the positions.\n",
    "This approach is often used in vision transformers.\n",
    "Another approach is applying, for example, sinusoidal functions with different frequencies on the input, for example\n",
    "\n",
    "$$\\texttt{tokenise}(n) = [\\sin(\\omega_1 n), \\dots, \\sin{\\omega_D n}],$$\n",
    "\n",
    "which are then concatentated to the tokenised features as described above.\n",
    "Other approaches bake in positional information directly into the $\\texttt{MHSA}$ layer, for example by making the attention weights depend on the position difference of pairs of tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now that we've covered all the details, let's implement a small transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import jax\n",
    "import equinox as eqx\n",
    "from jaxtyping import Float, Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(eqx.Module):\n",
    "\n",
    "    Uk: Float[Array, \"K D\"]\n",
    "    Uq: Float[Array, \"K D\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        input_dim: int,\n",
    "        projection_dim: int,\n",
    "    ):\n",
    "\n",
    "        # Set up keys for projection matrices\n",
    "        key1, key2 = jax.random.split(key)\n",
    "\n",
    "        # Initialize projection matrices Uk and Uq\n",
    "        self.Uk = eqx.initializers.xavier_normal(\n",
    "            key1,\n",
    "            (projection_dim, input_dim),\n",
    "        )\n",
    "        self.Uq = eqx.initializers.xavier_normal(\n",
    "            key2,\n",
    "            (projection_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def self_attention_weights(\n",
    "        self,\n",
    "        x: Float[Array, \"D N\"],\n",
    "    ) -> Float[Array, \"N N\"]:\n",
    "        \"\"\"\n",
    "        Compute self-attention weights for tokens in a sequence\n",
    "\n",
    "        Args:\n",
    "            x: input sequence of tokens, shape (D, N)\n",
    "        \n",
    "        Returns:\n",
    "            attention weights, shape (N, N)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the keys and queries\n",
    "        k = jax.numpy.matmul(self.Uk, x)\n",
    "        q = jax.numpy.matmul(self.Uq, x)\n",
    "\n",
    "        # Compute inner product of keys and queries\n",
    "        kq = jax.numpy.matmul(k.T, q)\n",
    "\n",
    "        # Attention weights are the softmax of the inner products\n",
    "        return jax.nn.softmax(kq, axis=0)\n",
    "    \n",
    "    def __call__(self, x: Float[Array, \"D N\"]) -> Float[Array, \"D N\"]:\n",
    "        \"\"\"\n",
    "        Apply self-attention to a sequence of tokens\n",
    "\n",
    "        Args:\n",
    "            x: input sequence of tokens, shape (D, N)\n",
    "\n",
    "        Returns:\n",
    "            output sequence of tokens, shape (D, N)\n",
    "        \"\"\"\n",
    "\n",
    "        a = self.self_attention_weights(x)\n",
    "        return jax.numpy.matmul(x, a)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(eqx.Module):\n",
    "    \n",
    "    self_attention_layers: List[SelfAttention]\n",
    "    linear_layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        input_dim: int,\n",
    "        projection_dim: int,\n",
    "        num_heads: int,\n",
    "    ):\n",
    "\n",
    "        keys = jax.random.split(key, 2*num_heads)\n",
    "        self.self_attention_layers = [\n",
    "            SelfAttention(\n",
    "                key=key,\n",
    "                input_dim=input_dim,\n",
    "                projection_dim=projection_dim,\n",
    "            ) for key in keys[::2]\n",
    "        ]\n",
    "\n",
    "        self.linear = [\n",
    "            eqx.Linear(key, input_dim, input_dim) for key in keys[1::2]\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"D N\"]) -> Float[Array, \"D N\"]:\n",
    "        \"\"\"\n",
    "        Apply multi-head self-attention to a sequence of tokens\n",
    "\n",
    "        Args:\n",
    "            x: input sequence of tokens, shape (D, N)\n",
    "\n",
    "        Returns:\n",
    "            output sequence of tokens, shape (D, N)\n",
    "        \"\"\"\n",
    "            \n",
    "        # Compute tokens for each head\n",
    "        heads = [layer(x) for layer in self.self_attention_layers]\n",
    "\n",
    "        # Apply linear transformation to each head\n",
    "        heads = [linear(h) for h, linear in zip(heads, self.linear)]\n",
    "\n",
    "        # Stack and sum across heads\n",
    "        heads = jax.numpy.stack(heads, axis=0)\n",
    "        heads = jax.numpy.sum(heads, axis=0)\n",
    "\n",
    "        return heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(eqx.Module):\n",
    "\n",
    "    layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        num_hidden: int,\n",
    "        num_layers: int,\n",
    "        num_features: int,\n",
    "    ):\n",
    "        # Set up input and output dimensions of linear layers\n",
    "        in_feats = [num_features] + [num_hidden] * num_layers\n",
    "        out_feats = [num_hidden] * num_layers + [num_features]\n",
    "\n",
    "        # Split the random key into sub-keys for each layer\n",
    "        keys = jax.random.split(key, num_layers)\n",
    "        \n",
    "        # Create linear layers with different random keys\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(\n",
    "                key=key,\n",
    "                in_features=in_feat,\n",
    "                out_features=out_feat,\n",
    "            )\n",
    "            for key, in_feat, out_feat in zip(keys, in_feats, out_feats)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"D\"]) -> Float[Array, \"D\"]:\n",
    "        \"\"\"\n",
    "        Compute forward pass through the MLP.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor of shape (in_features,)\n",
    "        \n",
    "        Returns:\n",
    "            output tensor of shape (out_features,)\n",
    "        \"\"\"\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            x = jax.nn.relu(x)\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations to other architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
