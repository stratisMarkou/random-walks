{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to transformers\n",
    "\n",
    "The transformer architecture {cite}`vaswani2017attention` is a deep learning architecture, which has powered many of the recent advances across a range of deep learning applications, including text modelling, image modelling {cite}`dosovitskiy2021image`, and many others.\n",
    "This is an overview of the transformer architecture, including a self-contained mathematical description of the architectural details, and a concise implementation.\n",
    "All of this exposition is based off an excellent introduction paper on transformers by Rich Turner {cite}`turner2023introduction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with tokens\n",
    "__One architecture, many applications.__\n",
    "The purpose of the transformer architecture was, originally, to model sequence data such as text.\n",
    "The approach for achieving this with the transformer is to first convert individual words, or characters, into one-dimensional arrays called _tokens_, and then operate on these tokens with a neural network.\n",
    "This approach extends far beyond word modelling.\n",
    "For example, the transformer can be applied to tasks as diverse as modelling of images and video, proteins, or weather.\n",
    "In all these applications, the data are first converted into sets of tokens.\n",
    "After this step, the transformer architecture can be applied in roughly the same way, irrespective of the original representation of the data.\n",
    "This versatility, together with their empirical performance, are some of the main appealing features of the transformer.\n",
    "\n",
    "__Inputs as tokens.__\n",
    "In particular, for the moment, we will assume that the input data have already been converted into tokens and defer the details of this tokenisation for later.\n",
    "For now, let us assume that each data example, e.g. a sentence or an image, has been conerted into a set of tokens $\\{x_n\\}_{n=1}^N,$ where each $x_n$ is a $D$ dimensional array $x_n \\in \\mathbb{R}^D.$\n",
    "We can collect these tokens into a single $D \\times N$ array $X^{(0)} \\in \\mathbb{R}^{D \\times N},$ forming a single data input for the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "Much like in other deep architectures, such as residual networks, the transformer architecture maintains a representation of the input data, and progressively refines it using a sequence of so-called transformer blocks.\n",
    "In particular, given an initial representation $X^{(0)}$ the archtecture comprises of $M$ transformer blocks, i.e. for each $m = 1, \\dots, M,$ it computes\n",
    "\n",
    "$$X^{(m)} = \\texttt{TransformerBlock}(X^{(m-1)}).$$\n",
    "\n",
    "Each of these blocks consists of two main operations, namely a self-attention operation and a pointwise multi-layer perceptron (MLP) operation.\n",
    "The self-attention operation has the role of combining the representations of different tokens in a sequence, in order to model dependencies between the tokens.\n",
    "It is applied collectively to all tokens within the transformer block.\n",
    "The MLP operation has the role of refining the representation of each token.\n",
    "It is applied separately to each token and is shared across all tokens within a transformer block.\n",
    "Let's look at these two operations in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention\n",
    "\n",
    "__Attention:__\n",
    "The role of the first operation in a transformer block is to combine the representations of different tokens in order to model dependencies between them.\n",
    "Given a $D \\times N$ input array $X^{(m)} = (x_1, \\dots, x_N^{(m)})$ the output of the self-attention layer is another $D \\times N$ array $Y^{(m)} = (y_1, \\dots, y_N^{(m)}),$ where each column is simply a weighted average of the input features, that is\n",
    "\n",
    "$$y^{(m)}_n = \\sum_{n' = 1}^N x^{(m - 1)}_{n'} A_{n', n}^{(m)}.$$\n",
    "\n",
    "The weighting array $A_{n', n}^{(m)}$ is of size $N \\times N$ and has the property that its columns normalise to one, that is $\\sum_{n'=1}^N A_{n', n}^{(m)} = 1.$\n",
    "It is referred to the attention matrix because it weights the extent to which the feature $y^{(m)}_n$ should depend on each $x^{(m)}_{n'},$ i.e. it determines the extent to which each $y^{(m)}_n$ should attend to each $x^{(m)}_{n'}.$\n",
    "For compactness, we can collect these equations to a single linear operation, that is\n",
    "\n",
    "$$Y^{(m)} = X^{(m - 1)} A^{(m)}.$$\n",
    "\n",
    "Clearly, the value of the attention weights is central in this step.\n",
    "In fact, a host of different operations, such as convolution layers in convolutional neural networks (CNNs) can be written as weighted sums of input arrays.\n",
    "Let's next look at the specifics of how the attention weights are themselves computed in the transformer.\n",
    "\n",
    "__Self-attention:__\n",
    "One of the innovations within the transformer architecture is that the attention weights are adaptive, meaning that they are computed based on the input itself.\n",
    "This is in contrast with other deep learning architectures such CNNs, where weighted sums are also used, but these weights are fixed and shared across all inputs.\n",
    "One straightforward way to compute attention weights would be to compare them by a simple similarity metric, such as an inner product.\n",
    "For example, given two tokens $x_i$ and $x_j,$ we can compute a dot-product between them, which acts as a similarity metric, exponetiate the result to make it positive and then normalise each column to ensure the resulting weights are between $0$ and $1,$ that is\n",
    "\n",
    "$$A^{(m)}_{ij} = \\frac{\\exp(x_i^\\top x_j^\\top)}{\\sum_{n' = 1}^N \\exp(x_i^\\top x_{n'}^\\top)}.$$\n",
    "\n",
    "An alternative, slightly more flexible approach is to transform each token in the sequence by a linear map, say by applying a matrix $U \\in \\mathbb{R}^{K \\times D}$ to each token first, that is\n",
    "\n",
    "$$A^{(m)}_{ij} = \\frac{\\exp(x_i^\\top U^\\top U x_j^\\top)}{\\sum_{n' = 1}^N \\exp(x_i^\\top U^\\top U x_{n'}^\\top)}.$$\n",
    "\n",
    "This allows the tokens to be compared in a different space.\n",
    "For example, if $K < D$ this approach automatically projects out some of the components of the tokens, comparing them in a lower-dimensional space.\n",
    "However, this approach still has an important limitation, namely symmetry.\n",
    "Specifically, the attention matrix above would be symmetric, which means that any two tokens would attend to each other with equal strengths.\n",
    "This might be undesirable because, for example, we could imagine that one token might be important for informing the representation of another token, but not the other way around.\n",
    "To address this, we can apply different linear operations, say $U_k$ and $U_q$ to each of the tokens being compared, and instead compute\n",
    "\n",
    "$$A^{(m)}_{ij} = \\frac{\\exp(x_i^\\top U_k^\\top U_q x_j^\\top)}{\\sum_{n' = 1}^N \\exp(x_i^\\top U_k^\\top U_q x_{n'}^\\top)}.$$\n",
    "\n",
    "In this way, the resulting attention matrix that is not necessarily symmetric and an overall more expressive architecture, and tokens no longer have to attend to each other with the same strength.\n",
    "This type of weighting is known as self-attention, since each token in the sequence attends to every other token of the same sequence.\n",
    "It is also possible to generalise this to attention between different sequences, which might be useful for some applications such as, for example joint modelling of text and images.\n",
    "This generalisation is called cross-attention, and we defer its discussion for later.\n",
    "\n",
    "__Multi-head self-attention.__\n",
    "In order to increase the capacity of the self-attention layer\n",
    "\n",
    "Now, let's look at the other central operation in the transformer architecture, namely the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "The self-attention layer has the role of aggregating information across tokens in a sequence to model joint dependencies.\n",
    "In order to refine the representations themselves, a simple MLP is applied to each token in isolation, in a relatively simple step\n",
    "\n",
    "$$x^{(m)_n} = \\text{MLP}(y^{(m)}_n).$$\n",
    "\n",
    "\n",
    "### Skip connections and normalisation\n",
    "\n",
    "### Positional embeddings\n",
    "\n",
    "### Putting it together\n",
    "\n",
    "In summary, we can collect\n",
    "\n",
    "$$\\begin{align}\n",
    "\\bar{X}^{(m-1)} &= \\texttt{LayerNorm}\\left(X^{(m-1)}\\right) \\\\\n",
    "Y^{(m)} &= \\bar{X}^{(m-1)} + \\texttt{MHSA}\\left(\\bar{X}^{(m-1)}\\right) \\\\\n",
    "\\bar{Y}^{(m)} &= \\texttt{LayerNorm}\\left(Y^{(m)}\\right) \\\\\n",
    "X^{(m)} &= Y^{(m)} + \\texttt{MLP}(\\bar{Y}^{(m)})\n",
    "\\end{align}$$\n",
    "\n",
    "## Implementation\n",
    "\n",
    "## Complementary view: keys, queries and values\n",
    "\n",
    "## Extensions\n",
    "\n",
    "## Relations to other architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
