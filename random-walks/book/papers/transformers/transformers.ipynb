{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to transformers\n",
    "\n",
    "The transformer architecture {cite}`vaswani2017attention` is a deep learning architecture, which has powered many of the recent advances across a range of deep learning applications, including text modelling, image modelling {cite}`dosovitskiy2021image`, and many others.\n",
    "This is an overview of the transformer architecture, including a self-contained mathematical description of the architectural details, and a concise implementation.\n",
    "All of this exposition is based off an excellent introduction paper on transformers by Rich Turner {cite}`turner2023introduction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence modelling\n",
    "\n",
    "## Transformer block\n",
    "\n",
    "Much like in other deep architectures, such as residual networks, the transformer architecture maintains a representation of the input data, and progressively refines it using a sequence of so-called transformer blocks.\n",
    "In particular, given an initial representation $X^{(0)}$ the archtecture comprises of $M$ transformer blocks\n",
    "\n",
    "$$X^{(m)} = \\texttt{TransformerBlock}(X^{(m-1)}).$$\n",
    "\n",
    "Each of these blocks consists of two main operations, namely a self-attention operation and a pointwise multi-layer perceptron (MLP) operation.\n",
    "The self-attention operation has the role of combining the representations of different tokens in a sequence, in order to model dependencies between the tokens.\n",
    "It is applied collectively to all tokens within the transformer block.\n",
    "The MLP operation has the role of refining the representation of each token.\n",
    "It is applied separately to each token and is shared across all tokens within a transformer block.\n",
    "\n",
    "\n",
    "### Self-attention\n",
    "\n",
    "__Attention:__\n",
    "The role of the first operation in a transformer block is to combine the representations of different tokens in order to model dependencies between the tokens.\n",
    "Given a $D \\times N$ input array $X^{(m)} = (x_1, \\dots, x_N^{(m)})$ the output of the self-attention layer is another $D \\times N$ array $Y^{(m)} = (y_1, \\dots, y_N^{(m)}),$ where each column is simply a weighted average of the input features, that is\n",
    "\n",
    "$$y^{(m)}_n = \\sum_{n' = 1}^N x^{(m - 1)}_{n'} A_{n', n}^{(m)}.$$\n",
    "\n",
    "The weighting array $A_{n', n}^{(m)}$ is of size $N \\times N$ and has the property that its columns normalise to one, that is $\\sum_{n'=1}^N A_{n', n}^{(m)} = 1.$\n",
    "It is referred to the attention matrix because, intuitively speaking, it weights the extent to which the feature $y^{(m)}_n$ should depend on each $x^{(m)}_{n'},$ i.e. it determines the extent to which each $y^{(m)}_n$ should attend to each $x^{(m)}_{n'}.$\n",
    "For compactness, we can collect these equations to a single linear operation, that is\n",
    "\n",
    "$$Y^{(m)} = X^{(m - 1)} A^{(m)}.$$\n",
    "\n",
    "__Self-attention:__\n",
    "Now we turn to how the attention weights are themselves computed.\n",
    "One of the innovations within the transformer architecture is that the attention weights are adaptive, meaning that they are computed based on the input itself.\n",
    "This is in contrast with other deep learning architectures such as convolutional neural networks (CNNs) where weighted sums are also used, but these weights are fixed and shared across all inputs.\n",
    "\n",
    "\n",
    "### Multi-layer perceptron\n",
    "\n",
    "### Skip connections and normalisation\n",
    "\n",
    "### Positional embeddings\n",
    "\n",
    "### Putting it together\n",
    "\n",
    "In summary, we can collect\n",
    "\n",
    "$$\\begin{align}\n",
    "\\bar{X}^{(m-1)} &= \\texttt{LayerNorm}\\left(X^{(m-1)}\\right) \\\\\n",
    "Y^{(m)} &= \\bar{X}^{(m-1)} + \\texttt{MHSA}\\left(\\bar{X}^{(m-1)}\\right) \\\\\n",
    "\\bar{Y}^{(m)} &= \\texttt{LayerNorm}\\left(Y^{(m)}\\right) \\\\\n",
    "X^{(m)} &= Y^{(m)} + \\texttt{MLP}(\\bar{Y}^{(m)})\n",
    "\\end{align}$$\n",
    "\n",
    "## Implementation\n",
    "\n",
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
