{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to transformers\n",
    "\n",
    "The transformer {cite}`vaswani2017attention` is a deep learning architecture which has powered many of the recent advances across a range of machine learning applications, including text modelling, image modelling {cite}`dosovitskiy2021image`, and many others.\n",
    "This is an overview of the transformer architecture, including a self-contained mathematical description of the architectural details, and a concise implementation.\n",
    "All of this exposition is based off an excellent introduction paper on transformers by Rich Turner {cite}`turner2023introduction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with tokens\n",
    "__One architecture, many applications.__\n",
    "The purpose of the transformer architecture was, originally, to model sequence data such as text.\n",
    "The approach for achieving this was to first convert individual words, or characters, into one-dimensional arrays called _tokens_, and then operate on these tokens with a neural network.\n",
    "This approach however extends beyond word modelling.\n",
    "For example, the transformer can be applied to tasks as diverse as modelling of images and video, proteins, or weather.\n",
    "In all these applications, the data are first converted into sets of tokens.\n",
    "After this step, the transformer can be applied in roughly the same way, irrespective of the original representation of the data.\n",
    "This versatility, together with their empirical performance, are some of the main appealing features of the transformer.\n",
    "\n",
    "__Inputs as tokens.__\n",
    "In particular, for the moment, we will assume that the input data have already been converted into tokens and defer the details of this tokenisation for later.\n",
    "More concretely, let us assume that each data example, e.g. a sentence, image, or protein,  has been conerted into a set of tokens $\\{x_n\\}_{n=1}^N,$ where each $x_n$ is a $D$ dimensional array $x_n \\in \\mathbb{R}^D.$\n",
    "We can collect these tokens into a single $D \\times N$ array $X^{(0)} \\in \\mathbb{R}^{D \\times N},$ forming a single data input for the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "Much like in other deep architectures, the transformer maintains a representation of the input data, and progressively refines it using a sequence of so-called _transformer blocks_.\n",
    "In particular, given an initial representation $X^{(0)}$ the archtecture comprises of $M$ transformer blocks, i.e. for each $m = 1, \\dots, M,$ it computes\n",
    "\n",
    "$$X^{(m)} = \\texttt{TransformerBlock}(X^{(m-1)}).$$\n",
    "\n",
    "Each of these blocks consists of two main operations, namely a self-attention operation and a token-wise multi-layer perceptron (MLP) operation.\n",
    "The self-attention operation has the role of combining the representations of different tokens in a sequence, in order to model dependencies between the tokens.\n",
    "It is applied collectively to all tokens within the transformer block.\n",
    "The MLP operation has the role of refining the representation of each token.\n",
    "It is applied separately to each token and is shared across all tokens within a transformer block.\n",
    "Let's look at these two operations in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention\n",
    "\n",
    "__Attention.__\n",
    "The role of the first operation in a transformer block is to combine the representations of different tokens in order to model dependencies between them.\n",
    "Given a $D \\times N$ input array $X^{(m)} = (x_1, \\dots, x_N^{(m)})$ the output of the self-attention layer is another $D \\times N$ array $Y^{(m)} = (y_1, \\dots, y_N^{(m)}),$ where each column is simply a weighted average of the input features, that is\n",
    "\n",
    "$$y^{(m)}_n = \\sum_{n' = 1}^N x^{(m - 1)}_{n'} A_{n', n}^{(m)}.$$\n",
    "\n",
    "The weighting array $A_{n', n}^{(m)}$ is of size $N \\times N$ and has the property that its columns normalise to one, that is $\\sum_{n'=1}^N A_{n', n}^{(m)} = 1.$\n",
    "It is referred to the attention matrix because it weighs the extent to which the feature $y^{(m)}_n$ should depend on each $x^{(m)}_{n'},$ i.e. it determines the extent to which each $y^{(m)}_n$ should attend to each $x^{(m)}_{n'}.$\n",
    "For compactness, we can collect these equations to a single linear operation, that is\n",
    "\n",
    "$$Y^{(m)} = X^{(m - 1)} A^{(m)}.$$\n",
    "\n",
    "But what about the attention weights themselves?\n",
    "We have not specified how these are computed and, their precise definition is going to be one important factor that differentiates transformers from other architectures.\n",
    "In fact, many other operations forming the core of other archictectures, such as convolution layers in convolutional neural networks (CNNs), can be written as similar weighted sums.\n",
    "Let's next look at the specifics of the transformer attention weights.\n",
    "\n",
    "__Self-attention.__\n",
    "One of the innovations within the transformer architecture is that the attention weights are adaptive, meaning that they are computed based on the input itself.\n",
    "This is in contrast with other deep learning architectures such CNNs, where weighted sums are also used, but these weights are fixed and shared across all inputs.\n",
    "One straightforward way to compute attention weights would be to compare them by a simple similarity metric, such as an inner product.\n",
    "For example, given two tokens $x_i$ and $x_j,$ we can compute a dot-product between them, which acts as a similarity metric, exponetiate the result to make it positive and then normalise the result to ensure that each column sums to one, that is\n",
    "\n",
    "$$A^{(m)}_{n, n'} = \\frac{\\exp(x_n^\\top x_{n'})}{\\sum_{n'' = 1}^N \\exp(x_{n''}^\\top x_{n'})}.$$\n",
    "\n",
    "An alternative, slightly more flexible approach is to transform each token in the sequence by a linear map, say by applying a matrix $U \\in \\mathbb{R}^{K \\times D}$ to each token first, that is\n",
    "\n",
    "$$A^{(m)}_{n, n'} = \\frac{\\exp(x_n^\\top U^\\top U x_{n'})}{\\sum_{n'' = 1}^N \\exp(x_{n''}^\\top U^\\top U x_{n'})}.$$\n",
    "\n",
    "This allows the tokens to be compared in a different space.\n",
    "For example, if $K < D$ this approach automatically projects out some of the components of the tokens, comparing them in a lower-dimensional space.\n",
    "However, this approach still has an important limitation, namely symmetry.\n",
    "Specifically, the attention matrix above would be symmetric, which means that any two tokens would attend to each other with equal strengths.\n",
    "This might be undesirable because, for example, we could imagine that one token might be important for informing the representation of another token, but not the other way around.\n",
    "To address this, we can apply different linear operations, say $U_k$ and $U_q$ to each of the tokens being compared, and instead compute\n",
    "\n",
    "$$A^{(m)}_{n, n'} = \\frac{\\exp(x_n^\\top U_k^\\top U_q x_{n'})}{\\sum_{n'' = 1}^N \\exp(x_{n''}^\\top U_k^\\top U_q x_{n'})}.$$\n",
    "\n",
    "In this way, the resulting attention matrix that is not necessarily symmetric and an overall more expressive architecture.\n",
    "Tokens no longer have to attend to each other with the same strength.\n",
    "This weighting is known as self-attention, since each token in the sequence attends to every other token of the same sequence.\n",
    "It is also possible to generalise this to attention between different sequences, which might be useful for some applications such as, for example joint modelling of text and images.\n",
    "This generalisation is called cross-attention, and we defer its discussion for later.\n",
    "\n",
    "__Multi-head self-attention.__\n",
    "In order to increase the capacity of the self-attention layer, the transformer block includes $H$ separate self-attention operations with different parameters, in parallel.\n",
    "The results of these operations are then projected down to a single $D \\times N$ array again, which is required for further processing.\n",
    "In particular, we have\n",
    "\n",
    "```{margin}\n",
    "As a recap to the notation in these equations: the $m$ superscript runs from $1$ to $M$ and is the index of the transformer block, the $n, n'$ and $n''$ superscripts run from $1$ to $N$ and index the tokens in the sequence within the current block, the $h$ subscript runs from $1$ to $H$ and denotes a particular self-attention head in the block.\n",
    "Finally the $k$ and $q$ subscripts are not indices, but symbols distinguishing the two different kinds of matrices $U_k$ and $U_q.$\n",
    "```\n",
    "\n",
    "$$\\begin{align}\n",
    "Y^{(m)} = \\texttt{MHSA}(X^{(m - 1)}) &= \\sum^H_{h = 1} V_h^{(m)} X^{(m - 1)} A_h^{(m)}, \\text{ where } \\\\\n",
    "\\left[A^{(m)}_h\\right]_{n, n'} &= \\frac{\\exp\\left(k_{h, n}^{(m)\\top} q_{h, n'}^{(m)}\\right)}{\\sum_{n'' = 1}^N \\exp\\left(k_{h, n''}^{(m)\\top} q_{h, n'}^{(m)}\\right)} \n",
    "\\end{align}$$\n",
    "\n",
    "where $q_{h, n}^{(m)} = U^{(m)}_{q, h} x_n^{(m-1)}$ and $k_{h, n}^{(m)} = U^{(m)}_{k, h} x_n^{(m-1)}.$\n",
    "At this point we should note that, due to the nonlinearity of $A^{(m)},$ together with the multiplication by $V^{(m)}_h$ and summation across $h,$ multi-head cross attention performs not just inter-feature but also intra-feature processing, i.e. each token interacts with and changes its own representation.\n",
    "However, the capacity of this intra-feature processing is limited, and it is the job of the second stage, the MLP, to address this.\n",
    "Let's next look at the MLP stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "The self-attention layer has the role of aggregating information across tokens in a sequence to model joint dependencies.\n",
    "In order to refine the representations themselves, a simple MLP is applied to each token in isolation, in a relatively simple step\n",
    "\n",
    "$$x^{(m)}_n = \\texttt{MLP}(y^{(m)}_n).$$\n",
    "\n",
    "Note that this MLP is shared across all input locations, i.e. all tokens, within a given layer.\n",
    "\n",
    "### Residuals and normalisation\n",
    "Before putting together the $\\texttt{MHSA}$ and $\\texttt{MLP}$ operations, we will add two ubiquitous deep learning operations to improve the stability and ease of training of the model, namely residual connections and normalisation.\n",
    "\n",
    "__Residual connections.__\n",
    "Residual connections {cite}`he2015deep` are widely used across deep learning architectures, because they simplify model initialisation, stabilise learning and provide a useful inductive bias toward simpler functions.{cite}`szegedy2017inception`\n",
    "Instead of specifying a mapping of the form $x^{(m)} = f(x^{(m)}),$ a residual connection amounts to specifying a function involving an identity function plus a residual term\n",
    "\n",
    "$$x^{(m)} = x^{(m-1)} + g(x^{(m)}).$$\n",
    "\n",
    "This can be equivalently thought of as learning to model differences between the representations at different blocks, that is $x^{(m)} - x^{(m-1)} = g(x^{(m)}).$\n",
    "If we do not use residual connections and compose multiple blocks together, the activations in each can become more extreme as we go deeper in the network, resulting in either zero or extremely large gradients, which can be problematic during training.\n",
    "One motivation for using residual connections is that, if we initialise the parameters of $g$ such that its outputs are close to zero, then $x^{(m)}$ will be approximately constant across $m = 1, \\dots, M.$\n",
    "This can improve training ease and stability because all blocks in the network, even the deeper ones, receive an input close to $x^{(0)},$ and the gradients will tend to receive less extreme gradients.\n",
    "Residual connections are used both in the $\\texttt{MHSA}$ and $\\texttt{MLP}$ layers of the transformer.\n",
    "\n",
    "__Token normalisation.__\n",
    "Another ubiquitous and extremely useful operation in deep learning is normalisation.\n",
    "There are various different kinds of normalisation, including LayerNorm {cite}`ba2016layer`, BatchNorm {cite}`ioffe2015batch`, GroupNorm {cite}`wu2018group` and InstanceNorm {cite}`ulyanov2016instance`.\n",
    "Normalisation has been widely found to improve learning stability and overall model performance.\n",
    "One reason for this is that normalisation typically prevents the inputs to a layer from becoming extremely large, which can result into extreme or staturated outputs, which in turn mean that the gradients with respect to the network parameters can be close to zero or extremely large.\n",
    "The transformer architecture uses LayerNorm which, when applied to the tokens, amounts to per-token normalisation.\n",
    "Specifically, when applied to an array $X$ of input tokens, LayerNorm amounts to\n",
    "\n",
    "$$\\bar{x}_{d, n} = \\texttt{LayerNorm}(X)_{d, n} = \\gamma_d \\frac{x_{d, n} - \\mu(x_n)}{\\sigma(x_n)} + \\beta_d,$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ denote operations that compute the mean and the standard deviation respectively, and $\\gamma_d$ and $\\beta_d$ are a learnt scale and a learnt shift.\n",
    "In other words, within a transformer, LayerNorm separately normalises each token within each sequence within each batch.\n",
    "\n",
    "\n",
    "### Putting it together\n",
    "In summary, we can collect these operations into the following equations\n",
    "\n",
    "$$\\begin{align}\n",
    "\\bar{X}^{(m-1)} &= \\texttt{LayerNorm}\\left(X^{(m-1)}\\right) \\\\\n",
    "Y^{(m)} &= \\bar{X}^{(m-1)} + \\texttt{MHSA}\\left(\\bar{X}^{(m-1)}\\right) \\\\\n",
    "\\bar{Y}^{(m)} &= \\texttt{LayerNorm}\\left(Y^{(m)}\\right) \\\\\n",
    "X^{(m)} &= Y^{(m)} + \\texttt{MLP}(\\bar{Y}^{(m)})\n",
    "\\end{align}$$\n",
    "\n",
    "These make up the entirety of the transformer block, which is repeated $M$ times to compute the output of the transformer.\n",
    "An important detail we have not discussed thus far is how to build the tokens themselves.\n",
    "\n",
    "\n",
    "### Tokens and embeddings\n",
    "\n",
    "__Tokenisation.__\n",
    "Tokenisation is an application-specific detail but, generally, there are two main approaches, depending on whether the inputs are continuous or discrete.\n",
    "As a reminder, in both cases, we want convert each input element in our sequence, say $s_n,$ to a $D$-dimensional array $x^{(0)}_n.$\n",
    "We will specify a map $\\texttt{tokenise}$ that performs the operation $s_n = \\texttt{tokenise}(x^{(0)}_n)$ separately for the case where the inputs $s_n$ are discrete or continuous.\n",
    "\n",
    "__Discrete or continuous inputs.__\n",
    "In text modelling the raw inputs are integers representing unique words or characters.\n",
    "In such applications, i.e. whenever we have discrete inputs, we can use a look-up table containing learnable vectors.\n",
    "That is, if $s_n \\in \\{1, \\dots, K\\},$ we can define $K$ arrays, each of length $D$, say $z_0, \\dots, z_K \\in \\mathbb{R}^D,$ and let\n",
    "\n",
    "$$x^{(0)}_n = \\texttt{tokenise}(s_n) = z_{s_n}.$$\n",
    "\n",
    "This allows us to map each word into a continuous space and operate on the resulting arrays with the transformer architecture.\n",
    "In other applications, such as vision, the inputs are typically treated as continuous, that is $s_n \\in \\mathbb{R}^{D_s}.$\n",
    "In such cases, we can simply apply a simple operation such as a linear transformation, to map each $s_n$ into a $D$-dimensional array.\n",
    "For example, letting $W \\in \\mathbb{R}^{D\\times D_s},$ we can define\n",
    "\n",
    "$$x^{(0)}_n = W s_n,$$\n",
    "\n",
    "giving a $D$-dimensional token which is ready for use in the transformer.\n",
    "We have now covered almost all parts of the transformer, except one final, but very important point concerning the embeddings.\n",
    "Thus far, we have glossed over the fact that the transformer block has no notion of position, which is a very important issue that we look into next.\n",
    "\n",
    "__Positional embeddings.__\n",
    "Specifically, the $\\texttt{MHSA}$ operation, the token-wise $\\texttt{MLP}$ operation, as well as $\\texttt{LayerNorm}$ and residual additions are all examples of permutation equivariant: permuting the tokens and applying any one of these operations gives the same result as first applying the operation and then permuting the resulting tokens.\n",
    "Composing these operations retains permutation equivariance, meaning that permuting the elements of the original sequence and applying the transformer will yield exactly the same result as first applying the transformer and then permuting the resulting features.\n",
    "This is undesirable because, for example in text modelling, the phrases \"Arsenal bets Chelsea\" and \"Chelsea beats Arsenal\" are composed of identical words but have opposite meanings, and we would like the resulting features produced by the transformer to reflect this.\n",
    "One way to get around this issue is augmenting the tokens with information about the position of an input feature within the sequence.\n",
    "For example, we could set up an additional embedding which directly maps each position to a learnable array and concatentate the result with the tokenised feature, that is\n",
    "\n",
    "$$x^{(0)}_n = \\texttt{tokenise}_1(s_n) \\odot \\texttt{tokenise}_2(n),$$\n",
    "\n",
    "where $\\odot$ denotes concatenation, and we have used different tokenisation functions for the sequence features and the positions.\n",
    "This approach is often used in vision transformers.\n",
    "Another approach is applying, for example, sinusoidal functions with different frequencies on the input, for example\n",
    "\n",
    "$$\\texttt{tokenise}(n) = [\\sin(\\omega_1 n), \\dots, \\sin{\\omega_D n}],$$\n",
    "\n",
    "which are then concatentated to the tokenised features as described above.\n",
    "Other approaches bake in positional information directly into the $\\texttt{MHSA}$ layer, for example by making the attention weights depend on the position difference of pairs of tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now that we've covered all the details, let's implement a small transformer!\n",
    "\n",
    "### (Multi head) self-attention\n",
    "First, we turn to the $\\texttt{MHSA}$ layer, which consists of self attention layers, one for each attention head.\n",
    "Let's first define the self attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'jax' has no attribute 'version' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mequinox\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01meqx\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rw/lib/python3.10/site-packages/jax/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _cloud_tpu_init\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_core\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _core\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Note: import <name> as <name> is required for names to be exported.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# See PEP 484 & https://github.com/google/jax/issues/7570\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rw/lib/python3.10/site-packages/jax/core.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The JAX Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Note: import <name> as <name> is required for names to be exported.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# See PEP 484 & https://github.com/google/jax/issues/7570\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m   AbstractToken \u001b[38;5;28;01mas\u001b[39;00m AbstractToken,\n\u001b[1;32m     20\u001b[0m   AbstractValue \u001b[38;5;28;01mas\u001b[39;00m AbstractValue,\n\u001b[1;32m     21\u001b[0m   Atom \u001b[38;5;28;01mas\u001b[39;00m Atom,\n\u001b[1;32m     22\u001b[0m   AxisSize \u001b[38;5;28;01mas\u001b[39;00m AxisSize,\n\u001b[1;32m     23\u001b[0m   CallPrimitive \u001b[38;5;28;01mas\u001b[39;00m CallPrimitive,\n\u001b[1;32m     24\u001b[0m   ClosedJaxpr \u001b[38;5;28;01mas\u001b[39;00m ClosedJaxpr,\n\u001b[1;32m     25\u001b[0m   ConcreteArray \u001b[38;5;28;01mas\u001b[39;00m ConcreteArray,\n\u001b[1;32m     26\u001b[0m   ConcretizationTypeError \u001b[38;5;28;01mas\u001b[39;00m ConcretizationTypeError,\n\u001b[1;32m     27\u001b[0m   DShapedArray \u001b[38;5;28;01mas\u001b[39;00m DShapedArray,\n\u001b[1;32m     28\u001b[0m   DropVar \u001b[38;5;28;01mas\u001b[39;00m DropVar,\n\u001b[1;32m     29\u001b[0m   Effect \u001b[38;5;28;01mas\u001b[39;00m Effect,\n\u001b[1;32m     30\u001b[0m   Effects \u001b[38;5;28;01mas\u001b[39;00m Effects,\n\u001b[1;32m     31\u001b[0m   EvalTrace \u001b[38;5;28;01mas\u001b[39;00m EvalTrace,\n\u001b[1;32m     32\u001b[0m   InDBIdx \u001b[38;5;28;01mas\u001b[39;00m InDBIdx,\n\u001b[1;32m     33\u001b[0m   InconclusiveDimensionOperation \u001b[38;5;28;01mas\u001b[39;00m InconclusiveDimensionOperation,\n\u001b[1;32m     34\u001b[0m   InputType \u001b[38;5;28;01mas\u001b[39;00m InputType,\n\u001b[1;32m     35\u001b[0m   Jaxpr \u001b[38;5;28;01mas\u001b[39;00m Jaxpr,\n\u001b[1;32m     36\u001b[0m   JaxprDebugInfo \u001b[38;5;28;01mas\u001b[39;00m JaxprDebugInfo,\n\u001b[1;32m     37\u001b[0m   JaxprEqn \u001b[38;5;28;01mas\u001b[39;00m JaxprEqn,\n\u001b[1;32m     38\u001b[0m   JaxprPpContext \u001b[38;5;28;01mas\u001b[39;00m JaxprPpContext,\n\u001b[1;32m     39\u001b[0m   JaxprPpSettings \u001b[38;5;28;01mas\u001b[39;00m JaxprPpSettings,\n\u001b[1;32m     40\u001b[0m   JaxprTypeError \u001b[38;5;28;01mas\u001b[39;00m JaxprTypeError,\n\u001b[1;32m     41\u001b[0m   Literal \u001b[38;5;28;01mas\u001b[39;00m Literal,\n\u001b[1;32m     42\u001b[0m   MainTrace \u001b[38;5;28;01mas\u001b[39;00m MainTrace,\n\u001b[1;32m     43\u001b[0m   MapPrimitive \u001b[38;5;28;01mas\u001b[39;00m MapPrimitive,\n\u001b[1;32m     44\u001b[0m   NameGatheringSubst \u001b[38;5;28;01mas\u001b[39;00m NameGatheringSubst,\n\u001b[1;32m     45\u001b[0m   NamedShape \u001b[38;5;28;01mas\u001b[39;00m NamedShape,\n\u001b[1;32m     46\u001b[0m   OutDBIdx \u001b[38;5;28;01mas\u001b[39;00m OutDBIdx,\n\u001b[1;32m     47\u001b[0m   OutputType \u001b[38;5;28;01mas\u001b[39;00m OutputType,\n\u001b[1;32m     48\u001b[0m   ParamDict \u001b[38;5;28;01mas\u001b[39;00m ParamDict,\n\u001b[1;32m     49\u001b[0m   Primitive \u001b[38;5;28;01mas\u001b[39;00m Primitive,\n\u001b[1;32m     50\u001b[0m   ShapedArray \u001b[38;5;28;01mas\u001b[39;00m ShapedArray,\n\u001b[1;32m     51\u001b[0m   Sublevel \u001b[38;5;28;01mas\u001b[39;00m Sublevel,\n\u001b[1;32m     52\u001b[0m   TRACER_LEAK_DEBUGGER_WARNING \u001b[38;5;28;01mas\u001b[39;00m TRACER_LEAK_DEBUGGER_WARNING,\n\u001b[1;32m     53\u001b[0m   ThreadLocalState \u001b[38;5;28;01mas\u001b[39;00m ThreadLocalState,\n\u001b[1;32m     54\u001b[0m   Token \u001b[38;5;28;01mas\u001b[39;00m Token,\n\u001b[1;32m     55\u001b[0m   Trace \u001b[38;5;28;01mas\u001b[39;00m Trace,\n\u001b[1;32m     56\u001b[0m   TraceStack \u001b[38;5;28;01mas\u001b[39;00m TraceStack,\n\u001b[1;32m     57\u001b[0m   TraceState \u001b[38;5;28;01mas\u001b[39;00m TraceState,\n\u001b[1;32m     58\u001b[0m   Tracer \u001b[38;5;28;01mas\u001b[39;00m Tracer,\n\u001b[1;32m     59\u001b[0m   UnshapedArray \u001b[38;5;28;01mas\u001b[39;00m UnshapedArray,\n\u001b[1;32m     60\u001b[0m   Value \u001b[38;5;28;01mas\u001b[39;00m Value,\n\u001b[1;32m     61\u001b[0m   Var \u001b[38;5;28;01mas\u001b[39;00m Var,\n\u001b[1;32m     62\u001b[0m   abstract_token \u001b[38;5;28;01mas\u001b[39;00m abstract_token,\n\u001b[1;32m     63\u001b[0m   apply_todos \u001b[38;5;28;01mas\u001b[39;00m apply_todos,\n\u001b[1;32m     64\u001b[0m   as_named_shape \u001b[38;5;28;01mas\u001b[39;00m as_named_shape,\n\u001b[1;32m     65\u001b[0m   aval_mapping_handlers \u001b[38;5;28;01mas\u001b[39;00m aval_mapping_handlers,\n\u001b[1;32m     66\u001b[0m   axis_frame \u001b[38;5;28;01mas\u001b[39;00m axis_frame,\n\u001b[1;32m     67\u001b[0m   call \u001b[38;5;28;01mas\u001b[39;00m call,\n\u001b[1;32m     68\u001b[0m   call_bind_with_continuation \u001b[38;5;28;01mas\u001b[39;00m call_bind_with_continuation,\n\u001b[1;32m     69\u001b[0m   call_impl \u001b[38;5;28;01mas\u001b[39;00m call_impl,\n\u001b[1;32m     70\u001b[0m   call_p \u001b[38;5;28;01mas\u001b[39;00m call_p,\n\u001b[1;32m     71\u001b[0m   canonicalize_shape \u001b[38;5;28;01mas\u001b[39;00m _deprecated_canonicalize_shape,\n\u001b[1;32m     72\u001b[0m   check_eqn \u001b[38;5;28;01mas\u001b[39;00m check_eqn,\n\u001b[1;32m     73\u001b[0m   check_jaxpr \u001b[38;5;28;01mas\u001b[39;00m check_jaxpr,\n\u001b[1;32m     74\u001b[0m   check_type \u001b[38;5;28;01mas\u001b[39;00m check_type,\n\u001b[1;32m     75\u001b[0m   check_valid_jaxtype \u001b[38;5;28;01mas\u001b[39;00m check_valid_jaxtype,\n\u001b[1;32m     76\u001b[0m   closed_call_p \u001b[38;5;28;01mas\u001b[39;00m closed_call_p,\n\u001b[1;32m     77\u001b[0m   concrete_aval \u001b[38;5;28;01mas\u001b[39;00m concrete_aval,\n\u001b[1;32m     78\u001b[0m   concrete_or_error \u001b[38;5;28;01mas\u001b[39;00m concrete_or_error,\n\u001b[1;32m     79\u001b[0m   concretization_function_error \u001b[38;5;28;01mas\u001b[39;00m concretization_function_error,\n\u001b[1;32m     80\u001b[0m   cur_sublevel \u001b[38;5;28;01mas\u001b[39;00m cur_sublevel,\n\u001b[1;32m     81\u001b[0m   custom_typechecks \u001b[38;5;28;01mas\u001b[39;00m custom_typechecks,\n\u001b[1;32m     82\u001b[0m   dedup_referents \u001b[38;5;28;01mas\u001b[39;00m dedup_referents,\n\u001b[1;32m     83\u001b[0m   definitely_equal \u001b[38;5;28;01mas\u001b[39;00m _deprecated_definitely_equal,\n\u001b[1;32m     84\u001b[0m   dimension_as_value \u001b[38;5;28;01mas\u001b[39;00m _deprecated_dimension_as_value,\n\u001b[1;32m     85\u001b[0m   do_subst_axis_names_jaxpr \u001b[38;5;28;01mas\u001b[39;00m do_subst_axis_names_jaxpr,\n\u001b[1;32m     86\u001b[0m   ensure_compile_time_eval \u001b[38;5;28;01mas\u001b[39;00m ensure_compile_time_eval,\n\u001b[1;32m     87\u001b[0m   escaped_tracer_error \u001b[38;5;28;01mas\u001b[39;00m escaped_tracer_error,\n\u001b[1;32m     88\u001b[0m   eval_context \u001b[38;5;28;01mas\u001b[39;00m eval_context,\n\u001b[1;32m     89\u001b[0m   eval_jaxpr \u001b[38;5;28;01mas\u001b[39;00m eval_jaxpr,\n\u001b[1;32m     90\u001b[0m   extend_axis_env \u001b[38;5;28;01mas\u001b[39;00m extend_axis_env,\n\u001b[1;32m     91\u001b[0m   extend_axis_env_nd \u001b[38;5;28;01mas\u001b[39;00m extend_axis_env_nd,\n\u001b[1;32m     92\u001b[0m   find_top_trace \u001b[38;5;28;01mas\u001b[39;00m find_top_trace,\n\u001b[1;32m     93\u001b[0m   full_lower \u001b[38;5;28;01mas\u001b[39;00m full_lower,\n\u001b[1;32m     94\u001b[0m   gensym \u001b[38;5;28;01mas\u001b[39;00m gensym,\n\u001b[1;32m     95\u001b[0m   get_aval \u001b[38;5;28;01mas\u001b[39;00m get_aval,\n\u001b[1;32m     96\u001b[0m   get_referent \u001b[38;5;28;01mas\u001b[39;00m get_referent,\n\u001b[1;32m     97\u001b[0m   is_constant_dim \u001b[38;5;28;01mas\u001b[39;00m is_constant_dim,\n\u001b[1;32m     98\u001b[0m   is_constant_shape \u001b[38;5;28;01mas\u001b[39;00m is_constant_shape,\n\u001b[1;32m     99\u001b[0m   jaxpr_as_fun \u001b[38;5;28;01mas\u001b[39;00m jaxpr_as_fun,\n\u001b[1;32m    100\u001b[0m   jaxpr_uses_outfeed \u001b[38;5;28;01mas\u001b[39;00m jaxpr_uses_outfeed,\n\u001b[1;32m    101\u001b[0m   jaxprs_in_params \u001b[38;5;28;01mas\u001b[39;00m jaxprs_in_params,\n\u001b[1;32m    102\u001b[0m   join_effects \u001b[38;5;28;01mas\u001b[39;00m join_effects,\n\u001b[1;32m    103\u001b[0m   join_named_shapes \u001b[38;5;28;01mas\u001b[39;00m join_named_shapes,\n\u001b[1;32m    104\u001b[0m   lattice_join \u001b[38;5;28;01mas\u001b[39;00m lattice_join,\n\u001b[1;32m    105\u001b[0m   leaked_tracer_error \u001b[38;5;28;01mas\u001b[39;00m leaked_tracer_error,\n\u001b[1;32m    106\u001b[0m   literalable_types \u001b[38;5;28;01mas\u001b[39;00m literalable_types,\n\u001b[1;32m    107\u001b[0m   map_bind \u001b[38;5;28;01mas\u001b[39;00m map_bind,\n\u001b[1;32m    108\u001b[0m   map_bind_with_continuation \u001b[38;5;28;01mas\u001b[39;00m map_bind_with_continuation,\n\u001b[1;32m    109\u001b[0m   mapped_aval \u001b[38;5;28;01mas\u001b[39;00m mapped_aval,\n\u001b[1;32m    110\u001b[0m   maybe_find_leaked_tracers \u001b[38;5;28;01mas\u001b[39;00m maybe_find_leaked_tracers,\n\u001b[1;32m    111\u001b[0m   max_dim \u001b[38;5;28;01mas\u001b[39;00m max_dim,\n\u001b[1;32m    112\u001b[0m   min_dim \u001b[38;5;28;01mas\u001b[39;00m min_dim,\n\u001b[1;32m    113\u001b[0m   new_base_main \u001b[38;5;28;01mas\u001b[39;00m new_base_main,\n\u001b[1;32m    114\u001b[0m   new_jaxpr_eqn \u001b[38;5;28;01mas\u001b[39;00m new_jaxpr_eqn,\n\u001b[1;32m    115\u001b[0m   new_main \u001b[38;5;28;01mas\u001b[39;00m new_main,\n\u001b[1;32m    116\u001b[0m   new_sublevel \u001b[38;5;28;01mas\u001b[39;00m new_sublevel,\n\u001b[1;32m    117\u001b[0m   no_axis_name \u001b[38;5;28;01mas\u001b[39;00m no_axis_name,\n\u001b[1;32m    118\u001b[0m   no_effects \u001b[38;5;28;01mas\u001b[39;00m no_effects,\n\u001b[1;32m    119\u001b[0m   non_negative_dim \u001b[38;5;28;01mas\u001b[39;00m _deprecated_non_negative_dim,\n\u001b[1;32m    120\u001b[0m   outfeed_primitives \u001b[38;5;28;01mas\u001b[39;00m outfeed_primitives,\n\u001b[1;32m    121\u001b[0m   pp_aval \u001b[38;5;28;01mas\u001b[39;00m pp_aval,\n\u001b[1;32m    122\u001b[0m   pp_eqn \u001b[38;5;28;01mas\u001b[39;00m pp_eqn,\n\u001b[1;32m    123\u001b[0m   pp_eqn_rules \u001b[38;5;28;01mas\u001b[39;00m pp_eqn_rules,\n\u001b[1;32m    124\u001b[0m   pp_eqns \u001b[38;5;28;01mas\u001b[39;00m pp_eqns,\n\u001b[1;32m    125\u001b[0m   pp_jaxpr \u001b[38;5;28;01mas\u001b[39;00m pp_jaxpr,\n\u001b[1;32m    126\u001b[0m   pp_jaxpr_eqn_range \u001b[38;5;28;01mas\u001b[39;00m pp_jaxpr_eqn_range,\n\u001b[1;32m    127\u001b[0m   pp_jaxpr_skeleton \u001b[38;5;28;01mas\u001b[39;00m pp_jaxpr_skeleton,\n\u001b[1;32m    128\u001b[0m   pp_jaxprs \u001b[38;5;28;01mas\u001b[39;00m pp_jaxprs,\n\u001b[1;32m    129\u001b[0m   pp_kv_pair \u001b[38;5;28;01mas\u001b[39;00m pp_kv_pair,\n\u001b[1;32m    130\u001b[0m   pp_kv_pairs \u001b[38;5;28;01mas\u001b[39;00m pp_kv_pairs,\n\u001b[1;32m    131\u001b[0m   pp_var \u001b[38;5;28;01mas\u001b[39;00m pp_var,\n\u001b[1;32m    132\u001b[0m   pp_vars \u001b[38;5;28;01mas\u001b[39;00m pp_vars,\n\u001b[1;32m    133\u001b[0m   primal_dtype_to_tangent_dtype \u001b[38;5;28;01mas\u001b[39;00m primal_dtype_to_tangent_dtype,\n\u001b[1;32m    134\u001b[0m   primitive_uses_outfeed \u001b[38;5;28;01mas\u001b[39;00m primitive_uses_outfeed,\n\u001b[1;32m    135\u001b[0m   process_env_traces_call \u001b[38;5;28;01mas\u001b[39;00m process_env_traces_call,\n\u001b[1;32m    136\u001b[0m   process_env_traces_map \u001b[38;5;28;01mas\u001b[39;00m process_env_traces_map,\n\u001b[1;32m    137\u001b[0m   pytype_aval_mappings \u001b[38;5;28;01mas\u001b[39;00m pytype_aval_mappings,\n\u001b[1;32m    138\u001b[0m   raise_as_much_as_possible \u001b[38;5;28;01mas\u001b[39;00m raise_as_much_as_possible,\n\u001b[1;32m    139\u001b[0m   raise_to_shaped \u001b[38;5;28;01mas\u001b[39;00m raise_to_shaped,\n\u001b[1;32m    140\u001b[0m   raise_to_shaped_mappings \u001b[38;5;28;01mas\u001b[39;00m raise_to_shaped_mappings,\n\u001b[1;32m    141\u001b[0m   reset_trace_state \u001b[38;5;28;01mas\u001b[39;00m reset_trace_state,\n\u001b[1;32m    142\u001b[0m   stash_axis_env \u001b[38;5;28;01mas\u001b[39;00m stash_axis_env,\n\u001b[1;32m    143\u001b[0m   str_eqn_compact \u001b[38;5;28;01mas\u001b[39;00m str_eqn_compact,\n\u001b[1;32m    144\u001b[0m   subjaxprs \u001b[38;5;28;01mas\u001b[39;00m subjaxprs,\n\u001b[1;32m    145\u001b[0m   subst_axis_names \u001b[38;5;28;01mas\u001b[39;00m subst_axis_names,\n\u001b[1;32m    146\u001b[0m   subst_axis_names_eqn \u001b[38;5;28;01mas\u001b[39;00m subst_axis_names_eqn,\n\u001b[1;32m    147\u001b[0m   subst_axis_names_jaxpr \u001b[38;5;28;01mas\u001b[39;00m subst_axis_names_jaxpr,\n\u001b[1;32m    148\u001b[0m   subst_axis_names_var \u001b[38;5;28;01mas\u001b[39;00m subst_axis_names_var,\n\u001b[1;32m    149\u001b[0m   substitute_vars_in_output_ty \u001b[38;5;28;01mas\u001b[39;00m substitute_vars_in_output_ty,\n\u001b[1;32m    150\u001b[0m   thread_local_state \u001b[38;5;28;01mas\u001b[39;00m thread_local_state,\n\u001b[1;32m    151\u001b[0m   trace_state_clean \u001b[38;5;28;01mas\u001b[39;00m trace_state_clean,\n\u001b[1;32m    152\u001b[0m   traverse_jaxpr_params \u001b[38;5;28;01mas\u001b[39;00m traverse_jaxpr_params,\n\u001b[1;32m    153\u001b[0m   typecheck \u001b[38;5;28;01mas\u001b[39;00m typecheck,\n\u001b[1;32m    154\u001b[0m   typecompat \u001b[38;5;28;01mas\u001b[39;00m typecompat,\n\u001b[1;32m    155\u001b[0m   typematch \u001b[38;5;28;01mas\u001b[39;00m typematch,\n\u001b[1;32m    156\u001b[0m   unmapped_aval \u001b[38;5;28;01mas\u001b[39;00m unmapped_aval,\n\u001b[1;32m    157\u001b[0m   used_axis_names \u001b[38;5;28;01mas\u001b[39;00m used_axis_names,\n\u001b[1;32m    158\u001b[0m   used_axis_names_jaxpr \u001b[38;5;28;01mas\u001b[39;00m used_axis_names_jaxpr,\n\u001b[1;32m    159\u001b[0m   valid_jaxtype \u001b[38;5;28;01mas\u001b[39;00m valid_jaxtype,\n\u001b[1;32m    160\u001b[0m )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core \u001b[38;5;28;01mas\u001b[39;00m _src_core\n\u001b[1;32m    164\u001b[0m _deprecations \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Added Oct 11, 2023:\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimSize\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     ),\n\u001b[1;32m    191\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/rw/lib/python3.10/site-packages/jax/_src/core.py:39\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ref\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m effects\n",
      "File \u001b[0;32m~/miniconda3/envs/rw/lib/python3.10/site-packages/jax/_src/dtypes.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DType, DTypeLike\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_module, StrictABC\n",
      "File \u001b[0;32m~/miniconda3/envs/rw/lib/python3.10/site-packages/jax/_src/config.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Generic, NamedTuple, NoReturn, TypeVar, cast\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jax_jit\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transfer_guard_lib\n",
      "File \u001b[0;32m~/miniconda3/envs/rw/lib/python3.10/site-packages/jax/_src/lib/__init__.py:76\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _jaxlib_version\n\u001b[1;32m     74\u001b[0m version_str \u001b[38;5;241m=\u001b[39m jaxlib\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     75\u001b[0m version \u001b[38;5;241m=\u001b[39m check_jaxlib_version(\n\u001b[0;32m---> 76\u001b[0m   jax_version\u001b[38;5;241m=\u001b[39m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[1;32m     77\u001b[0m   jaxlib_version\u001b[38;5;241m=\u001b[39mjaxlib\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[1;32m     78\u001b[0m   minimum_jaxlib_version\u001b[38;5;241m=\u001b[39mjax\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39m_minimum_jaxlib_version)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Before importing any C compiled modules from jaxlib, first import the CPU\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# feature guard module to verify that jaxlib was compiled in a way that only\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# uses instructions that are present on this machine.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpu_feature_guard\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcpu_feature_guard\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'jax' has no attribute 'version' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "# import optax\n",
    "from jaxtyping import Float, Int, Array, PyTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "Note that, the softmax operation used in the `self_attention_weights` method exponentiates the entries of an array and divides them by the sum of the resulting entries, in one step.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(eqx.Module):\n",
    "\n",
    "    Uk: Float[Array, \"K D\"]\n",
    "    Uq: Float[Array, \"K D\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        input_dim: int,\n",
    "        projection_dim: int,\n",
    "    ):\n",
    "\n",
    "        # Initialise projection matrices\n",
    "        key1, key2 = jax.random.split(key)\n",
    "        self.Uk = jax.random.normal(key1, (projection_dim, input_dim))\n",
    "        self.Uq = jax.random.normal(key2, (projection_dim, input_dim))\n",
    "\n",
    "\n",
    "    def self_attention_weights(\n",
    "        self,\n",
    "        x: Float[Array, \"D N\"],\n",
    "    ) -> Float[Array, \"N N\"]:\n",
    "        \"\"\"\n",
    "        Compute self-attention weights for tokens in a sequence\n",
    "\n",
    "        Args:\n",
    "            x: input sequence of tokens, shape (D, N)\n",
    "        \n",
    "        Returns:\n",
    "            attention weights, shape (N, N)\n",
    "        \"\"\"\n",
    "        return jax.nn.softmax((self.Uk @ x).T @ (self.Uq @ x), axis=0)\n",
    "    \n",
    "    def __call__(self, x: Float[Array, \"D N\"]) -> Float[Array, \"D N\"]:\n",
    "        \"\"\"\n",
    "        Apply self-attention to a sequence of tokens\n",
    "\n",
    "        Args:\n",
    "            x: input sequence of tokens, shape (D, N)\n",
    "\n",
    "        Returns:\n",
    "            output sequence of tokens, shape (D, N)\n",
    "        \"\"\"\n",
    "        return x @ self.self_attention_weights(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention is remarkably simple!\n",
    "Now, we can define multi-head self attention.\n",
    "This is a simple extension of our existing self-attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(eqx.Module):\n",
    "    \n",
    "    self_attention: List[SelfAttention]\n",
    "    linear: List[jnp.ndarray]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        input_dim: int,\n",
    "        projection_dim: int,\n",
    "        num_heads: int,\n",
    "    ):\n",
    "\n",
    "        keys = jax.random.split(key, 2*num_heads)\n",
    "        self.self_attention = [\n",
    "            SelfAttention(\n",
    "                key=key,\n",
    "                input_dim=input_dim,\n",
    "                projection_dim=projection_dim,\n",
    "            ) for key in keys[::2]\n",
    "        ]\n",
    "\n",
    "        self.linear = [\n",
    "            jax.random.normal(key, (input_dim, input_dim))\n",
    "            for key in keys[1::2]\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"D N\"]) -> Float[Array, \"D N\"]:\n",
    "        \"\"\"\n",
    "        Apply multi-head self-attention to a sequence of tokens\n",
    "\n",
    "        Args:\n",
    "            x: input sequence of tokens, shape (D, N)\n",
    "\n",
    "        Returns:\n",
    "            output sequence of tokens, shape (D, N)\n",
    "        \"\"\"\n",
    "            \n",
    "        # Compute tokens for each head and apply linear \n",
    "        heads = [\n",
    "            V @ sa(x) for sa, V in zip(self.self_attention, self.linear)\n",
    "        ]\n",
    "\n",
    "        # Stack and sum across heads\n",
    "        return jnp.sum(jnp.stack(heads, axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "Now we turn to the MLP.\n",
    "This is also a very simple implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(eqx.Module):\n",
    "\n",
    "    layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        num_hidden: int,\n",
    "        num_layers: int,\n",
    "        num_input: int,\n",
    "        num_output: Optional[int] = None,\n",
    "    ):\n",
    "\n",
    "        # Set up input and output dimensions of linear layers\n",
    "        in_feats = [num_input] + [num_hidden] * num_layers\n",
    "        out_feats = [num_hidden] * num_layers + [num_output]\n",
    "\n",
    "        # Split the random key into sub-keys for each layer\n",
    "        keys = jax.random.split(key, num_layers+1)\n",
    "        \n",
    "        # Create linear layers with different random keys\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(\n",
    "                key=key,\n",
    "                in_features=in_feat,\n",
    "                out_features=out_feat,\n",
    "            )\n",
    "            for key, in_feat, out_feat in zip(keys, in_feats, out_feats)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"D\"]) -> Float[Array, \"D\"]:\n",
    "        \"\"\"\n",
    "        Compute forward pass through the MLP.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor of shape (in_features,)\n",
    "        \n",
    "        Returns:\n",
    "            output tensor of shape (out_features,)\n",
    "        \"\"\"\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            x = jax.nn.relu(x)\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token normalisation\n",
    "\n",
    "Before setting up the transformer block, let's look at the normalisation layers.\n",
    "We are after normalising each token separately, which amounts to applying a `LayerNorm` operation on each token, and we'll wrap this in a single module called `TokenNorm`.\n",
    "\n",
    "```{margin}\n",
    "A note on `vmap`:\n",
    "In JAX, layers do not work on batches of examples by default.\n",
    "Suppose we have a function `f` which works on input arrays of some shape `[...]` and we want to apply it to a batched array `x` of shape `[B, ...]`.\n",
    "We can do this easily using `jax.vmap`, e.g. `jax.vmap(f)(x)`.\n",
    "This makes our code much clearer since we can drop batching dimensions altogether when writing a model, and then re-introduce them by using `vmap` in the end.\n",
    "Here we have used this to apply the `LayerNorm` and `MLP` operations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenNorm(eqx.Module):\n",
    "\n",
    "    layer_norm: eqx.nn.LayerNorm\n",
    "\n",
    "    def __init__(self, input_dim: int):\n",
    "        self.layer_norm = eqx.nn.LayerNorm(shape=(input_dim,))\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"D N\"]) -> Float[Array, \"D N\"]:\n",
    "        \"\"\"\n",
    "        Apply layer normalization to a sequence of tokens\n",
    "\n",
    "        Args:\n",
    "            x: input sequence of tokens, shape (D, N)\n",
    "\n",
    "        Returns:\n",
    "            output sequence of tokens, shape (D, N)\n",
    "        \"\"\"\n",
    "        return jax.vmap(self.layer_norm)(x.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer block\n",
    "\n",
    "Now we're ready to define the transformer block, which consists of the multi-head self attention and mlp operations, as well as two normalisation layers, connected with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(eqx.Module):\n",
    "\n",
    "    mhsa: MultiHeadSelfAttention\n",
    "    mlp: MLP\n",
    "    tn1: TokenNorm\n",
    "    tn2: TokenNorm\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        input_dim: int,\n",
    "        mlp_num_hidden: int,\n",
    "        mlp_num_layers: int,\n",
    "        num_heads: int,\n",
    "    ):\n",
    "\n",
    "        key1, key2 = jax.random.split(key)\n",
    "        self.mhsa = MultiHeadSelfAttention(\n",
    "            key=key1,\n",
    "            input_dim=input_dim,\n",
    "            projection_dim=input_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            key=key2,\n",
    "            num_hidden=mlp_num_hidden,\n",
    "            num_layers=mlp_num_layers,\n",
    "            num_input=input_dim,\n",
    "            num_output=input_dim,\n",
    "        )\n",
    "\n",
    "        self.tn1 = TokenNorm(input_dim)\n",
    "        self.tn2 = TokenNorm(input_dim)\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"D N\"]) -> Float[Array, \"D N\"]:\n",
    "    \n",
    "        x = x + self.mhsa(self.tn1(x))\n",
    "        x = x + jax.vmap(self.mlp)(self.tn2(x).T).T\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens and embeddings\n",
    "\n",
    "Next up, we'll need to also define how to tokenise a sequence and and generate positional embeddings.\n",
    "First, let us consider an image classification task for the moment, and use the vision transformer (ViT) {cite}`dosovitskiy2021image` embedding style.\n",
    "In ViT, an image is split into smaller sub-images called patches, each of which is linearly projected to form an embedding.\n",
    "Convolutions are very handy here:\n",
    "We can split an image into patches and project these, in one go, using convolutions with a stride equal to the kernel size.\n",
    "\n",
    "```{margin}\n",
    "The `Conv2D` layer here splits the image into patches and linearly embeds each one, doing both steps in one go.\n",
    "Specifically, convolution is a linear operation on a patch of size `(k, k)` where `k` is the kernel size.\n",
    "By using striding (with a stride equal to the kernel size), we ensure each patch is processed separately, resulting in an image of size `(k/p, k/p)` where `p` is the patch size, which is then reshaped into one long sequence of tokens.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTokeniser(eqx.Module):\n",
    "\n",
    "    conv: eqx.nn.Conv2d\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        input_dim: int,\n",
    "        token_dimension: int,\n",
    "        patch_size: int,\n",
    "    ):\n",
    "        assert patch_size % 2 == 0, \"Patch size must be even\"\n",
    "\n",
    "        self.conv = eqx.nn.Conv2d(\n",
    "            key=key,\n",
    "            in_channels=input_dim,\n",
    "            out_channels=token_dimension,\n",
    "            kernel_size=(patch_size, patch_size),\n",
    "            stride=(patch_size, patch_size),\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"C H W\"]) -> Float[Array, \"D N\"]:\n",
    "\n",
    "        assert (\n",
    "            x.shape[1] % self.conv.kernel_size[0] == 0\n",
    "            and x.shape[2] % self.conv.kernel_size[1] == 0\n",
    "        ), \"Input dimensions must be divisible by patch size\"\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x.reshape(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to position embeddings.\n",
    "For simplicity, let us assume that all sequences have a fixed length, i.e. all images we will process will process images of a fixed height and width.\n",
    "We'll adopt a fairly general approach, by letting each embedding be a learnable array, and using a different such array for each position in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(eqx.Module):\n",
    "\n",
    "    embeddings: Float[Array, \"D N\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        input_dim: int,\n",
    "        sequence_length: int,\n",
    "    ):\n",
    "        self.embeddings = jax.random.normal(key, (input_dim, sequence_length))\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"D N\"]) -> Float[Array, \"D N\"]:\n",
    "        return x + self.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "Finally, we're ready to put together the full transformer architecture.\n",
    "We'll build a little ViT using our transformer blocks, image tokeniser and position embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVisionTransformer(eqx.Module):\n",
    "\n",
    "    blocks: List[TransformerBlock]\n",
    "    tokeniser: ImageTokeniser\n",
    "    embedding: PositionEmbedding\n",
    "    final_mlp: MLP\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jax.random.PRNGKey,\n",
    "        tokeniser: ImageTokeniser,\n",
    "        embedding: PositionEmbedding,\n",
    "        token_dim: int,\n",
    "        mlp_num_hidden: int,\n",
    "        mlp_num_layers: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        num_classes: int,\n",
    "    ):\n",
    "\n",
    "        keys = jax.random.split(key, num_blocks+1)\n",
    "        self.blocks = [\n",
    "            TransformerBlock(\n",
    "                key,\n",
    "                token_dim,\n",
    "                mlp_num_hidden,\n",
    "                mlp_num_layers,\n",
    "                num_heads,\n",
    "            )\n",
    "            for key in keys[:-1]\n",
    "        ]\n",
    "\n",
    "        self.final_mlp = MLP(\n",
    "            key=keys[-1],\n",
    "            num_hidden=mlp_num_hidden,\n",
    "            num_layers=mlp_num_layers,\n",
    "            num_input=token_dim,\n",
    "            num_output=num_classes,\n",
    "        )\n",
    "\n",
    "        self.tokeniser = tokeniser\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def __call__(self, image: Float[Array, \"C H W\"]) -> Float[Array, \"D N\"]:\n",
    "\n",
    "        x = self.tokeniser(image)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.final_mlp(jax.numpy.mean(x, axis=1))\n",
    "        return x - jax.scipy.special.logsumexp(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "By now, MNIST has become a rather dull example.\n",
    "At the same time, because this is meant to be a demo that should run on a laptop, so we'll go for the next easiest thing which at least has some color: CIFAR 10.\n",
    "We'll use [tensorflow datasets](https://www.tensorflow.org/datasets/api_docs/python/tfds) to load the data and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    image = 2. * (tf.cast(image, tf.float32) / 255.) - 1.\n",
    "    image = tf.transpose(image, [2, 0, 1])\n",
    "    return image, label\n",
    "\n",
    "def get_batches(batch_size: int, split: str, data_dir: str=\"/tmp/tfds\"):\n",
    "\n",
    "    assert split in [\"train\", \"test\"], \"Split must be 'train' or 'test'\"\n",
    "    ds = tfds.load(\n",
    "        name=\"cifar10\",\n",
    "        split=split,\n",
    "        as_supervised=True,\n",
    "        data_dir=data_dir,\n",
    "    )\n",
    "    ds = ds.map(preprocess_image)\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    return tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "<!-- We need a few more bits to get our transformer to classify images.\n",
    "First, we need to somehow convert the processed tokens that it outputs into predictions, i.e. predicted class probabilities.\n",
    "Second, we need to define a loss, the cross entropy, to train with.\n",
    "Let's make  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy_loss_batch\u001b[39m(\n\u001b[1;32m     11\u001b[0m     model: TinyVisionTransformer,\n\u001b[1;32m     12\u001b[0m     images: Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB C H W\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     13\u001b[0m     labels: Int[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     14\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean(jax\u001b[38;5;241m.\u001b[39mvmap(cross_entropy_loss, in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))(model, images, labels))\n\u001b[0;32m---> 17\u001b[0m optim \u001b[38;5;241m=\u001b[39m \u001b[43moptax\u001b[49m\u001b[38;5;241m.\u001b[39madam(\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_jit\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_step\u001b[39m(\n\u001b[1;32m     21\u001b[0m     model: TinyVisionTransformer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     y: Int[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     25\u001b[0m ):\n\u001b[1;32m     26\u001b[0m     loss_value, grads \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mfilter_value_and_grad(cross_entropy_loss_batch)(model, x, y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optax' is not defined"
     ]
    }
   ],
   "source": [
    "def cross_entropy_loss(\n",
    "    model: TinyVisionTransformer,\n",
    "    image: Float[Array, \"C H W\"],\n",
    "    label: Int[Array, \"\"],\n",
    ") -> Float[Array, \"\"]:\n",
    "    logits = model(image)\n",
    "\n",
    "    return -jnp.mean(jnp.sum(logits * jax.nn.one_hot(label, num_classes=logits.shape[-1]), axis=-1))\n",
    "\n",
    "def cross_entropy_loss_batch(\n",
    "    model: TinyVisionTransformer,\n",
    "    images: Float[Array, \"B C H W\"],\n",
    "    labels: Int[Array, \"B\"],\n",
    ") -> Float[Array, \"B\"]:\n",
    "    return jnp.mean(jax.vmap(cross_entropy_loss, in_axes=(None, 0, 0))(model, images, labels))\n",
    "\n",
    "optim = optax.adam(1e-4)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(\n",
    "    model: TinyVisionTransformer,\n",
    "    opt_state: PyTree,\n",
    "    x: Float[Array, \"B C H W\"],\n",
    "    y: Int[Array, \"B\"],\n",
    "):\n",
    "    loss_value, grads = eqx.filter_value_and_grad(cross_entropy_loss_batch)(model, x, y)\n",
    "    updates, opt_state = optim.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n",
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyVisionTransformer(\n",
      "  blocks=[\n",
      "    TransformerBlock(\n",
      "      mhsa=MultiHeadSelfAttention(\n",
      "        self_attention=[\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77])\n",
      "        ],\n",
      "        linear=[\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77]\n",
      "        ]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        layers=[\n",
      "          Linear(\n",
      "            weight=f32[64,77],\n",
      "            bias=f32[64],\n",
      "            in_features=77,\n",
      "            out_features=64,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f32[64,64],\n",
      "            bias=f32[64],\n",
      "            in_features=64,\n",
      "            out_features=64,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f32[77,64],\n",
      "            bias=f32[77],\n",
      "            in_features=64,\n",
      "            out_features=77,\n",
      "            use_bias=True\n",
      "          )\n",
      "        ]\n",
      "      ),\n",
      "      tn1=TokenNorm(\n",
      "        layer_norm=LayerNorm(\n",
      "          shape=(77,),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[77],\n",
      "          bias=f32[77]\n",
      "        )\n",
      "      ),\n",
      "      tn2=TokenNorm(\n",
      "        layer_norm=LayerNorm(\n",
      "          shape=(77,),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[77],\n",
      "          bias=f32[77]\n",
      "        )\n",
      "      )\n",
      "    ),\n",
      "    TransformerBlock(\n",
      "      mhsa=MultiHeadSelfAttention(\n",
      "        self_attention=[\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77])\n",
      "        ],\n",
      "        linear=[\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77]\n",
      "        ]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        layers=[\n",
      "          Linear(\n",
      "            weight=f32[64,77],\n",
      "            bias=f32[64],\n",
      "            in_features=77,\n",
      "            out_features=64,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f32[64,64],\n",
      "            bias=f32[64],\n",
      "            in_features=64,\n",
      "            out_features=64,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f32[77,64],\n",
      "            bias=f32[77],\n",
      "            in_features=64,\n",
      "            out_features=77,\n",
      "            use_bias=True\n",
      "          )\n",
      "        ]\n",
      "      ),\n",
      "      tn1=TokenNorm(\n",
      "        layer_norm=LayerNorm(\n",
      "          shape=(77,),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[77],\n",
      "          bias=f32[77]\n",
      "        )\n",
      "      ),\n",
      "      tn2=TokenNorm(\n",
      "        layer_norm=LayerNorm(\n",
      "          shape=(77,),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[77],\n",
      "          bias=f32[77]\n",
      "        )\n",
      "      )\n",
      "    ),\n",
      "    TransformerBlock(\n",
      "      mhsa=MultiHeadSelfAttention(\n",
      "        self_attention=[\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77])\n",
      "        ],\n",
      "        linear=[\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77]\n",
      "        ]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        layers=[\n",
      "          Linear(\n",
      "            weight=f32[64,77],\n",
      "            bias=f32[64],\n",
      "            in_features=77,\n",
      "            out_features=64,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f32[64,64],\n",
      "            bias=f32[64],\n",
      "            in_features=64,\n",
      "            out_features=64,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f32[77,64],\n",
      "            bias=f32[77],\n",
      "            in_features=64,\n",
      "            out_features=77,\n",
      "            use_bias=True\n",
      "          )\n",
      "        ]\n",
      "      ),\n",
      "      tn1=TokenNorm(\n",
      "        layer_norm=LayerNorm(\n",
      "          shape=(77,),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[77],\n",
      "          bias=f32[77]\n",
      "        )\n",
      "      ),\n",
      "      tn2=TokenNorm(\n",
      "        layer_norm=LayerNorm(\n",
      "          shape=(77,),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[77],\n",
      "          bias=f32[77]\n",
      "        )\n",
      "      )\n",
      "    ),\n",
      "    TransformerBlock(\n",
      "      mhsa=MultiHeadSelfAttention(\n",
      "        self_attention=[\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77]),\n",
      "          SelfAttention(Uk=f32[77,77], Uq=f32[77,77])\n",
      "        ],\n",
      "        linear=[\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77],\n",
      "          f32[77,77]\n",
      "        ]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        layers=[\n",
      "          Linear(\n",
      "            weight=f32[64,77],\n",
      "            bias=f32[64],\n",
      "            in_features=77,\n",
      "            out_features=64,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f32[64,64],\n",
      "            bias=f32[64],\n",
      "            in_features=64,\n",
      "            out_features=64,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          Linear(\n",
      "            weight=f32[77,64],\n",
      "            bias=f32[77],\n",
      "            in_features=64,\n",
      "            out_features=77,\n",
      "            use_bias=True\n",
      "          )\n",
      "        ]\n",
      "      ),\n",
      "      tn1=TokenNorm(\n",
      "        layer_norm=LayerNorm(\n",
      "          shape=(77,),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[77],\n",
      "          bias=f32[77]\n",
      "        )\n",
      "      ),\n",
      "      tn2=TokenNorm(\n",
      "        layer_norm=LayerNorm(\n",
      "          shape=(77,),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[77],\n",
      "          bias=f32[77]\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  ],\n",
      "  tokeniser=ImageTokeniser(\n",
      "    conv=Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[77,3,4,4],\n",
      "      bias=f32[77,1,1],\n",
      "      in_channels=3,\n",
      "      out_channels=77,\n",
      "      kernel_size=(4, 4),\n",
      "      stride=(4, 4),\n",
      "      padding='VALID',\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=True,\n",
      "      padding_mode='ZEROS'\n",
      "    )\n",
      "  ),\n",
      "  embedding=PositionEmbedding(embeddings=f32[77,64]),\n",
      "  final_mlp=MLP(\n",
      "    layers=[\n",
      "      Linear(\n",
      "        weight=f32[64,77],\n",
      "        bias=f32[64],\n",
      "        in_features=77,\n",
      "        out_features=64,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Linear(\n",
      "        weight=f32[64,64],\n",
      "        bias=f32[64],\n",
      "        in_features=64,\n",
      "        out_features=64,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Linear(\n",
      "        weight=f32[10,64],\n",
      "        bias=f32[10],\n",
      "        in_features=64,\n",
      "        out_features=10,\n",
      "        use_bias=True\n",
      "      )\n",
      "    ]\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable TinyVisionTransformer object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m get_batches(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m         transformer, opt_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m, in \u001b[0;36mmake_step\u001b[0;34m(model, opt_state, x, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_jit\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_step\u001b[39m(\n\u001b[1;32m     21\u001b[0m     model: TinyVisionTransformer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     y: Int[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     25\u001b[0m ):\n\u001b[1;32m     26\u001b[0m     loss_value, grads \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mfilter_value_and_grad(cross_entropy_loss_batch)(model, x, y)\n\u001b[0;32m---> 27\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     model \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mapply_updates(model, updates)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, opt_state, loss_value\n",
      "File \u001b[0;32m~/miniconda3/envs/rw/lib/python3.10/site-packages/jax/example_libraries/optimizers.py:189\u001b[0m, in \u001b[0;36moptimizer.<locals>.tree_opt_maker.<locals>.tree_update\u001b[0;34m(i, grad_tree, opt_state)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(update)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_update\u001b[39m(i, grad_tree, opt_state):\n\u001b[0;32m--> 189\u001b[0m   states_flat, tree, subtrees \u001b[38;5;241m=\u001b[39m opt_state\n\u001b[1;32m    190\u001b[0m   grad_flat, tree2 \u001b[38;5;241m=\u001b[39m tree_flatten(grad_tree)\n\u001b[1;32m    191\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tree2 \u001b[38;5;241m!=\u001b[39m tree:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable TinyVisionTransformer object"
     ]
    }
   ],
   "source": [
    "# Dummy dimensions\n",
    "D = 77\n",
    "\n",
    "# Random key\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "tokeniser = ImageTokeniser(\n",
    "    key,\n",
    "    input_dim=3,\n",
    "    token_dimension=D,\n",
    "    patch_size=4,\n",
    ")\n",
    "\n",
    "embedding = PositionEmbedding(\n",
    "    key,\n",
    "    input_dim=D,\n",
    "    sequence_length=(32 // 4)**2,\n",
    ")\n",
    "\n",
    "# Create a transformer\n",
    "transformer = TinyVisionTransformer(\n",
    "    key,\n",
    "    tokeniser=tokeniser,\n",
    "    embedding=embedding,\n",
    "    token_dim=D,\n",
    "    mlp_num_hidden=64,\n",
    "    mlp_num_layers=2,\n",
    "    num_heads=8,\n",
    "    num_blocks=4,\n",
    "    num_classes=10,\n",
    ")\n",
    "\n",
    "x, y = next(get_batches(32, \"train\"))\n",
    "cross_entropy_loss_batch(transformer, x, y)\n",
    "value, grads = eqx.filter_value_and_grad(cross_entropy_loss_batch)(transformer, x, y)\n",
    "print(grads)\n",
    "\n",
    "opt_state = init_fun(eqx.filter(transformer, eqx.is_array))\n",
    "\n",
    "for _ in range(1):\n",
    "    for x, y in get_batches(32, \"train\"):\n",
    "        transformer, opt_state, loss = make_step(transformer, opt_state, x, y)\n",
    "        print(loss)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations to other architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
