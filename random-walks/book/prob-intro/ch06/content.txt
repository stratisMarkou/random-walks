{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b770e9e6-f5ea-47d5-8f90-b3720ace33e5",
   "metadata": {},
   "source": [
    "# Multivariate distributions\n",
    "\n",
    "Joint distributions and the independence of general random variables are defined. Joint continuous distributions are defined, and analogues of the results for multivariate discrete random variables are given for the continuous case.\n",
    "\n",
    "## Joint distributions\n",
    "\n",
    "We are often interested in the values taken by two random variables $X$ and $Y,$ defined on the same probability space $(\\Omega, \\mathcal{F}, \\mathbb{P}).$\n",
    "The joint distribution function describes the probability of the outcome that $X$ and $Y$ assume some values simultaneously.\n",
    "\n",
    ":::{prf:definition} Joint distribution function\n",
    "\n",
    "Given random variables $X, Y$ on $(\\Omega, \\mathcal{F}, \\mathbb{P})$, their joint distribution function is the mapping $F_{X, Y} : \\mathbb{R}^2 \\to [0, 1]$ given by\n",
    "  \n",
    "$$\\begin{align}\n",
    "F_{X, Y}(x, y) = \\mathbb{P}(X \\leq x, Y \\leq y).\n",
    "\\end{align}$$\n",
    "\n",
    ":::\n",
    "\n",
    "This definition can be extended to joint distributions of any number of variables, by adding more variables to the set being measured.\n",
    "The joint distribution function satisfies:\n",
    "  \n",
    "$$\\begin{align}\n",
    "\\lim_{x, y \\to -\\infty}F_{X, Y}(x, y) = 0&,\\\\\n",
    "\\lim_{x, y \\to \\infty}F_{X, Y}(x, y) = 1&,\\\\\n",
    "x_1 \\leq x_2 \\text { and } y_1 \\leq y_2 \\implies  F_{X, Y}(x_1, y_1) \\leq\n",
    "F_{X, Y}(x_2, y_2)&.\n",
    "\\end{align}$$\n",
    "  \n",
    "The joint distribution function $F_{X, Y}$ is related to its marginal distributions $F_X(x), F_{Y}(y)$ by:\n",
    " \n",
    "$$\\begin{align}\n",
    "\\lim_{y \\to \\infty} F_{X, Y}(x, y) = F_X(x),\n",
    "\\lim_{x \\to \\infty} F_{X, Y}(x, y) = F_Y(y).\n",
    "\\end{align}$$\n",
    " \n",
    "We are often interested in how two random variables are related.\n",
    "In the special case where they are unrelated, we call them independent.\n",
    " \n",
    ":::{prf:definition} Independence of variables\n",
    "\n",
    "We say that two random variables $X$ and $Y$ are independent if for all $x, y \\in \\mathbb{R}$, the events $\\{X \\leq x\\}$ and $\\{Y \\leq y\\}$ are independent.\n",
    ":::\n",
    "\n",
    "Note that previously we had defined independence between events, as well as between discrete random variables.\n",
    "This definition extends independence to all random variables (discrete, continuous or other)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bda7e9-c9aa-4801-a923-3c422fa98f49",
   "metadata": {},
   "source": [
    "## Joint density functions\n",
    "\n",
    "If $X$ and $Y$ are continuous we can go further and define their joint pmf from their density function.\n",
    "\n",
    ":::{prf:definition} Joint probability density function\n",
    "\n",
    "The random variables $X, Y$ on $(\\Omega, \\mathcal{F}, \\mathbb{P})$ are called jointly continuous if their joint distribution function can be expressed in the form\n",
    " \n",
    "$$F_{X, Y}(x, y) = \\mathbb{P}(X \\leq, Y \\leq y) = \\int^x_{-\\infty} \\int^y_\n",
    "{-\\infty} f(u, v) du dv$$\n",
    " \n",
    "for $x, y \\in \\mathbb{R}$ and $f : \\mathbb{R}^2 \\to [0, \\infty)$.\n",
    "If this holds, we say that $X, Y$ have joint distribution $f$, denoted $f_{X, Y}$.\n",
    ":::\n",
    "\n",
    "The joint probability density function is related to the joint mass function by\n",
    "\n",
    "$$\\begin{align}\n",
    "f_{X, Y}(x, y) = \\begin{cases}\n",
    "\\frac{d}{dx}\\frac{d}{dy} F_{X, Y}(x, y) & \\text{ if this exists at } (x, y), \\\\\n",
    "0 & \\text{ otherwise.}\n",
    "\\end{cases}\n",
    "\\end{align}$$\n",
    "\n",
    "The following theorem says that we can go the other way around, integrating the pdf to obtain the probability of an event.\n",
    " \n",
    ":::{prf:theorem} Integral of a pdf\n",
    "\n",
    "If $A$ is a regular subset of $\\mathbb{R}^2$ and $X, Y$ are jointly continuous random variables with joint density function $f_{X, Y}$, then\n",
    " \n",
    "$$\\begin{align}\n",
    "\\mathbb{P}\\left((X, Y) \\in A\\right) = \\int \\int_{(x, y) \\in A} f_{X, Y}(x, y\n",
    ")dxdy.\n",
    "\\end{align}$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b61402-d42c-4c12-b575-9f33f7c1284b",
   "metadata": {},
   "source": [
    "## Independence and sums\n",
    "\n",
    "The manipulation of a joint distribution may simplify considerably if the variables are independent.\n",
    "As with discrete random variables, two variables are independent if and only if the joint distribution of continuous random variables factorises.\n",
    "\n",
    ":::{prf:theorem} Independence $\\iff$ pdf factorises\n",
    "\n",
    "Two jointly continuous random variables $X$ and $Y$ are independent if and only if their joint density function may be expressed in the form\n",
    "  \n",
    "$$\\begin{align}\n",
    "f_{X, Y}(x, y) = g(x)h(y), \\text{ for } x, y \\in \\mathbb{R}.\n",
    "\\end{align}$$\n",
    ":::\n",
    "\n",
    "\n",
    "Again, much like with discrete random variables, the sum of two independent continuous random variables has pmf equal to the convolution of the pmfs of the summands.\n",
    "\n",
    ":::{prf:theorem} Convolution formula\n",
    "If the random variables $X$ and $Y$ are independent and continuous, with pdfs $f_X$ and $f_Y$, then the density function of their sum $Z = X + Y$ is\n",
    "\n",
    "$$\\begin{align}\n",
    "f_Z(z) = \\int^\\infty_{-\\infty} f_X(x)f_Y(z - x) dx, \\text{ for } z \\in\n",
    "\\mathbb{R}.\n",
    "\\end{align}$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "## Changes of variables\n",
    "\n",
    "Given random variables $X, Y$, we are often interested in the distribution of $T(X, Y)$.\n",
    "If the random variables are continuous, and the function $T$ is a bijection, then the pmf of $T$ is given by the result below.\n",
    "\n",
    ":::{prf:theorem} Jacobian formula\n",
    "\n",
    "Let $X$ and $Y$ be jointly continuous with pdf $f_{X, Y}$ and $B(x, y) = (u(x, y), v(x, y))$ is a bijection from $D = \\{(x, y) : f_{X, Y}(x, y) > 0\\}$ to $S \\subseteq \\mathbb{R}^2$. Then the pair $(U, V) = (u(X, Y), v(X, Y))$ is jointly continuous with joint pdf\n",
    "  \n",
    "$$\\begin{align}\n",
    "f_{U, V}(u, v) = \\begin{cases}\n",
    "f_{X, Y}\\left(x(u, v), y(u, v)\\right) |J(u, v)|, & \\text{ if } (u, v) \\in S,\\\\\n",
    "0 & \\text{ otherwise,}\n",
    "\\end{cases}\n",
    "\\end{align}$$\n",
    "\n",
    "where $J(u, v)$ is the Jacobian of $B$\n",
    "\n",
    "$$\\begin{align}\n",
    "J(u, v) = \\begin{vmatrix}\n",
    "\\frac{\\partial x}{\\partial u}& \\frac{\\partial x}{\\partial v} \\\\\n",
    "\\frac{\\partial y}{\\partial u}& \\frac{\\partial y}{\\partial v}\n",
    "\\end{vmatrix}.\n",
    "\\end{align}$$\n",
    ":::\n",
    "\n",
    "\n",
    "This result extends the single-variable analogue for $Y = g(X(\\omega))$\n",
    ", where $g$ is an invertible mapping:\n",
    "\n",
    "$$f_Y(y) = f_X(g^{-1}(y)) \\frac{d}{dy} g^{-1}(y).$$\n",
    "\n",
    "It can be extended to more random variables by adding further variables to\n",
    " the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161c199-f6d4-45f7-8be4-032403faedf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rw",
   "language": "python",
   "name": "venv-rw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
